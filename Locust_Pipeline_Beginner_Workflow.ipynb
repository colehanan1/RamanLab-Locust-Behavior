{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Locust Pipeline Beginner Workflow Notebook\n\nThis notebook is a step-by-step version of the current pipeline in this repository.\n\nIt covers:\n1. Optional file organization and cleanup utilities.\n2. Video tracking and annotated output generation (`process_all_videos.py` logic).\n3. Trial analysis and plotting (`locust_analysis.py` logic).\n4. Optional dropped-frame export (`export_dropped_frames.py`).\n\nRun cells from top to bottom.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## How This Notebook Is Organized\n\n- Every stage is separated into its own section.\n- Important settings are in one configuration cell.\n- Potentially destructive operations have explicit safety toggles.\n- You can run a single stage, or run all stages sequentially.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 0: Project location setup\n# This cell ensures we are running from the repository root.\n\nfrom pathlib import Path\nimport os\nimport sys\n\ncurrent_working_directory = Path.cwd().resolve()\nproject_root_directory = current_working_directory\n\n# If notebook was opened from another folder, use the expected repository path.\nif not (project_root_directory / \"process_all_videos.py\").exists():\n    fallback_project_root = Path(\"/home/ramanlab/Documents/cole/VSCode/RamanLab-Locust-Behavior\")\n    if fallback_project_root.exists():\n        project_root_directory = fallback_project_root\n\nos.chdir(project_root_directory)\n\nprint(f\"Project root: {project_root_directory}\")\nprint(f\"Python executable: {sys.executable}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Optional: Install Missing Python Packages\n\nIf imports fail later, run the cell below once. Otherwise, leave it disabled.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 0b: Optional dependency install\n# Set RUN_PACKAGE_INSTALL to True only if you are missing packages.\n\nRUN_PACKAGE_INSTALL = False\n\nif RUN_PACKAGE_INSTALL:\n    # Install packages into the current Jupyter/Python environment.\n    package_install_command = [\n        sys.executable,\n        \"-m\",\n        \"pip\",\n        \"install\",\n        \"-U\",\n        \"numpy\",\n        \"pandas\",\n        \"matplotlib\",\n        \"opencv-python\",\n        \"torch\",\n        \"ultralytics\",\n    ]\n    print(\"Running:\", \" \".join(package_install_command))\n    subprocess.run(package_install_command, check=True)\nelse:\n    print(\"Package install skipped (RUN_PACKAGE_INSTALL=False).\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 0c: Shared imports and helper utilities\n\nimport shlex\nimport subprocess\nimport importlib\nfrom typing import Iterable\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n\ndef run_cli_command(command_parts: Iterable[str], raise_on_error: bool = True):\n    \"\"\"\n    Run a command, print it clearly, and stream output for easier debugging.\n    \"\"\"\n    command_parts = [str(part) for part in command_parts]\n    printable_command = \" \".join(shlex.quote(part) for part in command_parts)\n    print(f\"Running command:\\n{printable_command}\\n\")\n\n    completed_process = subprocess.run(\n        command_parts,\n        cwd=project_root_directory,\n        text=True,\n        capture_output=True,\n    )\n\n    if completed_process.stdout.strip():\n        print(\"STDOUT:\\n\" + completed_process.stdout)\n    if completed_process.stderr.strip():\n        print(\"STDERR:\\n\" + completed_process.stderr)\n\n    if raise_on_error and completed_process.returncode != 0:\n        raise RuntimeError(f\"Command failed with code {completed_process.returncode}\")\n\n    return completed_process\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 1: Pipeline Configuration\n\nEdit values in the next cell before running the pipeline.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 1a: Main paths and stage toggles\n\n# Core directories\nall_videos_directory = project_root_directory / \"Locust\" / \"all_vids\"\nyolo_model_weights_path = project_root_directory / \"model\" / \"best.pt\"\nanalysis_output_directory = project_root_directory / \"Locust\" / \"analysis_outputs\"\ndropped_frame_export_directory = analysis_output_directory / \"dropped_frames\"\n\n# High-level stage toggles\nrun_optional_file_organization_stage = False\nrun_video_tracking_stage = True\nrun_analysis_stage = True\nrun_dropped_frame_export_stage = False\n\n# Video processing settings\nallow_cpu_if_no_gpu = True\noverwrite_existing_tracking_outputs = False\n\n# Optional filtering for processing stage.\n# Leave empty list [] to process every detected date directory.\n# Example:\n# date_directories_to_process = [\n#     all_videos_directory / \"Off_1s_LOOL\" / \"12.02.2025\",\n# ]\ndate_directories_to_process = []\n\n# Analysis script settings (these match locust_analysis.py defaults)\nanalysis_fps = 30.0\nanalysis_odor_on_seconds = 10.0\nanalysis_odor_off_seconds = 14.0\nanalysis_threshold_k = 4.0\nanalysis_min_samples_over_threshold = 20\nanalysis_min_floor_pixels = 9.5\nanalysis_skip_plot_generation = False\nanalysis_skip_combined_csv_outputs = False\n\n# Dropped-frame export settings\ndropped_frame_minimum_pixels = 9.5\n\nprint(\"Configuration loaded.\")\nprint(f\"all_videos_directory: {all_videos_directory}\")\nprint(f\"yolo_model_weights_path: {yolo_model_weights_path}\")\nprint(f\"analysis_output_directory: {analysis_output_directory}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 1b: Optional file-organization utility toggles\n# These map to helper scripts in this repository.\n\n# Safety toggle:\n# - False: dry-run preview only (recommended first)\n# - True: make real file changes\nfile_organization_execute_changes = False\n\nrun_rename_videos_step = False\nrun_move_videos_to_date_step = False\nrun_fix_duplicate_names_step = False\nrun_cleanup_keep_dates_step = False\nrun_delete_annotated_videos_step = False\n\nprint(\"File-organization toggles loaded.\")\nprint(f\"file_organization_execute_changes: {file_organization_execute_changes}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 1c: Validate important paths before running long jobs\n\nrequired_paths = {\n    \"project_root_directory\": project_root_directory,\n    \"all_videos_directory\": all_videos_directory,\n    \"yolo_model_weights_path\": yolo_model_weights_path,\n}\n\nmissing_paths = []\nfor name, path_value in required_paths.items():\n    if not Path(path_value).exists():\n        missing_paths.append((name, path_value))\n\nif missing_paths:\n    print(\"Missing required paths:\")\n    for name, path_value in missing_paths:\n        print(f\"- {name}: {path_value}\")\n    raise FileNotFoundError(\"Fix missing paths in Step 1 before continuing.\")\n\nprint(\"All required paths exist.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 2: Optional File Organization / Cleanup Stage\n\nThis section reuses these scripts directly:\n- `rename_videos.py`\n- `move_videos_to_date.py`\n- `fix_duplicate_names.py`\n- `cleanup_keep_dates.py`\n- `delete_palps.py`\n\nRun in dry-run mode first.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 2a: Run selected file-organization utilities\n\nif run_optional_file_organization_stage:\n    print(\"Running optional file organization stage...\")\n\n    rename_videos_module = importlib.import_module(\"rename_videos\")\n    move_videos_module = importlib.import_module(\"move_videos_to_date\")\n    fix_duplicate_names_module = importlib.import_module(\"fix_duplicate_names\")\n    cleanup_keep_dates_module = importlib.import_module(\"cleanup_keep_dates\")\n    delete_palps_module = importlib.import_module(\"delete_palps\")\n\n    # Override module-level base directories so the notebook config is always used.\n    rename_videos_module.BASE_DIR = all_videos_directory\n    move_videos_module.BASE_DIR = all_videos_directory\n    fix_duplicate_names_module.BASE_DIR = all_videos_directory\n    cleanup_keep_dates_module.BASE_DIR = all_videos_directory\n    delete_palps_module.BASE_DIR = all_videos_directory\n\n    dry_run_mode = not file_organization_execute_changes\n    print(f\"Dry-run mode: {dry_run_mode}\")\n\n    if run_rename_videos_step:\n        print(\"\\n--- Running rename_videos ---\")\n        rename_videos_module.rename_videos(dry_run=dry_run_mode)\n\n    if run_move_videos_to_date_step:\n        print(\"\\n--- Running move_videos_to_date ---\")\n        move_videos_module.move_files(dry_run=dry_run_mode)\n\n    if run_fix_duplicate_names_step:\n        print(\"\\n--- Running fix_duplicate_names ---\")\n        fix_duplicate_names_module.fix_duplicates(dry_run=dry_run_mode)\n\n    if run_cleanup_keep_dates_step:\n        print(\"\\n--- Running cleanup_keep_dates ---\")\n        cleanup_keep_dates_module.process(dry_run=dry_run_mode)\n\n    if run_delete_annotated_videos_step:\n        print(\"\\n--- Running delete_palps ---\")\n        delete_palps_module.run(dry_run=dry_run_mode)\n\n    print(\"\\nOptional file organization stage finished.\")\nelse:\n    print(\"Optional file organization stage skipped (run_optional_file_organization_stage=False).\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 3: Video Processing Stage (Tracking + Annotated Video + CSV)\n\nThis stage reuses `process_all_videos.py` logic directly.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 3a: Import processing module and prepare list of videos to process\n\nimport process_all_videos as processing_pipeline\n\n# Choose date directories from config.\nif date_directories_to_process:\n    selected_date_directories = [Path(p) for p in date_directories_to_process if Path(p).exists()]\n    missing_date_directories = [Path(p) for p in date_directories_to_process if not Path(p).exists()]\n    for missing_path in missing_date_directories:\n        print(f\"Warning: date directory does not exist and will be skipped: {missing_path}\")\nelse:\n    selected_date_directories = processing_pipeline.find_date_dirs(all_videos_directory)\n\n# Build concrete list of video files in selected date directories.\nvideo_file_paths_to_process = []\nfor date_directory in selected_date_directories:\n    for candidate_file in sorted(date_directory.iterdir()):\n        if not candidate_file.is_file():\n            continue\n        if candidate_file.suffix.lower() not in processing_pipeline.VIDEO_EXTENSIONS:\n            continue\n        if candidate_file.stem.endswith(\"_palps_annotated_30fps\"):\n            continue\n        video_file_paths_to_process.append(candidate_file)\n\nprint(f\"Selected date directories: {len(selected_date_directories)}\")\nprint(f\"Video files queued: {len(video_file_paths_to_process)}\")\n\nfor preview_path in video_file_paths_to_process[:10]:\n    print(f\"- {preview_path}\")\n\nif len(video_file_paths_to_process) > 10:\n    print(\"... (list truncated)\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 3b: Load YOLO model and choose compute device\n\nimport torch\nfrom ultralytics import YOLO\n\nif torch.cuda.is_available():\n    compute_device_name = \"cuda\"\n    print(\"CUDA is available. Using GPU.\")\nelse:\n    if allow_cpu_if_no_gpu:\n        compute_device_name = \"cpu\"\n        print(\"CUDA not available. Using CPU because allow_cpu_if_no_gpu=True.\")\n    else:\n        raise RuntimeError(\"CUDA is not available and allow_cpu_if_no_gpu=False.\")\n\nyolo_model = YOLO(str(yolo_model_weights_path))\nyolo_model.to(compute_device_name)\n\nprint(f\"YOLO model loaded on device: {compute_device_name}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 3c: Run tracking pipeline for each queued video\n\nif run_video_tracking_stage:\n    if not video_file_paths_to_process:\n        print(\"No videos queued. Nothing to process.\")\n    else:\n        print(f\"Starting tracking stage for {len(video_file_paths_to_process)} videos...\")\n\n        for video_index, video_file_path in enumerate(video_file_paths_to_process, start=1):\n            print(\"=\" * 80)\n            print(f\"[{video_index}/{len(video_file_paths_to_process)}] Processing: {video_file_path.name}\")\n            processing_pipeline.process_video(\n                video_path=video_file_path,\n                model=yolo_model,\n                overwrite=overwrite_existing_tracking_outputs,\n            )\n\n        print(\"Tracking stage finished.\")\nelse:\n    print(\"Video tracking stage skipped (run_video_tracking_stage=False).\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 4: Analysis Stage (Distance %, Threshold, Reactions, Plots)\n\nThis stage runs `locust_analysis.py` with your configured parameters.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 4a: Build and run locust_analysis.py command\n\nlocust_analysis_script_path = project_root_directory / \"locust_analysis.py\"\n\nanalysis_command = [\n    sys.executable,\n    str(locust_analysis_script_path),\n    \"--all-vids-dir\", str(all_videos_directory),\n    \"--output-dir\", str(analysis_output_directory),\n    \"--fps\", str(analysis_fps),\n    \"--odor-on-s\", str(analysis_odor_on_seconds),\n    \"--odor-off-s\", str(analysis_odor_off_seconds),\n    \"--threshold-k\", str(analysis_threshold_k),\n    \"--min-samples-over\", str(analysis_min_samples_over_threshold),\n]\n\nif analysis_min_floor_pixels is not None:\n    analysis_command += [\"--min-floor-px\", str(analysis_min_floor_pixels)]\nif analysis_skip_plot_generation:\n    analysis_command += [\"--skip-plots\"]\nif analysis_skip_combined_csv_outputs:\n    analysis_command += [\"--skip-combined\"]\n\nif run_analysis_stage:\n    run_cli_command(analysis_command, raise_on_error=True)\nelse:\n    print(\"Analysis stage skipped (run_analysis_stage=False).\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 5: Optional Dropped-Frame Export Stage\n\nThis stage runs `export_dropped_frames.py` to save frames where distance is invalid or below threshold.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 5a: Build and run export_dropped_frames.py command\n\nexport_dropped_frames_script_path = project_root_directory / \"export_dropped_frames.py\"\n\nexport_frames_command = [\n    sys.executable,\n    str(export_dropped_frames_script_path),\n    \"--all-vids-dir\", str(all_videos_directory),\n    \"--output-dir\", str(dropped_frame_export_directory),\n    \"--min-px\", str(dropped_frame_minimum_pixels),\n]\n\nif run_dropped_frame_export_stage:\n    run_cli_command(export_frames_command, raise_on_error=True)\nelse:\n    print(\"Dropped-frame export stage skipped (run_dropped_frame_export_stage=False).\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 6: Quick Output Inspection\n\nUse these cells to confirm the pipeline created the expected files.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 6a: Show important output files\n\nimportant_output_paths = [\n    analysis_output_directory / \"locust_combined_long.csv\",\n    analysis_output_directory / \"locust_combined_trials.csv\",\n    analysis_output_directory / \"traces\",\n    analysis_output_directory / \"reaction_matrix\",\n    analysis_output_directory / \"dataset_means\",\n    dropped_frame_export_directory,\n]\n\nfor output_path in important_output_paths:\n    exists_flag = output_path.exists()\n    print(f\"{output_path} -> exists={exists_flag}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 6b: Preview first rows of combined trials CSV (if available)\n\ncombined_trials_csv_path = analysis_output_directory / \"locust_combined_trials.csv\"\n\nif combined_trials_csv_path.exists():\n    combined_trials_dataframe = pd.read_csv(combined_trials_csv_path)\n    print(f\"Rows: {len(combined_trials_dataframe)}\")\n    display(combined_trials_dataframe.head(10))\nelse:\n    print(f\"File not found: {combined_trials_csv_path}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Optional Convenience Cell: Run Main Stages Sequentially\n\nSet stage toggles in Step 1 first, then run this cell to execute the selected stages in order.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Optional one-click runner for selected stages.\n# This simply reminds you of the intended run order.\n\nprint(\"Recommended sequential run order:\")\nprint(\"1) Step 2 (optional file organization)\")\nprint(\"2) Step 3 (video processing)\")\nprint(\"3) Step 4 (analysis)\")\nprint(\"4) Step 5 (optional dropped-frame export)\")\nprint(\"5) Step 6 (output inspection)\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}