{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0452a9c8-34bd-48cb-8ce6-78acf6addc09",
   "metadata": {},
   "source": [
    "# Imports & Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d604f36-98e9-4151-b7de-4d67384141ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "Allocated memory: 277.25 MB\n",
      "Reserved memory: 280.00 MB\n",
      "Max allocated memory: 8711.36 MB\n",
      "Max reserved memory: 8816.00 MB\n",
      "['L5', 'L2', 'L8', 'L6', 'L1', 'L3', 'L4', 'L7']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from skimage import measure\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "\n",
    "# CUDA setup\n",
    "torch.backends.cudnn.benchmark = True  # Optimize for GPUs with a fixed input size\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. This script requires a GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is available\")\n",
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments=True\n",
    "torch.cuda.empty_cache()  # Clear the CUDA memory before starting\n",
    "\n",
    "# Display current memory stats\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Reserved memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "print(f\"Max allocated memory: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Max reserved memory: {torch.cuda.max_memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "#From the end of YOLO training\n",
    "model_path = \"/home/ramanlab/Documents/Arshiya/Yolo-Model/YOLOLocustPalps/runs/obb/train72/weights/best.pt\"\n",
    "\n",
    "# Main directory containing individual fly folders\n",
    "main_directory = \"/home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX\" \n",
    "\n",
    "overall_mean_latency_s = 0\n",
    "OVERALL_MEAN_LATENCY_S = overall_mean_latency_s\n",
    "\n",
    "# Get list of all individual fly folders\n",
    "individual_fly_folders = [f for f in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, f))]\n",
    "\n",
    "print(f'{individual_fly_folders}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fd2eac19-3957-4431-bc0d-6c40ac7dce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run extra_scripts/timestampFunction.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46219043-cff1-4d83-a08d-328acfb3244c",
   "metadata": {},
   "source": [
    "# Run Yolo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba4acad8-2ac8-4d34-a41c-6646ac628ebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to CUDA.\n",
      "CSV file /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L5/Trial_1_Recording.csv not found. Timestamps will be synthetic.\n",
      "Processed video /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L5/Trial_1_Recording.mp4 in 4.54 seconds. Output saved to /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L5/1_Recording\n",
      "CSV file /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L2/Trial_1_Recording.csv not found. Timestamps will be synthetic.\n",
      "Processed video /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L2/Trial_1_Recording.mp4 in 4.45 seconds. Output saved to /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L2/1_Recording\n",
      "CSV file /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L8/Trial_1_Recording.csv not found. Timestamps will be synthetic.\n",
      "Processed video /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L8/Trial_1_Recording.mp4 in 4.47 seconds. Output saved to /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L8/1_Recording\n",
      "CSV file /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L6/Trial_1_Recording.csv not found. Timestamps will be synthetic.\n",
      "Processed video /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L6/Trial_1_Recording.mp4 in 4.42 seconds. Output saved to /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L6/1_Recording\n",
      "CSV file /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L1/Trial_1_Recording.csv not found. Timestamps will be synthetic.\n",
      "Processed video /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L1/Trial_1_Recording.mp4 in 4.55 seconds. Output saved to /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L1/1_Recording\n",
      "CSV file /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L3/Trial_1_Recording.csv not found. Timestamps will be synthetic.\n",
      "Processed video /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L3/Trial_1_Recording.mp4 in 4.48 seconds. Output saved to /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L3/1_Recording\n",
      "CSV file /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L4/Trial_1_Recording.csv not found. Timestamps will be synthetic.\n",
      "Processed video /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L4/Trial_1_Recording.mp4 in 4.43 seconds. Output saved to /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L4/1_Recording\n",
      "CSV file /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L7/Trial_1_Recording.csv not found. Timestamps will be synthetic.\n",
      "Processed video /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L7/Trial_1_Recording.mp4 in 4.48 seconds. Output saved to /home/ramanlab/Documents/Arshiya/all_vids/08.14.2025/Testing/HEX/L7/1_Recording\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO \n",
    "import torch\n",
    "from collections import deque\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "import subprocess\n",
    "\n",
    "def create_video_writer(output_path: str, width: int, height: int, fps: float):\n",
    "    \"\"\"\n",
    "    Try to create an OpenCV VideoWriter for MP4.\n",
    "    If fails, fallback to AVI + XVID.\n",
    "    Returns: writer object, final output path\n",
    "    \"\"\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    if writer.isOpened():\n",
    "        return writer, output_path\n",
    "\n",
    "    # fallback to AVI\n",
    "    avi_path = output_path.replace(\".mp4\", \".avi\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    writer = cv2.VideoWriter(avi_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    if not writer.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open video writer for {output_path} or fallback {avi_path}\")\n",
    "    \n",
    "    print(f\"[INFO] OpenCV MP4 writer failed, using AVI fallback: {avi_path}\")\n",
    "    return writer, avi_path\n",
    "\n",
    "def convert_avi_to_mp4(avi_path: str, mp4_path: str):\n",
    "    \"\"\"Convert AVI to MP4 using ffmpeg\"\"\"\n",
    "    cmd = [\n",
    "        \"ffmpeg\",\n",
    "        \"-y\",  # overwrite if exists\n",
    "        \"-i\", avi_path,\n",
    "        \"-c:v\", \"libx264\",\n",
    "        \"-crf\", \"18\",\n",
    "        \"-preset\", \"fast\",\n",
    "        mp4_path\n",
    "    ]\n",
    "    print(f\"[INFO] Converting {avi_path} -> {mp4_path} via ffmpeg\")\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# USER CONFIG\n",
    "# Expect 'model_path' and 'main_directory' to be defined in your environment.\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Target: track TWO instances of class 1 (palps) from an OBB model\n",
    "TARGET_CLASS_ID = 1\n",
    "NUM_TARGETS     = 2\n",
    "\n",
    "# Force output video to 30 FPS (regardless of input/CSV)\n",
    "OUTPUT_FPS      = 30.0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "    print(\"Model moved to CUDA.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Running on CPU.\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# TEMPORAL / SPATIAL ENHANCEMENT CONFIG\n",
    "CONF_THRES      = 0.40   # detector confidence threshold\n",
    "IOU_MATCH_THRES = 0.70   # IoU threshold for associating detections to existing track\n",
    "MAX_AGE         = 15     # frames allowed without fresh detection before a track is considered stale\n",
    "EMA_ALPHA       = 0.20   # extra exponential smoothing after Kalman update (0=off; 0.15–0.25 recommended)\n",
    "\n",
    "# Optical flow (optional, only used when a track has no fresh detection this frame)\n",
    "FLOW_ENABLE     = True\n",
    "FLOW_SKIP_EDGE  = 10     # ignore flow near borders (px)\n",
    "FLOW_PARAMS     = dict(pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.2, flags=0)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Geometry helpers\n",
    "def order_corners(corners):\n",
    "    \"\"\"\n",
    "    corners: np.array shape (4,2) in arbitrary order\n",
    "    Returns: ordered list of corners in clockwise order.\n",
    "    \"\"\"\n",
    "    pts = np.array(corners, dtype=np.float32)\n",
    "    cx, cy = np.mean(pts, axis=0)\n",
    "    angles = np.arctan2(pts[:, 1] - cy, pts[:, 0] - cx)\n",
    "    pts_sorted = pts[np.argsort(angles)]\n",
    "    return pts_sorted.tolist()\n",
    "\n",
    "def xyxy_to_cxcywh(b):\n",
    "    x1, y1, x2, y2 = b\n",
    "    w = max(0.0, x2 - x1)\n",
    "    h = max(0.0, y2 - y1)\n",
    "    cx = x1 + w / 2.0\n",
    "    cy = y1 + h / 2.0\n",
    "    return np.array([cx, cy, w, h], dtype=np.float32)\n",
    "\n",
    "def cxcywh_to_xyxy(s):\n",
    "    cx, cy, w, h = s\n",
    "    x1 = cx - w / 2.0\n",
    "    y1 = cy - h / 2.0\n",
    "    x2 = cx + w / 2.0\n",
    "    y2 = cy + h / 2.0\n",
    "    return np.array([x1, y1, x2, y2], dtype=np.float32)\n",
    "\n",
    "def iou(a, b):\n",
    "    # a,b: [N,4] xyxy\n",
    "    N = a.shape[0]\n",
    "    M = b.shape[0]\n",
    "    if N == 0 or M == 0:\n",
    "        return np.zeros((N, M), dtype=np.float32)\n",
    "\n",
    "    ax1, ay1, ax2, ay2 = a[:, 0][:, None], a[:, 1][:, None], a[:, 2][:, None], a[:, 3][:, None]\n",
    "    bx1, by1, bx2, by2 = b[:, 0][None, :], b[:, 1][None, :], b[:, 2][None, :], b[:, 3][None, :]\n",
    "\n",
    "    inter_w = np.maximum(0, np.minimum(ax2, bx2) - np.maximum(ax1, bx1))\n",
    "    inter_h = np.maximum(0, np.minimum(ay2, by2) - np.maximum(ay1, by1))\n",
    "    inter   = inter_w * inter_h\n",
    "\n",
    "    area_a = (ax2 - ax1) * (ay2 - ay1)\n",
    "    area_b = (bx2 - bx1) * (by2 - by1)\n",
    "    union  = area_a + area_b - inter + 1e-6\n",
    "    return (inter / union).astype(np.float32)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Kalman for (cx, cy, w, h) + multi-object tracker (single class)\n",
    "class KalmanBBox:\n",
    "    def __init__(self):\n",
    "        self.x = np.zeros((8, 1), dtype=np.float32)\n",
    "        self.P = np.eye(8, dtype=np.float32) * 10.0\n",
    "\n",
    "        self.F = np.eye(8, dtype=np.float32)\n",
    "        for i in range(4):\n",
    "            self.F[i, i + 4] = 1.0  # constant velocity\n",
    "\n",
    "        self.H = np.zeros((4, 8), dtype=np.float32)\n",
    "        self.H[0, 0] = self.H[1, 1] = self.H[2, 2] = self.H[3, 3] = 1.0\n",
    "\n",
    "        self.Q = np.eye(8, dtype=np.float32) * 0.02\n",
    "        self.R = np.eye(4, dtype=np.float32) * 1.0\n",
    "\n",
    "    def init(self, cxcywh):\n",
    "        self.x[:4, 0] = cxcywh\n",
    "        self.x[4:, 0] = 0.0\n",
    "        self.P = np.eye(8, dtype=np.float32) * 10.0\n",
    "\n",
    "    def predict(self):\n",
    "        self.x = self.F @ self.x\n",
    "        self.P = self.F @ self.P @ self.F.T + self.Q\n",
    "        return self.x[:4, 0].copy()\n",
    "\n",
    "    def update(self, z):\n",
    "        z = z.reshape(4, 1)\n",
    "        y = z - (self.H @ self.x)\n",
    "        S = self.H @ self.P @ self.H.T + self.R\n",
    "        K = self.P @ self.H.T @ np.linalg.inv(S)\n",
    "        self.x = self.x + K @ y\n",
    "        I = np.eye(8, dtype=np.float32)\n",
    "        self.P = (I - K @ self.H) @ self.P\n",
    "\n",
    "class Track:\n",
    "    _next_id = 1\n",
    "\n",
    "    def __init__(self, cxcywh, score, corners: Optional[List[List[float]]] = None):\n",
    "        self.id = Track._next_id\n",
    "        Track._next_id += 1\n",
    "\n",
    "        self.kf = KalmanBBox()\n",
    "        self.kf.init(cxcywh)\n",
    "\n",
    "        self.score = float(score)\n",
    "        self.box_xyxy = cxcywh_to_xyxy(cxcywh)\n",
    "\n",
    "        self.time_since_update = 0\n",
    "        self.hits = 1\n",
    "        self.history = deque(maxlen=30)\n",
    "\n",
    "        # Last known OBB corners (ordered 4x2 list), if available\n",
    "        self.corners = corners\n",
    "\n",
    "    def predict(self):\n",
    "        pred = self.kf.predict()\n",
    "        box = cxcywh_to_xyxy(pred)\n",
    "        self.box_xyxy = box\n",
    "        self.history.append(box.copy())\n",
    "        self.time_since_update += 1\n",
    "        return box\n",
    "\n",
    "    def correct(self, cxcywh, score, corners: Optional[List[List[float]]] = None):\n",
    "        self.kf.update(cxcywh)\n",
    "        box = cxcywh_to_xyxy(self.kf.x[:4, 0])\n",
    "\n",
    "        # EMA smoothing to reduce jitter\n",
    "        self.box_xyxy = (1 - EMA_ALPHA) * box + EMA_ALPHA * self.box_xyxy\n",
    "\n",
    "        self.score = float(score)\n",
    "        self.hits += 1\n",
    "        self.time_since_update = 0\n",
    "        self.history.append(self.box_xyxy.copy())\n",
    "\n",
    "        if corners is not None:\n",
    "            self.corners = corners\n",
    "\n",
    "class MultiObjectSingleClassTracker:\n",
    "    \"\"\"Maintains 0–N tracks for ONE class; returns all active tracks sorted by quality.\"\"\"\n",
    "    def __init__(self, iou_thres=0.25, max_age=15):\n",
    "        self.iou_thres = iou_thres\n",
    "        self.max_age   = max_age\n",
    "        self.tracks: List[Track] = []\n",
    "\n",
    "    def step(self, det_xyxy: np.ndarray, det_scores: np.ndarray, det_corners: Optional[List[Optional[List[List[float]]]]] = None) -> List[Track]:\n",
    "        if det_corners is None:\n",
    "            det_corners = [None] * len(det_xyxy)\n",
    "\n",
    "        # 1) Predict all\n",
    "        preds = [t.predict() for t in self.tracks]\n",
    "\n",
    "        # 2) Greedy IoU match (track <-> detection)\n",
    "        assigned_tr, assigned_det = set(), set()\n",
    "        if len(self.tracks) and len(det_xyxy):\n",
    "            M = iou(np.stack(preds), det_xyxy)\n",
    "            while True:\n",
    "                i, j = np.unravel_index(np.argmax(M), M.shape)\n",
    "                if M[i, j] < self.iou_thres:\n",
    "                    break\n",
    "                if i in assigned_tr or j in assigned_det:\n",
    "                    M[i, j] = -1\n",
    "                    continue\n",
    "\n",
    "                self.tracks[i].correct(\n",
    "                    xyxy_to_cxcywh(det_xyxy[j]),\n",
    "                    det_scores[j],\n",
    "                    corners=det_corners[j]\n",
    "                )\n",
    "                assigned_tr.add(i)\n",
    "                assigned_det.add(j)\n",
    "                M[i, :] = -1\n",
    "                M[:, j] = -1\n",
    "\n",
    "        # 3) Create tracks for unmatched detections\n",
    "        for j in range(len(det_xyxy)):\n",
    "            if j in assigned_det:\n",
    "                continue\n",
    "            self.tracks.append(\n",
    "                Track(xyxy_to_cxcywh(det_xyxy[j]), det_scores[j], corners=det_corners[j])\n",
    "            )\n",
    "\n",
    "        # 4) Prune stale\n",
    "        self.tracks = [t for t in self.tracks if t.time_since_update <= self.max_age]\n",
    "\n",
    "        # 5) Sort: freshest first, then most hits, then score\n",
    "        self.tracks.sort(key=lambda t: (t.time_since_update, -t.hits, -t.score, t.id))\n",
    "        return self.tracks\n",
    "\n",
    "def flow_nudge(prev_gray, gray, box_xyxy):\n",
    "    if prev_gray is None:\n",
    "        return box_xyxy\n",
    "\n",
    "    x1, y1, x2, y2 = box_xyxy.astype(int)\n",
    "    x1 = max(FLOW_SKIP_EDGE, x1)\n",
    "    y1 = max(FLOW_SKIP_EDGE, y1)\n",
    "    x2 = min(gray.shape[1] - FLOW_SKIP_EDGE, x2)\n",
    "    y2 = min(gray.shape[0] - FLOW_SKIP_EDGE, y2)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return box_xyxy\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(\n",
    "        prev_gray[y1:y2, x1:x2],\n",
    "        gray[y1:y2, x1:x2],\n",
    "        None,\n",
    "        **FLOW_PARAMS\n",
    "    )\n",
    "    dx = np.median(flow[..., 0])\n",
    "    dy = np.median(flow[..., 1])\n",
    "\n",
    "    nudged = box_xyxy.copy().astype(np.float32)\n",
    "    nudged[0::2] += dx\n",
    "    nudged[1::2] += dy\n",
    "    return nudged\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CSV timestamp helpers (unchanged)\n",
    "TIMESTAMP_CANDIDATES = [\"UTC_ISO\", \"Timestamp\", \"Number\", \"MonoNs\"]\n",
    "FRAME_CANDIDATES = [\"Frame Number\", \"FrameNumber\"]\n",
    "\n",
    "def _pick_timestamp_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in TIMESTAMP_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _pick_frame_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in FRAME_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _to_seconds_series(df: pd.DataFrame, ts_col: str) -> pd.Series:\n",
    "    s = df[ts_col]\n",
    "    if ts_col in (\"UTC_ISO\", \"Timestamp\"):\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\", utc=(ts_col == \"UTC_ISO\"))\n",
    "        secs = dt.astype(\"int64\") / 1e9\n",
    "        t0 = np.nanmin(secs.values)\n",
    "        return (secs - t0).astype(float)\n",
    "    if ts_col == \"Number\":\n",
    "        vals = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "        t0 = np.nanmin(vals.values)\n",
    "        return vals - t0\n",
    "    if ts_col == \"MonoNs\":\n",
    "        vals = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "        secs = vals / 1e9\n",
    "        t0 = np.nanmin(secs.values)\n",
    "        return secs - t0\n",
    "    raise ValueError(f\"Unsupported timestamp column: {ts_col}\")\n",
    "\n",
    "def _estimate_fps_from_seconds(seconds_series: pd.Series) -> Optional[float]:\n",
    "    mask = seconds_series.notna()\n",
    "    if mask.sum() < 2:\n",
    "        return None\n",
    "    duration = seconds_series[mask].iloc[-1] - seconds_series[mask].iloc[0]\n",
    "    if duration <= 0:\n",
    "        return None\n",
    "    return mask.sum() / duration\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Per-frame processing: detect ALL class-1 palps, track, draw boxes+centers, line between two\n",
    "def process_frame(\n",
    "    frame,\n",
    "    frame_number,\n",
    "    current_timestamp,\n",
    "    tracker: MultiObjectSingleClassTracker,\n",
    "    prev_gray\n",
    "):\n",
    "    # 1) YOLO inference\n",
    "    r = model.predict(source=frame, conf=CONF_THRES, verbose=False)[0]\n",
    "\n",
    "    # 2) Collect ALL detections for TARGET_CLASS_ID (xyxy + score + OBB corners if present)\n",
    "    det_boxes = []\n",
    "    det_scores = []\n",
    "    det_corners = []  # list of ordered corners (4x2) or None per det\n",
    "\n",
    "    if hasattr(r, \"obb\") and r.obb is not None:\n",
    "        xyxyxyxy = r.obb.xyxyxyxy.cpu().numpy()  # (N,8)\n",
    "        cls_arr  = r.obb.cls.cpu().numpy().astype(int)\n",
    "        conf_arr = (\n",
    "            r.obb.conf.cpu().numpy().astype(np.float32)\n",
    "            if hasattr(r.obb, \"conf\") and r.obb.conf is not None\n",
    "            else np.ones_like(cls_arr, dtype=np.float32)\n",
    "        )\n",
    "\n",
    "        for i, (c, s) in enumerate(zip(cls_arr, conf_arr)):\n",
    "            if c != TARGET_CLASS_ID:\n",
    "                continue\n",
    "            corners = xyxyxyxy[i].reshape(4, 2)\n",
    "            x1, y1 = float(corners[:, 0].min()), float(corners[:, 1].min())\n",
    "            x2, y2 = float(corners[:, 0].max()), float(corners[:, 1].max())\n",
    "            det_boxes.append([x1, y1, x2, y2])\n",
    "            det_scores.append(float(s))\n",
    "            det_corners.append(order_corners(corners))\n",
    "    else:\n",
    "        # Fallback if model outputs axis-aligned boxes\n",
    "        if r.boxes is not None and len(r.boxes) > 0:\n",
    "            xyxy = r.boxes.xyxy.cpu().numpy()\n",
    "            cls_arr  = r.boxes.cls.cpu().numpy().astype(int)\n",
    "            conf_arr = r.boxes.conf.cpu().numpy().astype(np.float32)\n",
    "            for b, c, s in zip(xyxy, cls_arr, conf_arr):\n",
    "                if c != TARGET_CLASS_ID:\n",
    "                    continue\n",
    "                det_boxes.append([float(b[0]), float(b[1]), float(b[2]), float(b[3])])\n",
    "                det_scores.append(float(s))\n",
    "                det_corners.append(None)\n",
    "\n",
    "    det_xyxy = np.array(det_boxes, dtype=np.float32) if len(det_boxes) else np.zeros((0, 4), dtype=np.float32)\n",
    "    det_scores = np.array(det_scores, dtype=np.float32) if len(det_scores) else np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 3) Update tracker (multi-object). Select best two tracks.\n",
    "    tracks = tracker.step(det_xyxy, det_scores, det_corners)\n",
    "    selected = tracks[:NUM_TARGETS]\n",
    "\n",
    "    # Optional: if tracks are stale this frame, nudge them by optical flow (display-only)\n",
    "    if FLOW_ENABLE and prev_gray is not None:\n",
    "        for t in selected:\n",
    "            if t.time_since_update > 0:\n",
    "                nudged = flow_nudge(prev_gray, gray, t.box_xyxy)\n",
    "                t.box_xyxy = nudged\n",
    "\n",
    "    # 4) Build outputs for CSV + draw annotations\n",
    "    palps = []\n",
    "    for idx in range(NUM_TARGETS):\n",
    "        if idx < len(selected):\n",
    "            t = selected[idx]\n",
    "            box = t.box_xyxy.astype(np.float32)\n",
    "            cx, cy, bw, bh = xyxy_to_cxcywh(box)\n",
    "            palps.append({\n",
    "                \"track_id\": t.id,\n",
    "                \"x1\": float(box[0]), \"y1\": float(box[1]), \"x2\": float(box[2]), \"y2\": float(box[3]),\n",
    "                \"cx\": float(cx), \"cy\": float(cy),\n",
    "                \"corners\": (t.corners if t.corners is not None else np.nan),\n",
    "                \"age\": int(t.time_since_update),\n",
    "                \"score\": float(t.score),\n",
    "            })\n",
    "\n",
    "            # Draw OBB polygon if available, else draw axis-aligned rectangle\n",
    "            if t.corners is not None and isinstance(t.corners, list) and len(t.corners) == 4:\n",
    "                pts = np.array(t.corners, dtype=np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(frame, [pts], isClosed=True, color=(0, 255, 255), thickness=2)\n",
    "            else:\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (int(box[0]), int(box[1])),\n",
    "                    (int(box[2]), int(box[3])),\n",
    "                    (0, 255, 255),\n",
    "                    2\n",
    "                )\n",
    "\n",
    "            # Draw center\n",
    "            cv2.circle(frame, (int(cx), int(cy)), 1, (0, 255, 255), -1)\n",
    "\n",
    "            # Label\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"palp#{idx+1} id={t.id} age={t.time_since_update}\",\n",
    "                (max(0, int(box[0])), max(20, int(box[1]) - 8)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.3,\n",
    "                (0, 255, 255),\n",
    "                1\n",
    "            )\n",
    "        else:\n",
    "            palps.append({\n",
    "                \"track_id\": np.nan,\n",
    "                \"x1\": np.nan, \"y1\": np.nan, \"x2\": np.nan, \"y2\": np.nan,\n",
    "                \"cx\": np.nan, \"cy\": np.nan,\n",
    "                \"corners\": np.nan,\n",
    "                \"age\": np.nan,\n",
    "                \"score\": np.nan,\n",
    "            })\n",
    "\n",
    "    # 5) Draw line connecting the two centers (if both exist)\n",
    "    distance = np.nan\n",
    "    if not (np.isnan(palps[0][\"cx\"]) or np.isnan(palps[1][\"cx\"])):\n",
    "        p0 = (int(palps[0][\"cx\"]), int(palps[0][\"cy\"]))\n",
    "        p1 = (int(palps[1][\"cx\"]), int(palps[1][\"cy\"]))\n",
    "        cv2.line(frame, p0, p1, (0, 255, 0), 2)\n",
    "        distance = float(np.hypot(palps[0][\"cx\"] - palps[1][\"cx\"], palps[0][\"cy\"] - palps[1][\"cy\"]))\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"dist={distance:.2f}px\",\n",
    "            (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.9,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    # 6) CSV row\n",
    "    row = {\n",
    "        \"frame\": frame_number,\n",
    "        \"timestamp\": current_timestamp,\n",
    "\n",
    "        \"track_id_palp1\": palps[0][\"track_id\"],\n",
    "        \"x1_palp1\": palps[0][\"x1\"], \"y1_palp1\": palps[0][\"y1\"], \"x2_palp1\": palps[0][\"x2\"], \"y2_palp1\": palps[0][\"y2\"],\n",
    "        \"cx_palp1\": palps[0][\"cx\"], \"cy_palp1\": palps[0][\"cy\"],\n",
    "        \"corners_palp1\": str(palps[0][\"corners\"]),\n",
    "        \"age_palp1\": palps[0][\"age\"],\n",
    "        \"score_palp1\": palps[0][\"score\"],\n",
    "\n",
    "        \"track_id_palp2\": palps[1][\"track_id\"],\n",
    "        \"x1_palp2\": palps[1][\"x1\"], \"y1_palp2\": palps[1][\"y1\"], \"x2_palp2\": palps[1][\"x2\"], \"y2_palp2\": palps[1][\"y2\"],\n",
    "        \"cx_palp2\": palps[1][\"cx\"], \"cy_palp2\": palps[1][\"cy\"],\n",
    "        \"corners_palp2\": str(palps[1][\"corners\"]),\n",
    "        \"age_palp2\": palps[1][\"age\"],\n",
    "        \"score_palp2\": palps[1][\"score\"],\n",
    "\n",
    "        \"distance_palp1_palp2_px\": distance,\n",
    "    }\n",
    "\n",
    "    return frame, row, gray\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# MAIN LOOP\n",
    "prev_gray = None\n",
    "\n",
    "individual_fly_folders = [\n",
    "    f for f in os.listdir(main_directory)\n",
    "    if os.path.isdir(os.path.join(main_directory, f))\n",
    "]\n",
    "\n",
    "for fly_folder in individual_fly_folders:\n",
    "    fly_folder_path = os.path.join(main_directory, fly_folder)\n",
    "\n",
    "    video_files = [\n",
    "        f for f in os.listdir(fly_folder_path)\n",
    "        if f.lower().endswith((\".mp4\", \".avi\")) and f.split(\".\")[0]\n",
    "    ]\n",
    "\n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(fly_folder_path, video_file)\n",
    "        video_name = os.path.basename(video_path)\n",
    "        video_base_name = video_name.split(\".\")[0]\n",
    "\n",
    "        csv_file_name = video_base_name.replace(\"_preprocessed\", \"\") + \".csv\"\n",
    "        csv_file_path = os.path.join(fly_folder_path, csv_file_name)\n",
    "\n",
    "        folder_name = \"_\".join(video_base_name.split(\"_\")[1:7])\n",
    "        output_folder = os.path.join(fly_folder_path, folder_name)\n",
    "        output_video_path = os.path.join(output_folder, f\"{folder_name}_palps_annotated_30fps.mp4\")\n",
    "\n",
    "        if os.path.isdir(output_folder):\n",
    "            print(f\"Skipping {video_path} — found existing folder: {output_folder}\")\n",
    "            continue\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video file {video_path}\")\n",
    "            continue\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        max_frame = total_frames - 1\n",
    "\n",
    "        timestamps = {}\n",
    "        inferred_fps_for_timestamp = cap.get(cv2.CAP_PROP_FPS) or OUTPUT_FPS\n",
    "\n",
    "        if os.path.exists(csv_file_path):\n",
    "            df_timestamps = pd.read_csv(csv_file_path)\n",
    "            row_count = len(df_timestamps)\n",
    "            print(f\"Found CSV file {csv_file_path} with {row_count} rows.\")\n",
    "\n",
    "            frame_col = _pick_frame_column(df_timestamps)\n",
    "            if frame_col is not None:\n",
    "                ts_col = _pick_timestamp_column(df_timestamps)\n",
    "                if ts_col is not None:\n",
    "                    secs = _to_seconds_series(df_timestamps, ts_col)\n",
    "                    tmp = pd.DataFrame({\n",
    "                        \"_frame\": pd.to_numeric(df_timestamps[frame_col], errors=\"coerce\"),\n",
    "                        \"seconds\": secs\n",
    "                    }).dropna(subset=[\"_frame\", \"seconds\"])\n",
    "                    tmp[\"_frame\"] = tmp[\"_frame\"].astype(int)\n",
    "                    timestamps = tmp.set_index(\"_frame\")[\"seconds\"].to_dict()\n",
    "\n",
    "                    if not tmp[\"_frame\"].empty:\n",
    "                        max_frame = int(tmp[\"_frame\"].max())\n",
    "\n",
    "                    fps_from_csv = _estimate_fps_from_seconds(secs)\n",
    "                    if fps_from_csv and np.isfinite(fps_from_csv) and fps_from_csv > 0:\n",
    "                        inferred_fps_for_timestamp = float(fps_from_csv)\n",
    "                        print(\"Calculated FPS from CSV timestamps:\", inferred_fps_for_timestamp)\n",
    "                    else:\n",
    "                        inferred_fps_for_timestamp = float(cap.get(cv2.CAP_PROP_FPS) or OUTPUT_FPS)\n",
    "                        print(\"FPS from CSV unavailable; falling back to video FPS for timestamps:\", inferred_fps_for_timestamp)\n",
    "                else:\n",
    "                    print(\"CSV has a frame column but no recognized timestamp column; timestamps will be synthetic.\")\n",
    "            else:\n",
    "                print(\"CSV missing frame column; timestamps will be synthetic.\")\n",
    "        else:\n",
    "            print(f\"CSV file {csv_file_path} not found. Timestamps will be synthetic.\")\n",
    "\n",
    "        # Use OUTPUT_FPS for the *writer* (strict 30 fps output)\n",
    "        writer = None\n",
    "\n",
    "        writer_fps = OUTPUT_FPS\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        \n",
    "        # inside the while loop, right before the first writer.write(...)\n",
    "        \n",
    "        if writer is None:\n",
    "        \n",
    "            out_h, out_w = frame.shape[:2]              # frame is already resized (or not) at this point\n",
    "        \n",
    "            writer = cv2.VideoWriter(output_video_path, fourcc, writer_fps, (out_w, out_h))\n",
    "        \n",
    "            if not writer.isOpened():\n",
    "        \n",
    "                raise RuntimeError(\n",
    "        \n",
    "                    f\"VideoWriter failed to open. \"\n",
    "        \n",
    "                    f\"Path={output_video_path}, fourcc=mp4v, fps={writer_fps}, size={(out_w, out_h)}\"\n",
    "        \n",
    "                )\n",
    "        \n",
    "        writer.write(frame)\n",
    "\n",
    "\n",
    "        all_rows = []\n",
    "\n",
    "        # One tracker for class-1 palps (multi-object)\n",
    "        tracker = MultiObjectSingleClassTracker(iou_thres=IOU_MATCH_THRES, max_age=MAX_AGE)\n",
    "\n",
    "        start_time = time.time()\n",
    "        frame_count = 0\n",
    "        prev_gray = None  # reset per video\n",
    "\n",
    "        while cap.isOpened() and frame_count <= max_frame:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "\n",
    "            # timestamp priority:\n",
    "            # - if CSV provides seconds, use it\n",
    "            # - else use synthetic timestamp consistent with OUTPUT_FPS (since you requested 30 fps)\n",
    "            current_timestamp = timestamps.get(frame_count, frame_count / OUTPUT_FPS)\n",
    "\n",
    "            frame, row, prev_gray = process_frame(\n",
    "                frame=frame,\n",
    "                frame_number=frame_count,\n",
    "                current_timestamp=current_timestamp,\n",
    "                tracker=tracker,\n",
    "                prev_gray=prev_gray\n",
    "            )\n",
    "\n",
    "            writer.write(frame)\n",
    "            all_rows.append(row)\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "\n",
    "        out_csv_path = os.path.join(output_folder, f\"{folder_name}_palps_tracks.csv\")\n",
    "        pd.DataFrame(all_rows).to_csv(out_csv_path, index=False)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed video {video_path} in {elapsed_time:.2f} seconds. Output saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7be56-9ab6-4f85-a046-09e78efa5372",
   "metadata": {},
   "source": [
    "# Fruit Fly Analysis Code Is Below must modify to do what u want distance between class 1 (palp centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456ab00-d8c6-45ad-8ed2-6d13675547ab",
   "metadata": {},
   "source": [
    "# Plots & Data for Distance Between The Eye and Proboscis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65c0372c-7e36-4d8f-95e2-342d9f79349b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No in‑range 'distance' data found in /home/ramanlab/Documents/Arshiya/all_vids/04.15.2025/Testing/HEX/L4.\n",
      "No in‑range 'distance' data found in /home/ramanlab/Documents/Arshiya/all_vids/04.15.2025/Testing/HEX/L3.\n",
      "No in‑range 'distance' data found in /home/ramanlab/Documents/Arshiya/all_vids/04.15.2025/Testing/HEX/L2.\n",
      "No in‑range 'distance' data found in /home/ramanlab/Documents/Arshiya/all_vids/04.15.2025/Testing/HEX/L1.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute global min/max distance values for each fly folder\n",
    "while **ignoring** any distances outside user‑defined hard limits.\n",
    "\n",
    "Workflow\n",
    "========\n",
    "1.  Scan every CSV matching ``*class_2.csv`` in each fly folder.\n",
    "2.  For each file, keep only distances in **[HARD_MIN, HARD_MAX]**.\n",
    "3.  Find that file’s min/max; aggregate to obtain the folder‑level\n",
    "    global min/max.\n",
    "4.  Write results to ``global_distance_stats_class_2.json`` inside the\n",
    "   corresponding fly folder.\n",
    "\n",
    "If *all* values in a file are out of range, the file is skipped.\n",
    "If no files contain in‑range data, the folder is reported and skipped.\n",
    "\n",
    "Edit ``HARD_MIN``, and ``HARD_MAX`` below.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Only values in this inclusive range are considered.\n",
    "HARD_MIN_eye: float = 70    # TODO: 105 for manual 70 for OCTo\n",
    "HARD_MAX_eye: float = 250  # TODO: \n",
    "# --------------------------------------------------------\n",
    "\n",
    "main_directory = Path(main_directory).expanduser().resolve()\n",
    "\n",
    "if not main_directory.is_dir():\n",
    "    raise NotADirectoryError(f\"{main_directory} is not a valid directory\")\n",
    "\n",
    "# Identify each immediate sub‑folder (one per fly)\n",
    "fly_folders = [p for p in main_directory.iterdir() if p.is_dir()]\n",
    "\n",
    "for fly_folder in fly_folders:\n",
    "    pattern = fly_folder / \"**\" / \"*merged.csv\"\n",
    "    csv_files = glob.glob(str(pattern), recursive=True)\n",
    "\n",
    "    global_min = float(\"inf\")\n",
    "    global_max = float(\"-inf\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if \"distance_2_6\" not in df.columns:\n",
    "            print(f\"Skipping {csv_file} — 'distance' column missing.\")\n",
    "            continue\n",
    "\n",
    "        # Keep only values within hard limits\n",
    "        in_range = df[\"distance_2_6\"].between(HARD_MIN_eye, HARD_MAX_eye, inclusive=\"both\")\n",
    "        distances = df.loc[in_range, \"distance_2_6\"]\n",
    "\n",
    "        if distances.empty:\n",
    "            print(f\"All 'distance' values out of range in {csv_file}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        file_min = distances.min()\n",
    "        file_max = distances.max()\n",
    "        global_min = min(global_min, file_min)\n",
    "        global_max = max(global_max, file_max)\n",
    "\n",
    "    # Were any valid values found across all files?\n",
    "    if global_min == float(\"inf\") or global_max == float(\"-inf\"):\n",
    "        print(f\"No in‑range 'distance' data found in {fly_folder}.\")\n",
    "        continue\n",
    "\n",
    "    stats = {\"global_min\": global_min, \"global_max\": global_max}\n",
    "    stats_path = fly_folder / \"global_distance_stats_class_2.json\"\n",
    "    with open(stats_path, \"w\") as f:\n",
    "        json.dump(stats, f)\n",
    "\n",
    "    print(\n",
    "        f\"{fly_folder.name}: min = {global_min}, max = {global_max} \"\n",
    "        f\"(range {HARD_MIN_eye}–{HARD_MAX_eye}). → {stats_path}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36f7a758-e0bd-4884-ab14-98ae0e6401cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No global stats found for L4; skipping.\n",
      "No global stats found for L3; skipping.\n",
      "No global stats found for L2; skipping.\n",
      "No global stats found for L1; skipping.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add distance‑percentage normalisation to every *class_2.csv*, using the\n",
    "folder‑level global min/max produced by *Set Hard Min Max Distance*.\n",
    "\n",
    "Rules\n",
    "-----\n",
    "* If a distance > global_max  → distance_percentage = **101**\n",
    "* If a distance < global_min  → distance_percentage = **-1**\n",
    "* Otherwise                     distance_percentage = 100 · (d − global_min)/(global_max − global_min)\n",
    "\n",
    "The script also writes the reference min/max to each row (columns\n",
    "``min_distance`` and ``max_distance``) for reproducibility.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "main_directory = Path(main_directory).expanduser().resolve()\n",
    "\n",
    "if not main_directory.is_dir():\n",
    "    raise NotADirectoryError(f\"{main_directory} is not a valid directory\")\n",
    "\n",
    "fly_folders = [p for p in main_directory.iterdir() if p.is_dir()]\n",
    "\n",
    "for fly_folder in fly_folders:\n",
    "    stats_path = fly_folder / \"global_distance_stats_class_2.json\"\n",
    "    if not stats_path.exists():\n",
    "        print(f\"No global stats found for {fly_folder.name}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    with open(stats_path, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    global_min = stats[\"global_min\"]\n",
    "    global_max = stats[\"global_max\"]\n",
    "\n",
    "    if global_max == global_min:\n",
    "        print(\n",
    "            f\"Warning: global_max == global_min in {fly_folder.name}; \"\n",
    "            \"distance‑percentage set to 0 for in‑range values.\"\n",
    "        )\n",
    "\n",
    "    pattern = fly_folder / \"**\" / \"*merged.csv\"\n",
    "    csv_files = glob.glob(str(pattern), recursive=True)\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        if \"distance_2_6\" not in df.columns:\n",
    "            print(f\"'distance' column missing in {csv_file}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Add reference columns\n",
    "        df[\"min_distance_2_6\"] = global_min\n",
    "        df[\"max_distance_2_6\"] = global_max\n",
    "\n",
    "        # Vectorised classification\n",
    "        d = df[\"distance_2_6\"].to_numpy()\n",
    "        perc = np.empty_like(d, dtype=float)\n",
    "\n",
    "        over = d > global_max\n",
    "        under = d < global_min\n",
    "        in_range = ~(over | under)\n",
    "\n",
    "        perc[over] = 101.0\n",
    "        perc[under] = -1.0\n",
    "\n",
    "        if global_max != global_min:\n",
    "            perc[in_range] = 100.0 * (d[in_range] - global_min) / (\n",
    "                global_max - global_min\n",
    "            )\n",
    "        else:\n",
    "            perc[in_range] = 0.0  # arbitrary when range is zero\n",
    "\n",
    "        df[\"distance_percentage_2_6\"] = perc\n",
    "\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"Updated {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4266f91-84af-498e-a060-6e91bd68d2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create time-series plots of distance_percentage for each *class_2.csv*,\n",
    "adding \"_time\" to the generated PNG filenames.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "1.  Set ``main_directory`` to the path that contains the individual fly\n",
    "    folders.\n",
    "2.  Ensure ``timestamp_to_seconds`` is defined in the same namespace or\n",
    "    import it from your utilities module.\n",
    "3.  Run the script (e.g. ``python plot_distance_percentage_time.py``).\n",
    "\n",
    "Each plot is saved in the same directory as its CSV with the pattern:\n",
    "``<original-csv-name>_time.png``.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "main_directory = Path(main_directory).expanduser().resolve()\n",
    "\n",
    "fly_folders = [p for p in main_directory.iterdir() if p.is_dir()]\n",
    "\n",
    "for fly_folder in fly_folders:\n",
    "    pattern = fly_folder / \"**\" / \"*merged.csv\"\n",
    "    csv_files = glob.glob(str(pattern), recursive=True)\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        if {\"timestamp\", \"distance_percentage_2_6\"}.issubset(df.columns):\n",
    "            # Convert timestamps → seconds\n",
    "            df[\"time_seconds\"] = df[\"timestamp\"].apply(timestamp_to_seconds)\n",
    "            df = df.dropna(subset=[\"time_seconds\"])\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Skipping {csv_file}: no valid timestamps.\")\n",
    "                continue\n",
    "\n",
    "            # Normalise start-time to 0\n",
    "            df[\"time_seconds\"] -= df[\"time_seconds\"].iloc[0]\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(\n",
    "                df[\"time_seconds\"],\n",
    "                df[\"distance_percentage_2_6\"],\n",
    "                label=\"Normalised Distance %\",\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                markersize=3,\n",
    "            )\n",
    "            plt.xlabel(\"Time (seconds)\")\n",
    "            plt.ylabel(\"Normalised Distance %\")\n",
    "            plt.title(f\"Normalised Distance %\\n{Path(csv_file).name}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Build output filename with \"_time\" suffix\n",
    "            csv_path = Path(csv_file)\n",
    "            plot_file = csv_path.with_suffix(\"\").as_posix() + \"_time.png\"\n",
    "\n",
    "            plt.savefig(plot_file)\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"Plot saved → {plot_file}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Skipping {csv_file}: missing 'timestamp' or 'distance_percentage' columns.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6eff59c2-9212-4b23-9cf0-e3dc990f7f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Identify each fly folder in the main directory\n",
    "fly_folders = [os.path.join(main_directory, f) for f in os.listdir(main_directory)\n",
    "               if os.path.isdir(os.path.join(main_directory, f))]\n",
    "\n",
    "for fly_folder in fly_folders:\n",
    "    # Glob for CSV files that contain \"class_2.csv\" anywhere in the filename.\n",
    "    pattern = os.path.join(fly_folder, \"**\", \"*merged.csv\")\n",
    "    csv_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Strip spaces from column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Check if 'frame' column exists\n",
    "        if 'frame' not in df.columns:\n",
    "            print(f\"Warning: 'frame' column not found in {csv_file}. Skipping file.\")\n",
    "            continue\n",
    "        \n",
    "        # Convert 'distance' column to numeric and check for NaNs or blanks\n",
    "        if 'distance_2_6' in df.columns:\n",
    "            df['distance_2_6'] = pd.to_numeric(df['distance_2_6'], errors='coerce')\n",
    "            dropped_frames = df[df['distance_2_6'].isna()]['frame'].tolist()\n",
    "        else:\n",
    "            dropped_frames = []\n",
    "        \n",
    "        # Get the unique frame numbers present in the CSV\n",
    "        present_frames = sorted(df['frame'].unique())\n",
    "        if not present_frames:\n",
    "            continue  # Skip if no frame numbers are found\n",
    "        \n",
    "        # Determine the full range of expected frame numbers\n",
    "        min_frame = present_frames[0]\n",
    "        max_frame = present_frames[-1]\n",
    "        expected_frames = set(range(min_frame, max_frame + 1))\n",
    "        \n",
    "        # Identify missing frames (i.e., frames not present at all)\n",
    "        missing_frames = sorted(expected_frames - set(present_frames))\n",
    "        \n",
    "        # Combine missing frames and NaN distance frames\n",
    "        all_dropped_frames = sorted(set(missing_frames) | set(dropped_frames))\n",
    "        total_dropped = len(all_dropped_frames)\n",
    "        \n",
    "        # Create a text file with the same base name as the CSV (append _dropped_frames.txt)\n",
    "        txt_file = csv_file.replace(\".csv\", \"_dropped_frames.txt\")\n",
    "        \n",
    "        with open(txt_file, \"w\") as f:\n",
    "            if total_dropped == 0:\n",
    "                f.write(\"No dropped frames found.\\n\")\n",
    "            else:\n",
    "                f.write(\"Dropped frames (missing or NaN distance):\\n\")\n",
    "                for frame in all_dropped_frames:\n",
    "                    f.write(f\"{frame}\\n\")\n",
    "                f.write(f\"\\nTotal dropped frames: {total_dropped}\\n\")\n",
    "        \n",
    "        print(f\"Dropped frames details saved to {txt_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef99d3-1950-4bf5-8b3e-018f1bb61022",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Heatmaps of Eye / Proboscis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25f962-4cac-43fe-b27c-150effd8dc7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Identify each fly folder in the main directory.\n",
    "fly_folders = [os.path.join(main_directory, f) \n",
    "               for f in os.listdir(main_directory) \n",
    "               if os.path.isdir(os.path.join(main_directory, f))]\n",
    "\n",
    "for fly_folder in fly_folders:\n",
    "    # Create the destination folder inside each fly folder.\n",
    "    destination_folder = os.path.join(fly_folder, \"Eye_Prob_Dist\")\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    \n",
    "    # Glob for CSV files that contain \"class_2.csv\" anywhere in the filename.\n",
    "    pattern = os.path.join(fly_folder, \"**\", \"*merged.csv\")\n",
    "    csv_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Skip files that are already in the destination folder to avoid duplicates.\n",
    "        if destination_folder in os.path.dirname(csv_file):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # Keep only the columns: frame, distance_percentage, and timestamp.\n",
    "            df_filtered = df[[\"frame\", \"timestamp\", \"x_class2\", \"y_class2\", \"x_class6\", \"y_class6\", \"distance_percentage_2_6\"]]\n",
    "            # Construct the destination file path with a prefix \"updated_\".\n",
    "            dest_file = os.path.join(destination_folder, \"updated_\" + os.path.basename(csv_file))\n",
    "            # Write the filtered data to a new CSV file.\n",
    "            df_filtered.to_csv(dest_file, index=False)\n",
    "            print(f\"Copied filtered data from {csv_file} to {dest_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {csv_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f020456-8616-4367-b6fa-e9cd69e194df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the main directory.\n",
    "main_dir = main_directory\n",
    "\n",
    "# Iterate over each fly folder in the main directory.\n",
    "for fly_folder in os.listdir(main_dir):\n",
    "    fly_path = os.path.join(main_dir, fly_folder)\n",
    "    if not os.path.isdir(fly_path):\n",
    "        continue\n",
    "\n",
    "    # Define the cvs_class_2 folder within the fly folder.\n",
    "    cvs_class_dir = os.path.join(fly_path, 'Eye_Prob_Dist')\n",
    "    if not (os.path.exists(cvs_class_dir) and os.path.isdir(cvs_class_dir)):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing fly folder: {fly_path}\")\n",
    "\n",
    "    # Process only CSV files in cvs_class_2 that start with \"updated_\"\n",
    "    for file in os.listdir(cvs_class_dir):\n",
    "        if file.endswith('.csv') and file.startswith('updated_'):\n",
    "            updated_csv_path = os.path.join(cvs_class_dir, file)\n",
    "            print(f\"  Found updated CSV: {file}\")\n",
    "\n",
    "            # Extract the identifier from the file name.\n",
    "            base_name = file[len('updated_'):]\n",
    "            tokens = base_name.split('_')\n",
    "            if len(tokens) < 6:\n",
    "                print(f\"    Skipping {file}: not enough tokens to extract identifier.\")\n",
    "                continue\n",
    "            identifier = '_'.join(tokens[:6])\n",
    "            print(f\"    Identifier extracted: {identifier}\")\n",
    "\n",
    "            # Search for the corresponding output CSV in the fly folder.\n",
    "            corresponding_output = None\n",
    "            for f in os.listdir(fly_path):\n",
    "                if f.startswith(\"output_\" + identifier) and f.endswith('.csv'):\n",
    "                    corresponding_output = os.path.join(fly_path, f)\n",
    "                    break\n",
    "\n",
    "            if corresponding_output is None:\n",
    "                print(f\"    No corresponding output file found for identifier {identifier}.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"    Found corresponding output file: {os.path.basename(corresponding_output)}\")\n",
    "\n",
    "            # Load the output CSV to determine odor on/off frames.\n",
    "            df_output = pd.read_csv(corresponding_output)\n",
    "            if \"ActiveOFM\" not in df_output.columns:\n",
    "                print(f\"    File {corresponding_output} does not contain 'Active OFM Pin' column.\")\n",
    "                continue\n",
    "\n",
    "            # Determine frames where odor is on: any value not equal to \"off\"\n",
    "            odor_on_indices = df_output.index[df_output[\"ActiveOFM\"].astype(str) != \"off\"].tolist()\n",
    "            if not odor_on_indices:\n",
    "                print(f\"    No odor on detected in {corresponding_output}.\")\n",
    "                continue\n",
    "\n",
    "            odor_on_first = min(odor_on_indices)\n",
    "            odor_on_last = max(odor_on_indices)\n",
    "            print(f\"    Odor on from frame {odor_on_first} to {odor_on_last}\")\n",
    "\n",
    "            # Load the updated CSV file to update.\n",
    "            df_updated = pd.read_csv(updated_csv_path)\n",
    "            # Use the \"Frame\" column if available; otherwise, use the DataFrame index.\n",
    "            if \"Frame\" in df_updated.columns:\n",
    "                df_updated[\"OFM_State\"] = df_updated[\"Frame\"].apply(\n",
    "                    lambda x: \"before\" if x < odor_on_first else (\"during\" if odor_on_first <= x <= odor_on_last else \"after\")\n",
    "                )\n",
    "            else:\n",
    "                df_updated[\"OFM_State\"] = df_updated.index.map(\n",
    "                    lambda x: \"before\" if x < odor_on_first else (\"during\" if odor_on_first <= x <= odor_on_last else \"after\")\n",
    "                )\n",
    "            \n",
    "            # Overwrite the updated CSV file with the new columns.\n",
    "            df_updated.to_csv(updated_csv_path, index=False)\n",
    "            print(f\"    Overwritten {file} with updated odor state information.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf412e-f3f6-4c9a-8678-def47015ce22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define valid month prefixes\n",
    "MONTHS = [\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\n",
    "]\n",
    "\n",
    "# List all fly folders inside the main directory.\n",
    "fly_folders = [\n",
    "    os.path.join(main_directory, folder)\n",
    "    for folder in os.listdir(main_directory)\n",
    "    if os.path.isdir(os.path.join(main_directory, folder))\n",
    "    and folder.lower().startswith(tuple(MONTHS))  # Only include folders starting with a month\n",
    "]\n",
    "\n",
    "for fly_folder in fly_folders:\n",
    "    fly_name = os.path.basename(fly_folder)\n",
    "    print(f\"Processing fly: {fly_name}\")\n",
    "    \n",
    "    # Define the subfolder containing CSV files.\n",
    "    cvs_folder = os.path.join(fly_folder, \"Eye_Prob_Dist\")\n",
    "    if not os.path.exists(cvs_folder):\n",
    "        print(f\"  No 'Eye_Prob_Dist' folder found in {fly_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create training and testing directories inside the Eye_Prob_Dist folder if they don't exist.\n",
    "    training_dir = os.path.join(cvs_folder, \"training\")\n",
    "    testing_dir = os.path.join(cvs_folder, \"testing\")\n",
    "    os.makedirs(training_dir, exist_ok=True)\n",
    "    os.makedirs(testing_dir, exist_ok=True)\n",
    "\n",
    "    # List all CSV files in the Eye_Prob_Dist folder.\n",
    "    csv_files = [os.path.join(cvs_folder, f) for f in os.listdir(cvs_folder) if f.endswith('.csv')]\n",
    "\n",
    "    # Sort CSV files into training and testing groups.\n",
    "    training_files = sorted([f for f in csv_files if \"training\" in os.path.basename(f).lower()])\n",
    "    testing_files = sorted([f for f in csv_files if \"testing\" in os.path.basename(f).lower()])\n",
    "\n",
    "    # Move the training files to the training directory.\n",
    "    print(\"  Moving Training CSV Files:\")\n",
    "    for file in training_files:\n",
    "        destination = os.path.join(training_dir, os.path.basename(file))\n",
    "        print(f\"    Moving {file} to {destination}\")\n",
    "        shutil.move(file, destination)\n",
    "\n",
    "    # Move the testing files to the testing directory.\n",
    "    print(\"  Moving Testing CSV Files:\")\n",
    "    for file in testing_files:\n",
    "        destination = os.path.join(testing_dir, os.path.basename(file))\n",
    "        print(f\"    Moving {file} to {destination}\")\n",
    "        shutil.move(file, destination)\n",
    "\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5005fcf8-0080-485d-899c-13498e619189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import os                       # ← already used later; kept for completeness\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1.  Matplotlib defaults (unchanged)\n",
    "# --------------------------------------------------------------------------\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'lines.linewidth': 2,\n",
    "    'axes.linewidth': 1.5,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'legend.fontsize': 12\n",
    "})\n",
    "\n",
    "def log_tick_formatter(x, pos):\n",
    "    original = np.expm1(x)\n",
    "    return f\"{original:.0f}\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.  Locate fly folders (unchanged)\n",
    "# --------------------------------------------------------------------------\n",
    "fly_folders = [os.path.join(main_directory, f) for f in os.listdir(main_directory)\n",
    "               if os.path.isdir(os.path.join(main_directory, f))]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.  Main loop (only the sections marked ➊–➌ differ from your original code)\n",
    "# --------------------------------------------------------------------------\n",
    "for fly_folder in fly_folders:\n",
    "    fly_name = os.path.basename(fly_folder)\n",
    "    cvs_folder = os.path.join(fly_folder, \"Eye_Prob_Dist\")\n",
    "    if not os.path.exists(cvs_folder):\n",
    "        continue\n",
    "\n",
    "    heat_maps_folder = os.path.join(cvs_folder, \"heat_maps\")\n",
    "    os.makedirs(heat_maps_folder, exist_ok=True)\n",
    "\n",
    "    import re\n",
    "\n",
    "    def trial_index(name: str, category: str) -> int:\n",
    "        \"\"\"\n",
    "        Return the numeric trial id that follows '{category}_' in the filename.\n",
    "        Falls back to the last number in the name if no match is found.\n",
    "        \"\"\"\n",
    "        m = re.search(fr'_{category}_(\\d+)\\b', name)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        # Fallback: use the last number in the string\n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        return int(nums[-1]) if nums else float('inf')\n",
    "    \n",
    "    for category in [\"training\", \"testing\"]:\n",
    "        category_folder = os.path.join(cvs_folder, category)\n",
    "        csv_files = [\n",
    "            os.path.join(category_folder, f) for f in os.listdir(category_folder)\n",
    "            if f.startswith(\"updated\") and f.endswith(\".csv\")\n",
    "        ]\n",
    "        # ✅ Sort by the trial number, so 1…9 come before 10\n",
    "        csv_files = sorted(csv_files, key=lambda p: trial_index(os.path.basename(p), category))\n",
    "\n",
    "        if not csv_files:\n",
    "            continue\n",
    "\n",
    "        trials = []\n",
    "        for csv_file in csv_files:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df.columns = df.columns.str.strip()\n",
    "            if 'timestamp' not in df or 'distance_percentage_2_6' not in df or \"OFM_State\" not in df or \"frame\" not in df:\n",
    "                continue\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Timestamp processing (unchanged)\n",
    "            # ------------------------------------------------------------------\n",
    "            df['time_seconds'] = df['timestamp'].apply(timestamp_to_seconds)\n",
    "            df.dropna(subset=['time_seconds'], inplace=True)\n",
    "            if df.empty or len(df) < 2:\n",
    "                continue\n",
    "            df['relative_time'] = df['time_seconds'] - df['time_seconds'].iloc[0]\n",
    "\n",
    "            odor_df = df[df[\"OFM_State\"].str.lower() == \"during\"]\n",
    "            if odor_df.empty:\n",
    "                continue\n",
    "            odor_onset = odor_df['relative_time'].iloc[0]\n",
    "            odor_offset = odor_df['relative_time'].iloc[-1]\n",
    "\n",
    "            trial_label = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "            if category == \"testing\" or (category == \"training\" and not any(x in trial_label \n",
    "                                                                              for x in [\"training_5\", \"training_6\", \"training_7\", \"training_8\"])):\n",
    "                odor_duration = 30.0\n",
    "            else:\n",
    "                odor_duration = odor_offset - odor_onset\n",
    "\n",
    "            final_total_duration = 30 + odor_duration + 90\n",
    "\n",
    "            total_frames = np.arange(df['frame'].min(), df['frame'].max() + 1)\n",
    "            full_data = np.full_like(total_frames, np.nan, dtype=float)\n",
    "            frame_indices = np.searchsorted(total_frames, df['frame'].values)\n",
    "            full_data[frame_indices] = df['distance_percentage_2_6'].values\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # ➊ Flag special values so they render in red\n",
    "            # ------------------------------------------------------------------\n",
    "            data_for_plot = full_data.copy()\n",
    "            data_for_plot[data_for_plot == -1] = -0.5   # below vmin → ‘under’ colour\n",
    "            data_for_plot[data_for_plot == 101] = 101    # above vmax → ‘over’ colour\n",
    "\n",
    "            new_time = np.linspace(0, final_total_duration, len(total_frames))\n",
    "\n",
    "            trials.append({\n",
    "                'label': trial_label,\n",
    "                'time': new_time,\n",
    "                'data': data_for_plot,\n",
    "                'odor_start': 30,\n",
    "                'odor_end': 30 + odor_duration\n",
    "            })\n",
    "\n",
    "        if not trials:\n",
    "            continue\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # ➋ Create a copy of the viridis colormap and paint extremes red\n",
    "        # ------------------------------------------------------------------\n",
    "        # --- colormap setup -------------------------------------------------------\n",
    "        cmap = mpl.colormaps['viridis'].copy()\n",
    "        cmap.set_under('pink')\n",
    "        cmap.set_over('pink')\n",
    "        cmap.set_bad('dimgray')          # ← NEW: NaNs (blank frames) show as dark-gray\n",
    "\n",
    "        # Normalise on log1p scale (unchanged)\n",
    "        norm = Normalize(vmin=np.log1p(0), vmax=np.log1p(100))\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Plotting (minor edits at pcm and colour-bar lines only)\n",
    "        # ------------------------------------------------------------------\n",
    "        n_trials = len(trials)\n",
    "        fig, axs = plt.subplots(n_trials, 1, figsize=(18, 2 * n_trials), sharex=True)\n",
    "        if n_trials == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        legend_handles = [Line2D([0], [0], color='red', linewidth=2.5, linestyle='-', label='Odor Period')]\n",
    "\n",
    "        for i, trial in enumerate(trials):\n",
    "            ax = axs[i]\n",
    "            time_edges = np.linspace(trial['time'][0], trial['time'][-1], len(trial['time']) + 1)\n",
    "            X, Y = np.meshgrid(time_edges, [0, 1])\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # ➌ Use the updated data array and custom cmap\n",
    "            # ------------------------------------------------------------------\n",
    "            data_row = np.log1p(trial['data']).reshape(1, -1)\n",
    "            pcm = ax.pcolormesh(X, Y, data_row, cmap=cmap, shading='auto', norm=norm)\n",
    "\n",
    "            ax.set_yticks([])\n",
    "            ax.tick_params(axis='x', direction='out')\n",
    "            ax.set_xlim(trial['time'][0], trial['time'][-1])\n",
    "            ax.set_title(trial['label'], loc='left')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.axvline(trial['odor_start'], color='red', linewidth=2.5)\n",
    "            ax.axvline(trial['odor_end'], color='red', linewidth=2.5)\n",
    "\n",
    "        axs[-1].set_xlabel(\"Time (seconds)\")\n",
    "        fig.suptitle(f\"{fly_name} - {category.capitalize()} Trials\\nLog-Transformed (log1p) Heatmaps\", fontsize=20)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # ➍ Enable arrows on the colour-bar so users can see the red extremes\n",
    "        # ------------------------------------------------------------------\n",
    "        cbar = fig.colorbar(pcm, ax=axs, orientation='vertical',\n",
    "                            fraction=0.02, pad=0.04, extend='both')\n",
    "        cbar.set_label(\"Distance Percentage\", fontsize=14)\n",
    "        cbar.ax.yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "        fig.legend(handles=legend_handles, loc='upper right', frameon=True)\n",
    "\n",
    "        out_path = os.path.join(heat_maps_folder, f\"{fly_name}_{category}_heatmap_log.png\")\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Heatmap saved for fly {fly_name} in category {category}: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533c7ad-4ab3-4b09-b650-e179b5c72b13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import Normalize\n",
    "import os\n",
    "# --------------------------------------------------------------------------\n",
    "# Matplotlib defaults\n",
    "# --------------------------------------------------------------------------\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'lines.linewidth': 2,\n",
    "    'axes.linewidth': 1.5,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'legend.fontsize': 12\n",
    "})\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Locate fly folders\n",
    "# --------------------------------------------------------------------------\n",
    "fly_folders = [os.path.join(main_directory, f) for f in os.listdir(main_directory)\n",
    "               if os.path.isdir(os.path.join(main_directory, f))]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Main loop\n",
    "# --------------------------------------------------------------------------\n",
    "for fly_folder in fly_folders:\n",
    "    fly_name = os.path.basename(fly_folder)\n",
    "    cvs_folder = os.path.join(fly_folder, \"Eye_Prob_Dist\")\n",
    "    if not os.path.exists(cvs_folder):\n",
    "        continue\n",
    "\n",
    "    heat_maps_folder = os.path.join(cvs_folder, \"heat_maps\")\n",
    "    os.makedirs(heat_maps_folder, exist_ok=True)\n",
    "\n",
    "    import re\n",
    "\n",
    "    def trial_index(name: str, category: str) -> int:\n",
    "        \"\"\"\n",
    "        Return the numeric trial id that follows '{category}_' in the filename.\n",
    "        Falls back to the last number in the name if no match is found.\n",
    "        \"\"\"\n",
    "        m = re.search(fr'_{category}_(\\d+)\\b', name)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        # Fallback: use the last number in the string\n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        return int(nums[-1]) if nums else float('inf')\n",
    "    \n",
    "    for category in [\"training\", \"testing\"]:\n",
    "        category_folder = os.path.join(cvs_folder, category)\n",
    "        csv_files = [\n",
    "            os.path.join(category_folder, f) for f in os.listdir(category_folder)\n",
    "            if f.startswith(\"updated\") and f.endswith(\".csv\")\n",
    "        ]\n",
    "        # ✅ Sort by the trial number, so 1…9 come before 10\n",
    "        csv_files = sorted(csv_files, key=lambda p: trial_index(os.path.basename(p), category))\n",
    "\n",
    "        if not csv_files:\n",
    "            continue\n",
    "\n",
    "        trials = []\n",
    "        for csv_file in csv_files:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df.columns = df.columns.str.strip()\n",
    "            if {'timestamp', 'distance_percentage_2_6', 'OFM_State', 'frame'} - set(df.columns):\n",
    "                continue\n",
    "\n",
    "            # --- timestamp handling ------------------------------------------------\n",
    "            df['time_seconds'] = df['timestamp'].apply(timestamp_to_seconds)\n",
    "            df.dropna(subset=['time_seconds'], inplace=True)\n",
    "            if df.empty or len(df) < 2:\n",
    "                continue\n",
    "            df['relative_time'] = df['time_seconds'] - df['time_seconds'].iloc[0]\n",
    "\n",
    "            odor_df = df[df[\"OFM_State\"].str.lower() == \"during\"]\n",
    "            if odor_df.empty:\n",
    "                continue\n",
    "            odor_onset = odor_df['relative_time'].iloc[0]\n",
    "            odor_offset = odor_df['relative_time'].iloc[-1]\n",
    "\n",
    "            trial_label = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "            if category == \"testing\" or (\n",
    "                category == \"training\"\n",
    "                and not any(x in trial_label for x in [\"training_5\", \"training_6\", \"training_7\", \"training_8\"])\n",
    "            ):\n",
    "                odor_duration = 30.0\n",
    "            else:\n",
    "                odor_duration = odor_offset - odor_onset\n",
    "\n",
    "            final_total_duration = 30 + odor_duration + 90\n",
    "\n",
    "            total_frames = np.arange(df['frame'].min(), df['frame'].max() + 1)\n",
    "            full_data = np.full_like(total_frames, np.nan, dtype=float)\n",
    "            frame_indices = np.searchsorted(total_frames, df['frame'].values)\n",
    "            full_data[frame_indices] = df['distance_percentage_2_6'].values\n",
    "\n",
    "            # --- flag sentinel values for red colouring ---------------------------\n",
    "            data_for_plot = full_data.copy()\n",
    "            data_for_plot[data_for_plot == -1] = -0.5   # below vmin\n",
    "            data_for_plot[data_for_plot == 101] = 101    # above vmax\n",
    "\n",
    "            new_time = np.linspace(0, final_total_duration, len(total_frames))\n",
    "\n",
    "            trials.append({\n",
    "                'label': trial_label,\n",
    "                'time': new_time,\n",
    "                'data': data_for_plot,\n",
    "                'odor_start': 30,\n",
    "                'odor_end': 30 + odor_duration\n",
    "            })\n",
    "\n",
    "        if not trials:\n",
    "            continue\n",
    "\n",
    "        # --- colormap setup -------------------------------------------------------\n",
    "        cmap = mpl.colormaps['viridis'].copy()\n",
    "        cmap.set_under('pink')\n",
    "        cmap.set_over('pink')\n",
    "        cmap.set_bad('dimgray')          # ← NEW: NaNs (blank frames) show as dark-gray\n",
    "\n",
    "        # linear normalisation 0–100 %\n",
    "        norm = Normalize(vmin=0, vmax=100)\n",
    "\n",
    "        # --- plotting ------------------------------------------------------------\n",
    "        n_trials = len(trials)\n",
    "        fig, axs = plt.subplots(n_trials, 1, figsize=(18, 2 * n_trials), sharex=True)\n",
    "        if n_trials == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        legend_handles = [Line2D([0], [0], color='red', linewidth=2.5, label='Odor Period')]\n",
    "\n",
    "        for i, trial in enumerate(trials):\n",
    "            ax = axs[i]\n",
    "            time_edges = np.linspace(trial['time'][0], trial['time'][-1], len(trial['time']) + 1)\n",
    "            X, Y = np.meshgrid(time_edges, [0, 1])\n",
    "\n",
    "            data_row = trial['data'].reshape(1, -1)\n",
    "            pcm = ax.pcolormesh(X, Y, data_row, cmap=cmap, shading='auto', norm=norm)\n",
    "\n",
    "            ax.set_yticks([])\n",
    "            ax.tick_params(axis='x', direction='out')\n",
    "            ax.set_xlim(trial['time'][0], trial['time'][-1])\n",
    "            ax.set_title(trial['label'], loc='left')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.axvline(trial['odor_start'], color='red', linewidth=2.5)\n",
    "            ax.axvline(trial['odor_end'], color='red', linewidth=2.5)\n",
    "\n",
    "        axs[-1].set_xlabel(\"Time (seconds)\")\n",
    "        fig.suptitle(f\"{fly_name} – {category.capitalize()} Trials\", fontsize=20)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "        cbar = fig.colorbar(pcm, ax=axs, orientation='vertical',\n",
    "                            fraction=0.02, pad=0.04, extend='both')\n",
    "        cbar.set_label(\"Distance Percentage (%)\", fontsize=14)\n",
    "\n",
    "        fig.legend(handles=legend_handles, loc='upper right', frameon=True)\n",
    "\n",
    "        out_path = os.path.join(heat_maps_folder, f\"{fly_name}_{category}_heatmap.png\")\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Heatmap saved for fly {fly_name} in category {category}: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8f3a2c9-bc50-48c6-b2ab-49727d391207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# REQUIRED: main_directory must already be defined.\n",
    "global_heatmap_folder = os.path.join(main_directory, \"heat_maps\")\n",
    "\n",
    "# Create the global heat_maps folder if missing\n",
    "os.makedirs(global_heatmap_folder, exist_ok=True)\n",
    "\n",
    "# Create Eye_Prob_Dist if missing\n",
    "eye_prob_dist_dir = os.path.join(global_heatmap_folder, \"Eye_Prob_Dist\")\n",
    "os.makedirs(eye_prob_dist_dir, exist_ok=True)\n",
    "\n",
    "abs_dest_root = os.path.abspath(global_heatmap_folder)\n",
    "\n",
    "for root, _, files in os.walk(main_directory, topdown=True):\n",
    "    # 1) Skip the destination tree to avoid copying into itself\n",
    "    if os.path.abspath(root).startswith(abs_dest_root):\n",
    "        continue\n",
    "\n",
    "    # 2) Only gather from source heat_maps folders\n",
    "    if \"heat_maps\" not in root:\n",
    "        continue\n",
    "\n",
    "    for file in files:\n",
    "        if not (file.endswith(\".png\") and \"heatmap\" in file):\n",
    "            continue\n",
    "\n",
    "        src_path = os.path.join(root, file)\n",
    "        dest_path = os.path.join(eye_prob_dist_dir, file)\n",
    "\n",
    "        # Skip if source already equals destination (extra safety)\n",
    "        if os.path.abspath(src_path) == os.path.abspath(dest_path):\n",
    "            continue\n",
    "\n",
    "        # Overwrite existing files with the same name\n",
    "        shutil.copy2(src_path, dest_path)\n",
    "        print(f\"Copied (overwritten if existed): {src_path} → {dest_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c7fe7-50a0-4358-8930-d201e5d8fc3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Envelope RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67560826-9b10-44c7-9b40-ad8d6f7b30ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Identify each fly folder in the main directory.\n",
    "fly_folders = [os.path.join(main_directory, f) \n",
    "               for f in os.listdir(main_directory) \n",
    "               if os.path.isdir(os.path.join(main_directory, f))]\n",
    "\n",
    "for fly_folder in fly_folders:\n",
    "    # Create the destination folder inside each fly folder.\n",
    "    destination_folder = os.path.join(fly_folder, \"RMS_calculations\")\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    \n",
    "    # Glob for CSV files that contain \"class_2.csv\" anywhere in the filename.\n",
    "    pattern = os.path.join(fly_folder, \"**\", \"*merged.csv\")\n",
    "    csv_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Skip files that are already in the destination folder to avoid duplicates.\n",
    "        if destination_folder in os.path.dirname(csv_file):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # Keep only the columns: frame, distance_percentage, and timestamp.\n",
    "            df_filtered = df[[\"frame\", \"timestamp\", \"x_class2\", \"y_class2\", \"x_class6\", \"y_class6\", \"distance_percentage_2_6\"]]\n",
    "            # Construct the destination file path with a prefix \"updated_\".\n",
    "            dest_file = os.path.join(destination_folder, \"updated_\" + os.path.basename(csv_file))\n",
    "            # Write the filtered data to a new CSV file.\n",
    "            df_filtered.to_csv(dest_file, index=False)\n",
    "            print(f\"Copied filtered data from {csv_file} to {dest_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {csv_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10abf95-2bef-454d-a7bb-dc38f716b31e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the main directory.\n",
    "main_dir = main_directory\n",
    "\n",
    "# Iterate over each fly folder in the main directory.\n",
    "for fly_folder in os.listdir(main_dir):\n",
    "    fly_path = os.path.join(main_dir, fly_folder)\n",
    "    if not os.path.isdir(fly_path):\n",
    "        continue\n",
    "\n",
    "    # Define the cvs_class_2 folder within the fly folder.\n",
    "    cvs_class_dir = os.path.join(fly_path, 'RMS_calculations')\n",
    "    if not (os.path.exists(cvs_class_dir) and os.path.isdir(cvs_class_dir)):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing fly folder: {fly_path}\")\n",
    "\n",
    "    # Process only CSV files in cvs_class_2 that start with \"updated_\"\n",
    "    for file in os.listdir(cvs_class_dir):\n",
    "        if file.endswith('.csv') and file.startswith('updated_'):\n",
    "            updated_csv_path = os.path.join(cvs_class_dir, file)\n",
    "            print(f\"  Found updated CSV: {file}\")\n",
    "\n",
    "            # Extract the identifier from the file name.\n",
    "            base_name = file[len('updated_'):]\n",
    "            tokens = base_name.split('_')\n",
    "            if len(tokens) < 6:\n",
    "                print(f\"    Skipping {file}: not enough tokens to extract identifier.\")\n",
    "                continue\n",
    "            identifier = '_'.join(tokens[:6])\n",
    "            print(f\"    Identifier extracted: {identifier}\")\n",
    "\n",
    "            # Search for the corresponding output CSV in the fly folder.\n",
    "            corresponding_output = None\n",
    "            for f in os.listdir(fly_path):\n",
    "                if f.startswith(\"output_\" + identifier) and f.endswith('.csv'):\n",
    "                    corresponding_output = os.path.join(fly_path, f)\n",
    "                    break\n",
    "\n",
    "            if corresponding_output is None:\n",
    "                print(f\"    No corresponding output file found for identifier {identifier}.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"    Found corresponding output file: {os.path.basename(corresponding_output)}\")\n",
    "\n",
    "            # Load the output CSV to determine odor on/off frames.\n",
    "            df_output = pd.read_csv(corresponding_output)\n",
    "            if \"ActiveOFM\" not in df_output.columns:\n",
    "                print(f\"    File {corresponding_output} does not contain 'Active OFM Pin' column.\")\n",
    "                continue\n",
    "\n",
    "            # Determine frames where odor is on: any value not equal to \"off\"\n",
    "            odor_on_indices = df_output.index[df_output[\"ActiveOFM\"].astype(str) != \"off\"].tolist()\n",
    "            if not odor_on_indices:\n",
    "                print(f\"    No odor on detected in {corresponding_output}.\")\n",
    "                continue\n",
    "\n",
    "            odor_on_first = min(odor_on_indices)\n",
    "            odor_on_last = max(odor_on_indices)\n",
    "            print(f\"    Odor on from frame {odor_on_first} to {odor_on_last}\")\n",
    "\n",
    "            # Load the updated CSV file to update.\n",
    "            df_updated = pd.read_csv(updated_csv_path)\n",
    "            # Use the \"Frame\" column if available; otherwise, use the DataFrame index.\n",
    "            if \"Frame\" in df_updated.columns:\n",
    "                df_updated[\"OFM_State\"] = df_updated[\"Frame\"].apply(\n",
    "                    lambda x: \"before\" if x < odor_on_first else (\"during\" if odor_on_first <= x <= odor_on_last else \"after\")\n",
    "                )\n",
    "            else:\n",
    "                df_updated[\"OFM_State\"] = df_updated.index.map(\n",
    "                    lambda x: \"before\" if x < odor_on_first else (\"during\" if odor_on_first <= x <= odor_on_last else \"after\")\n",
    "                )\n",
    "            \n",
    "            # Overwrite the updated CSV file with the new columns.\n",
    "            df_updated.to_csv(updated_csv_path, index=False)\n",
    "            print(f\"    Overwritten {file} with updated odor state information.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b510ea-5154-4a64-a8eb-c84bc5363aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# fly_distance_histograms.py — v12b (Notebook-ready): Segment-wise RMS ratio\n",
    "# Accepts `time_seconds` or `timestamp` (numeric/string, HH:MM:SS(:MS), or ISO datetime)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "DEFAULT_MAIN_DIRECTORY = main_directory\n",
    "\n",
    "PHASE_ALIASES: Dict[str, str] = {\n",
    "    \"before\": \"before\", \"pre\": \"before\", \"baseline\": \"before\",\n",
    "    \"during\": \"during\", \"on\": \"during\", \"odor_on\": \"during\",\n",
    "    \"after\": \"after\", \"post\": \"after\", \"odor_off\": \"after\",\n",
    "}\n",
    "\n",
    "MEASURE_COLS    = [\"distance_percentage_2_6\", \"distance_percentage\"]\n",
    "BAR_EDGE_COLOR  = \"black\"\n",
    "FIGSIZE         = (9, 5)\n",
    "THRESHOLD       = 1.1\n",
    "FPS_DEFAULT     = 40\n",
    "WINDOW_SEC      = 0.5\n",
    "OUT_FIG_DIR     = \"RMS_calculations/histograms\"\n",
    "TESTING_REGEX   = re.compile(r\"testing_\\d+\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# HELPERS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _resolve_ofm_column(df: pd.DataFrame) -> str:\n",
    "    for cand in [\"OFM State\", \"OFM_State\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise KeyError(f\"OFM State column not found: {list(df.columns)}\")\n",
    "\n",
    "def _resolve_measure_column(df: pd.DataFrame) -> str:\n",
    "    for cand in MEASURE_COLS:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise KeyError(f\"Measure column not found. Expected {MEASURE_COLS}, got {list(df.columns)}\")\n",
    "\n",
    "def _normalize_state(val) -> str:\n",
    "    if not isinstance(val, str):\n",
    "        raise TypeError(f\"State value is not a string: {val}\")\n",
    "    key = val.strip().lower()\n",
    "    if key not in PHASE_ALIASES:\n",
    "        raise ValueError(f\"Unknown OFM state '{val}' encountered\")\n",
    "    return PHASE_ALIASES[key]\n",
    "\n",
    "def _extract_trial_label(stem: str) -> str:\n",
    "    m = TESTING_REGEX.search(stem)\n",
    "    return m.group(0) if m else stem\n",
    "\n",
    "def _resolve_time_seconds(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns seconds-from-start as float Series.\n",
    "    Accepts:\n",
    "      - time_seconds (numeric or numeric-looking strings)\n",
    "      - timestamp: numeric or numeric-looking strings (sec/ms), ISO datetimes,\n",
    "                   'HH:MM:SS', or 'HH:MM:SS:MS'.\n",
    "    \"\"\"\n",
    "    # Helper: numeric coercion if column is numeric-like strings\n",
    "    def _coerce_numeric(series: pd.Series) -> pd.Series | None:\n",
    "        num = pd.to_numeric(series, errors=\"coerce\")\n",
    "        # Accept if majority are numeric\n",
    "        if num.notna().sum() >= max(3, int(0.8 * len(num))):\n",
    "            return num.astype(float)\n",
    "        return None\n",
    "\n",
    "    if \"time_seconds\" in df.columns:\n",
    "        ts = _coerce_numeric(df[\"time_seconds\"])\n",
    "        if ts is None:\n",
    "            raise ValueError(\"time_seconds present but not numeric/numeric-like.\")\n",
    "        ts = ts - ts.iloc[0]\n",
    "        return ts\n",
    "\n",
    "    if \"timestamp\" in df.columns:\n",
    "        col = df[\"timestamp\"]\n",
    "\n",
    "        # 1) Try numeric-like first (handles \"0.0\", \"25\", etc.)\n",
    "        num = _coerce_numeric(col)\n",
    "        if num is not None:\n",
    "            ts = num - num.iloc[0]\n",
    "            # Infer ms vs s using median delta\n",
    "            dt = ts.diff().median()\n",
    "            if pd.isna(dt) or dt <= 0:\n",
    "                # Bad or zero diffs → assume ms to be safe\n",
    "                ts = ts / 1000.0\n",
    "            elif dt > 5:  # Typical ms steps (e.g., 25 for 40 FPS)\n",
    "                ts = ts / 1000.0\n",
    "            return ts\n",
    "\n",
    "        # 2) Try pandas datetime parsing (errors=coerce to avoid warnings)\n",
    "        dt_series = pd.to_datetime(col, errors=\"coerce\", utc=False)\n",
    "        if dt_series.notna().any():\n",
    "            base = dt_series[dt_series.notna()].iloc[0]\n",
    "            ts = (dt_series - base).dt.total_seconds()\n",
    "            # Fill any NaT rows by forward/backward fill\n",
    "            ts = ts.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "            return ts\n",
    "\n",
    "        # 3) Custom clock-format parsing\n",
    "        def _parse_clock(s) -> float | np.nan:\n",
    "            s = str(s).strip()\n",
    "            if s == \"\" or s.lower() == \"nan\":\n",
    "                return np.nan\n",
    "            if s.count(\":\") == 3:  # HH:MM:SS:MS\n",
    "                try:\n",
    "                    h, m, sec, ms = s.split(\":\")\n",
    "                    return int(h)*3600 + int(m)*60 + float(sec) + float(ms)/1000.0\n",
    "                except Exception:\n",
    "                    return np.nan\n",
    "            if s.count(\":\") == 2:  # HH:MM:SS\n",
    "                try:\n",
    "                    h, m, sec = s.split(\":\")\n",
    "                    return int(h)*3600 + int(m)*60 + float(sec)\n",
    "                except Exception:\n",
    "                    return np.nan\n",
    "            # Fallback: plain float seconds if looks like \"0.0\"\n",
    "            try:\n",
    "                return float(s)\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "\n",
    "        ts = col.map(_parse_clock)\n",
    "        if ts.notna().sum() == 0:\n",
    "            raise KeyError(\"Unable to parse 'timestamp' into seconds.\")\n",
    "        ts = ts - ts.dropna().iloc[0]\n",
    "        return ts\n",
    "\n",
    "    raise KeyError(\"Neither 'time_seconds' nor 'timestamp' column found.\")\n",
    "\n",
    "def compute_burst_metrics(series: pd.Series, fps: float, win_s: float = WINDOW_SEC) -> float:\n",
    "    w = max(int(round(win_s * fps)), 1)\n",
    "    rms = series.pow(2).rolling(w, center=True).mean().pipe(np.sqrt)\n",
    "    return rms.max(skipna=True)\n",
    "\n",
    "def compute_segment_rms(series: pd.Series) -> float:\n",
    "    if series.empty:\n",
    "        raise ValueError(\"Empty series for segment RMS\")\n",
    "    return float(np.sqrt(np.mean(series**2)))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PLOTTING\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def plot_grouped_bar_chart(\n",
    "    fly_name: str,\n",
    "    data: List[Tuple[str, Dict[str, float]]],\n",
    "    out_path: Path\n",
    "):\n",
    "    labels      = [t for t, _ in data]\n",
    "    during_vals = [d[\"during\"] for _, d in data]\n",
    "    after_vals  = [d[\"after\"]  for _, d in data]\n",
    "    x           = np.arange(len(labels))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.bar(x - 0.35, during_vals, width=0.35,\n",
    "           edgecolor=BAR_EDGE_COLOR,\n",
    "           color=[\"red\" if v < THRESHOLD else \"green\" for v in during_vals])\n",
    "    ax.bar(x + 0.00, after_vals,  width=0.35,\n",
    "           edgecolor=BAR_EDGE_COLOR,\n",
    "           color=[\"red\" if v < THRESHOLD else \"green\" for v in after_vals])\n",
    "    for bar in ax.patches:\n",
    "        ax.text(bar.get_x()+bar.get_width()/2,\n",
    "                bar.get_height()+0.02,\n",
    "                f\"{bar.get_height():.2f}\", ha=\"center\", va=\"bottom\")\n",
    "    ax.axhline(THRESHOLD, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Mean ratio (segment ∕ baseline)\")\n",
    "    ax.set_title(f\"Fly: {fly_name} — During & After Ratios\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_rms_ratio_chart(\n",
    "    fly_name: str,\n",
    "    data: List[Tuple[str, float]],\n",
    "    out_path: Path\n",
    "):\n",
    "    labels, values = zip(*data) if data else ([], [])\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.bar(labels, values, edgecolor=BAR_EDGE_COLOR)\n",
    "    for bar in ax.patches:\n",
    "        ax.text(bar.get_x()+bar.get_width()/2,\n",
    "                bar.get_height()+0.02,\n",
    "                f\"{bar.get_height():.2f}\", ha=\"center\", va=\"bottom\")\n",
    "    ax.set_ylabel(\"RMS ratio ((d+a)/b)\")\n",
    "    ax.set_title(f\"Fly: {fly_name} — RMS Ratio per Trial\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN PIPELINE\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def process_fly(fly_folder: Path, *, verbose: bool = False):\n",
    "    # Only process flies that actually have RMS_calculations\n",
    "    testing_dir = fly_folder / \"RMS_calculations\"\n",
    "    csv_paths = sorted(testing_dir.glob(\"*testing*.csv\"))\n",
    "    if not csv_paths:\n",
    "        return  # silent skip (avoids noisy 'invalid month prefix' messages)\n",
    "\n",
    "    # Determine FPS from time_seconds or timestamp\n",
    "    df0 = pd.read_csv(csv_paths[0])\n",
    "    ts0 = _resolve_time_seconds(df0)\n",
    "    dt = ts0.diff().median()\n",
    "    fps_est = (1.0 / dt) if (pd.notna(dt) and dt > 0) else FPS_DEFAULT\n",
    "\n",
    "    ratio_data: List[Tuple[str, Dict[str, float]]] = []\n",
    "    rms_ratio_data: List[Tuple[str, float]] = []\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        trial = _extract_trial_label(path.stem)\n",
    "        meas = _resolve_measure_column(df)\n",
    "        state = _resolve_ofm_column(df)\n",
    "\n",
    "        # ✅ Ensure numeric and drop out-of-range values (>100% or <0)\n",
    "        df[meas] = pd.to_numeric(df[meas], errors=\"coerce\")\n",
    "        clean = df.loc[df[meas].between(0, 100, inclusive=\"both\")].copy()\n",
    "        if clean.empty:\n",
    "            if verbose:\n",
    "                print(f\"{trial}: no valid {meas} values in [0, 100]; skipped.\")\n",
    "            continue\n",
    "\n",
    "        clean[\"phase\"] = clean[state].apply(_normalize_state)\n",
    "\n",
    "        baseline = clean.loc[clean.phase == \"before\", meas]\n",
    "        if baseline.empty:\n",
    "            continue\n",
    "        mean_base = baseline.mean()\n",
    "        clean[\"ratio\"] = clean[meas] / mean_base\n",
    "\n",
    "        d_val = clean.loc[clean.phase == \"during\", \"ratio\"].mean()\n",
    "        a_val = clean.loc[clean.phase == \"after\",  \"ratio\"].mean()\n",
    "        ratio_data.append((trial, {\"during\": d_val, \"after\": a_val}))\n",
    "\n",
    "        rms_b = compute_segment_rms(clean.loc[clean.phase == \"before\", \"ratio\"])\n",
    "        rms_d = compute_segment_rms(clean.loc[clean.phase == \"during\", \"ratio\"])\n",
    "        rms_a = compute_segment_rms(clean.loc[clean.phase == \"after\",  \"ratio\"])\n",
    "        ratio_rms = (rms_d + rms_a) / rms_b\n",
    "        rms_ratio_data.append((trial, ratio_rms))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{trial}: fps≈{fps_est:.2f}, mean_d={d_val:.3f}, mean_a={a_val:.3f}, rms_ratio={ratio_rms:.3f}\")\n",
    "\n",
    "    out_dir = fly_folder / OUT_FIG_DIR\n",
    "    plot_grouped_bar_chart(fly_folder.name, ratio_data, out_dir / \"odor_response_bar.png\")\n",
    "    plot_rms_ratio_chart(fly_folder.name, rms_ratio_data, out_dir / \"rms_ratio.png\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AUTO-RUN FOR JUPYTER\n",
    "# -----------------------------------------------------------------------------\n",
    "root = Path(DEFAULT_MAIN_DIRECTORY)\n",
    "for fly in root.iterdir():\n",
    "    if fly.is_dir():\n",
    "        process_fly(fly, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36536b-2d30-4876-906b-61183a5570e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# fly_envelope_over_time.py — analytic envelope via Hilbert transform over time per trial\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "# ───────────────────────────────── CONFIG ───────────────────────────────\n",
    "DEFAULT_MAIN_DIRECTORY = main_directory\n",
    "OUT_FIG_DIR           = \"RMS_calculations/envelope_over_time_plots\"\n",
    "FPS_DEFAULT           = 40\n",
    "WINDOW_SEC            = 0.25\n",
    "WINDOW_FRAMES         = max(int(WINDOW_SEC * FPS_DEFAULT), 1)\n",
    "MEASURE_COLS    = [\"distance_percentage_2_6\", \"distance_percentage\"]\n",
    "TESTING_REGEX         = re.compile(r\"testing_(\\d+)\")\n",
    "\n",
    "# Odor timing (seconds)\n",
    "ODOR_ON_S  = 30.0\n",
    "ODOR_OFF_S = 60.0\n",
    "\n",
    "# ─────────────────────────────── HELPERS ────────────────────────────────\n",
    "def _resolve_measure_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    return next((c for c in MEASURE_COLS if c in df.columns), None)\n",
    "\n",
    "def _extract_trials(testing_dir: Path):\n",
    "    \"\"\"Yield trials sorted numerically by their testing index.\"\"\"\n",
    "    files = list(testing_dir.glob(\"*testing*.csv\"))\n",
    "\n",
    "    def extract_number(p: Path):\n",
    "        m = TESTING_REGEX.search(p.stem)\n",
    "        return int(m.group(1)) if m else float('inf')  # if no match, send to end\n",
    "\n",
    "    for csv_path in sorted(files, key=extract_number):\n",
    "        m = TESTING_REGEX.search(csv_path.stem)\n",
    "        label = m.group(0) if m else csv_path.stem\n",
    "        yield label, csv_path\n",
    "\n",
    "def _compute_envelope(series: pd.Series, win_frames: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute analytic envelope on valid samples only.\n",
    "    - Values outside [0, 100] and non-numeric are treated as missing (NaN) and do not count.\n",
    "    - We temporarily interpolate to run Hilbert, then re-mask invalid positions back to NaN.\n",
    "    - Final rolling mean ignores NaNs.\n",
    "    \"\"\"\n",
    "    # Coerce numeric\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "    # Valid = within [0, 100]\n",
    "    valid = s.between(0, 100, inclusive=\"both\")\n",
    "\n",
    "    if not valid.any():\n",
    "        # No usable data\n",
    "        return pd.Series(np.nan, index=s.index)\n",
    "\n",
    "    # Prepare a series for Hilbert (no NaNs), but keep track of invalids\n",
    "    s_interp = s.where(valid)\n",
    "    s_interp = s_interp.interpolate(limit_direction=\"both\")\n",
    "\n",
    "    # If still all-NaN (pathological), bail out\n",
    "    if s_interp.isna().all():\n",
    "        return pd.Series(np.nan, index=s.index)\n",
    "\n",
    "    analytic = hilbert(s_interp.to_numpy())\n",
    "    env = np.abs(analytic)\n",
    "\n",
    "    env_series = pd.Series(env, index=s.index)\n",
    "\n",
    "    # Re-apply mask so invalid samples do not count downstream\n",
    "    env_series = env_series.mask(~valid)\n",
    "\n",
    "    # Smooth; NaNs are ignored within the window\n",
    "    return (\n",
    "        env_series\n",
    "        .rolling(window=win_frames, center=True, min_periods=1)\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "def _rolling_rms_valid(series: pd.Series, win_frames: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rolling RMS over a centered window of size `win_frames`.\n",
    "    - Uses only valid samples within [0, 100]; invalids are treated as NaN.\n",
    "    - The rolling mean ignores NaNs (min_periods=1), so edges still produce values.\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    valid = s.between(0, 100, inclusive=\"both\")\n",
    "    s2 = s.where(valid).pow(2)\n",
    "    rms = s2.rolling(window=win_frames, center=True, min_periods=1).mean()\n",
    "    return rms.pow(0.5)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PLOTTING\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def plot_envelope_subplots(\n",
    "    fly_name: str,\n",
    "    trials_data: dict[str, tuple[np.ndarray, np.ndarray, float]],\n",
    "    out_path: Path,\n",
    "    y_max: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot analytic envelope over time for multiple trials as stacked subplots,\n",
    "    marking the global peak and drawing θ = μ_before + 4σ_before as a horizontal line.\n",
    "    \"\"\"\n",
    "    n = len(trials_data)\n",
    "    if n == 0:\n",
    "        print(f\"[WARN] {fly_name}: no trials to plot.\")\n",
    "        return\n",
    "\n",
    "    padded_max = y_max * 1.02 if y_max > 0 else 1.0\n",
    "\n",
    "    plt.rcParams.update({\"figure.dpi\": 300, \"savefig.dpi\": 300})\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(10, 2.5 * n), sharex=True)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (label, (time_s, env_vals, thr)) in zip(axes, trials_data.items()):\n",
    "        ax.plot(time_s, env_vals, linewidth=1, clip_on=False)\n",
    "\n",
    "        # Peak marker only if we have finite values\n",
    "        if np.any(np.isfinite(env_vals)):\n",
    "            idx = np.nanargmax(env_vals)\n",
    "            ax.plot(time_s[idx], env_vals[idx], marker='o', markersize=10, color='red', zorder=5)\n",
    "\n",
    "        ax.axhline(thr, linestyle='-', linewidth=1, label='θ = μ_before + 4σ')\n",
    "\n",
    "        # Odor on/off markers + shaded interval\n",
    "        ax.axvline(ODOR_ON_S,  linestyle='--', linewidth=1)\n",
    "        ax.axvline(ODOR_OFF_S, linestyle='--', linewidth=1)\n",
    "        ax.axvspan(ODOR_ON_S, ODOR_OFF_S, alpha=0.15)\n",
    "\n",
    "        # Axes cosmetics\n",
    "        ax.set_ylim(0, padded_max, auto=False)\n",
    "        ax.autoscale(enable=False, axis=\"y\")\n",
    "        ax.margins(x=0, y=0)\n",
    "        ax.set_ylabel(\"Envelope\")\n",
    "        ax.set_title(label)\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Legend (compact)\n",
    "        peak_handle = plt.Line2D([0], [0], marker='o', color='red', linestyle='None', markersize=6, label='Peak')\n",
    "        on_handle   = plt.Line2D([0], [0], linestyle='--', label='Odor on/off')\n",
    "        span_handle = plt.Rectangle((0,0), 1, 1, alpha=0.15, label='Odor on window')\n",
    "        thr_handle  = plt.Line2D([0], [0], linestyle='-', label='θ = μ_before + 4σ')\n",
    "        ax.legend(handles=[peak_handle, thr_handle, on_handle, span_handle], loc='upper right', frameon=True, fontsize=8)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\")\n",
    "    fig.suptitle(\n",
    "        f\"{fly_name}: Analytic Envelope Over Time (window={WINDOW_SEC}s; odor {ODOR_ON_S:.0f}–{ODOR_OFF_S:.0f}s)\",\n",
    "        y=0.98\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] {out_path}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# WORKFLOW\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def process_fly_envelope(fly_folder: Path):\n",
    "    \"\"\"\n",
    "    Compute analytic envelope for each trial then plot as subplots\n",
    "    with a common y-axis from 0 to the fly’s global maximum.\n",
    "    \"\"\"\n",
    "    fly_name   = fly_folder.name\n",
    "    testing_dir = fly_folder / \"RMS_calculations\"\n",
    "    if not testing_dir.is_dir():\n",
    "        print(f\"[WARN] {fly_name}: no testing directory.\")\n",
    "        return\n",
    "\n",
    "    trials_data: dict[str, tuple[np.ndarray, np.ndarray, float]] = {}\n",
    "    all_max = 0.0\n",
    "\n",
    "    for label, csv_path in _extract_trials(testing_dir):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # time axis\n",
    "        if \"time_seconds\" in df.columns:\n",
    "            time_s = df[\"time_seconds\"].to_numpy(dtype=float)\n",
    "        else:\n",
    "            time_s = np.arange(len(df)) / FPS_DEFAULT\n",
    "\n",
    "        # measurement series\n",
    "        meas_col = _resolve_measure_column(df)\n",
    "        if meas_col is None:\n",
    "            print(f\"[ERROR] {fly_name} {label}: no measure column.\")\n",
    "            continue\n",
    "\n",
    "        # Coerce numeric and keep raw series; envelope fn handles validity\n",
    "        series = pd.to_numeric(df[meas_col], errors=\"coerce\")\n",
    "\n",
    "        # ✅ Envelope (excludes out-of-range samples internally)\n",
    "        env_vals = _compute_envelope(series, WINDOW_FRAMES).to_numpy()\n",
    "\n",
    "        # ✅ Rolling RMS column (excludes >100% and <0)\n",
    "        rms_col = f\"{meas_col}_rms_win{WINDOW_FRAMES}\"\n",
    "        df[rms_col] = _rolling_rms_valid(df[meas_col], WINDOW_FRAMES)\n",
    "\n",
    "        # ✅ Persist back to the same CSV (overwrite in place)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Compute threshold from pre-odor window (time < ODOR_ON_S)\n",
    "        pre_mask = time_s < ODOR_ON_S\n",
    "        if np.any(pre_mask):\n",
    "            pre_vals = env_vals[pre_mask]\n",
    "        else:\n",
    "            # Fallback: use the first ODOR_ON_S seconds worth of frames by FPS\n",
    "            n_pre = max(int(ODOR_ON_S * FPS_DEFAULT), 1)\n",
    "            pre_vals = env_vals[:n_pre]\n",
    "\n",
    "        mu_before = float(np.nanmean(pre_vals)) if pre_vals.size else 0.0\n",
    "        sd_before = float(np.nanstd(pre_vals, ddof=0)) if pre_vals.size else 0.0\n",
    "        thr = mu_before + 4.0 * sd_before\n",
    "\n",
    "        trial_max = float(np.nanmax(env_vals)) if env_vals.size else 0.0\n",
    "        \n",
    "        try:\n",
    "            trial_max = float(np.nanmax(env_vals))\n",
    "        except ValueError:\n",
    "            trial_max = float('nan')\n",
    "\n",
    "        print(f\"[DEBUG] {fly_name} {label} peak={0.0 if not np.isfinite(trial_max) else trial_max:.3f}  μ_before={mu_before:.3f}  σ_before={sd_before:.3f}  θ={thr:.3f}\")\n",
    "\n",
    "        trials_data[label] = (time_s, env_vals, thr)\n",
    "        if np.isfinite(trial_max) and (trial_max > all_max):\n",
    "            all_max = trial_max\n",
    "\n",
    "    print(f\"[DEBUG] {fly_name} global peak envelope = {all_max:.3f}\")\n",
    "\n",
    "    out_dir  = fly_folder / OUT_FIG_DIR\n",
    "    out_path = out_dir / f\"{fly_name}_envelope_over_time_subplots.png\"\n",
    "    plot_envelope_subplots(fly_name, trials_data, out_path, y_max=all_max)\n",
    "\n",
    "def run_envelope_over_time(main_directory: Optional[Union[Path, str]] = None):\n",
    "    root = Path(main_directory) if main_directory else Path(DEFAULT_MAIN_DIRECTORY)\n",
    "    root = root.expanduser().resolve()\n",
    "    for fly in root.iterdir():\n",
    "        if fly.is_dir():\n",
    "            process_fly_envelope(fly)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_envelope_over_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0bc99-bc5c-4a72-9a4a-5572533054a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def collect_all_plots(main_directory: Union[str, Path], dest_folder: str = \"all_envelope_plots\"):\n",
    "    \"\"\"\n",
    "    Collect all envelope_over_time_subplots.png files from fly folders\n",
    "    into a single folder inside main_directory.\n",
    "    \"\"\"\n",
    "    root = Path(main_directory).expanduser().resolve()\n",
    "    dest = root / dest_folder\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "    for fly in root.iterdir():\n",
    "        if not fly.is_dir():\n",
    "            continue\n",
    "        # expected location of plot\n",
    "        plot_path = fly / OUT_FIG_DIR / f\"{fly.name}_envelope_over_time_subplots.png\"\n",
    "        if plot_path.is_file():\n",
    "            new_name = f\"{fly.name}_envelope_over_time_subplots.png\"\n",
    "            shutil.copy2(plot_path, dest / new_name)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[OK] Collected {count} plots into {dest}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_envelope_over_time()  # generate plots\n",
    "    collect_all_plots(DEFAULT_MAIN_DIRECTORY)  # gather them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfcad1-62ff-42f5-b8e9-05efb1011b68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Proboscis Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d700db1-5d74-4190-8beb-441352b4cc31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Angle at point2 with per-fly centering using distance_percentage==0\n",
    "# - Uncentered plots: fly_dir/angle_plots/\n",
    "# - Centered plots:   fly_dir/angle_plots_centered/\n",
    "# Input match: *_distance_class_2.csv\n",
    "# Output CSV:  *_distance_class_2_angle_ARB.csv  (adds/updates angle_ARB_deg, angle_centered_deg)\n",
    "\n",
    "import os, re, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ───────── prerequisites ─────────\n",
    "main_directory = Path(main_directory)\n",
    "assert main_directory.is_dir(), f\"Not a directory: {main_directory}\"\n",
    "\n",
    "# ───────── config ─────────\n",
    "# Point 1 (arbitrary anchor). If you prefer strict in-bounds indexing use 1079.0.\n",
    "ANCHOR_X, ANCHOR_Y = 1080.0, 540.0\n",
    "IN_SUFFIX  = \"merged.csv\"\n",
    "OUT_SUFFIX = \"merged.csv\"\n",
    "FPS = 40.0  # Frames per second (adjust if needed)\n",
    "\n",
    "# Month names for folder matching (case-insensitive)\n",
    "MONTHS = (\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "          \"july\",\"august\",\"september\",\"october\",\"november\",\"december\")\n",
    "\n",
    "def is_month_folder(p: Path) -> bool:\n",
    "    return p.is_dir() and p.name.lower().startswith(MONTHS)\n",
    "\n",
    "def compute_angle_deg_at_point2(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Angle at point 2 (class2) between segments:\n",
    "      (point1 → point2) and (point2 → point3)\n",
    "    Using vectors:\n",
    "      u = point1 - point2\n",
    "      v = point3 - point2\n",
    "    angle = atan2(|u×v|, u·v) in degrees ∈ [0, 180].\n",
    "    Returns a pandas Series aligned to df.index.\n",
    "    \"\"\"\n",
    "    required = [\"x_class2\", \"y_class2\", \"x_class6\", \"y_class6\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    # Point 2 (vertex)\n",
    "    p2x = df[\"x_class2\"].astype(\"float64\")\n",
    "    p2y = df[\"y_class2\"].astype(\"float64\")\n",
    "\n",
    "    # Point 1 (anchor) and Point 3 (class6)\n",
    "    p1x, p1y = ANCHOR_X, ANCHOR_Y\n",
    "    p3x = df[\"x_class6\"].astype(\"float64\")\n",
    "    p3y = df[\"y_class6\"].astype(\"float64\")\n",
    "\n",
    "    # u = p1 - p2 ; v = p3 - p2\n",
    "    ux = p1x - p2x\n",
    "    uy = p1y - p2y\n",
    "    vx = p3x - p2x\n",
    "    vy = p3y - p2y\n",
    "\n",
    "    dot   = ux*vx + uy*vy\n",
    "    cross = ux*vy - uy*vx\n",
    "\n",
    "    n1 = np.hypot(ux, uy)\n",
    "    n2 = np.hypot(vx, vy)\n",
    "    valid = (n1 > 0) & (n2 > 0) & np.isfinite(dot) & np.isfinite(cross)\n",
    "\n",
    "    angle_rad = np.full(len(df), np.nan, dtype=\"float64\")\n",
    "    angle_rad[valid.to_numpy()] = np.arctan2(np.abs(cross[valid]), dot[valid])  # [0, π]\n",
    "    angle_deg = np.degrees(angle_rad)  # ndarray\n",
    "\n",
    "    return pd.Series(angle_deg, index=df.index, name=\"angle_ARB_deg\")\n",
    "\n",
    "def get_time_axis(df: pd.DataFrame):\n",
    "    \"\"\"Return time array (seconds) for plotting.\"\"\"\n",
    "    if \"time_s\" in df.columns:\n",
    "        return df[\"time_s\"].astype(float).to_numpy()\n",
    "    elif \"frame\" in df.columns:\n",
    "        return (df[\"frame\"].astype(float) / FPS).to_numpy()\n",
    "    else:\n",
    "        return (np.arange(len(df)) / FPS)\n",
    "\n",
    "def require_distance_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Return the name of the distance percentage column, raising if not found.\"\"\"\n",
    "    for cand in [\"distance_percentage\", \"distance_percentage_2_6\", \"distance_pct\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise ValueError(\"Could not find a distance percentage column (tried: \"\n",
    "                     \"'distance_percentage', 'distance_percentage_2_6', 'distance_pct').\")\n",
    "\n",
    "def find_fly_reference_angle(csv_paths):\n",
    "    \"\"\"\n",
    "    Across all CSVs for a fly:\n",
    "    - Prefer the first row where distance_percentage == 0\n",
    "    - Otherwise choose the row with minimal |distance_percentage_2_6|\n",
    "    Returns (ref_angle, info_string).\n",
    "    \"\"\"\n",
    "    best = None  # tuple (priority, abs_val, angle_deg, file, idx, time_s)\n",
    "\n",
    "    for p in csv_paths:\n",
    "        df = pd.read_csv(p)\n",
    "        angle_ser = compute_angle_deg_at_point2(df)        # pandas Series\n",
    "        dist_col  = require_distance_col(df)\n",
    "        dist      = df[dist_col].astype(float)\n",
    "\n",
    "        # exact zeros in this file?\n",
    "        exact_idx = np.flatnonzero(dist.to_numpy() == 0)\n",
    "        if exact_idx.size > 0:\n",
    "            idx = int(exact_idx[0])\n",
    "            angle_here = float(angle_ser.iloc[idx]) if np.isfinite(angle_ser.iloc[idx]) else np.nan\n",
    "            tvals = get_time_axis(df)\n",
    "            candidate = (0, 0.0, angle_here, p, idx, float(tvals[idx]))\n",
    "            if best is None or candidate < best:\n",
    "                best = candidate\n",
    "            continue\n",
    "\n",
    "        # fallback: min |dist|\n",
    "        with np.errstate(invalid=\"ignore\"):\n",
    "            absdist = np.abs(dist.to_numpy(dtype=float))\n",
    "        if absdist.size == 0 or not np.isfinite(absdist).any():\n",
    "            continue\n",
    "        idx = int(np.nanargmin(absdist))\n",
    "        candidate_abs = float(absdist[idx])\n",
    "        angle_here = float(angle_ser.iloc[idx]) if np.isfinite(angle_ser.iloc[idx]) else np.nan\n",
    "        tvals = get_time_axis(df)\n",
    "        candidate = (1, candidate_abs, angle_here, p, idx, float(tvals[idx]))\n",
    "        if best is None or candidate < best:\n",
    "            best = candidate\n",
    "\n",
    "    if best is None:\n",
    "        raise RuntimeError(\"No suitable reference frame found for this fly.\")\n",
    "    priority, absval, ref_angle, ref_file, ref_idx, ref_time = best\n",
    "    how = \"distance_percentage == 0\" if priority == 0 else f\"min |distance_percentage| = {absval:.6g}\"\n",
    "    msg = f\"Ref: {how} at {Path(ref_file).name}[row {ref_idx}] t={ref_time:.3f}s → ref_angle={ref_angle:.3f}°\"\n",
    "    return ref_angle, msg\n",
    "\n",
    "# ───────── traversal ─────────\n",
    "processed, plotted_unc, plotted_ctr, failed = 0, 0, 0, 0\n",
    "errors = []\n",
    "\n",
    "for fly_dir in sorted([p for p in main_directory.iterdir() if p.is_dir()]):\n",
    "    # Gather all candidate CSVs for this fly (under subfolders that start with a month)\n",
    "    month_folders = [sub for sub in fly_dir.rglob(\"*\") if is_month_folder(sub)]\n",
    "    csv_paths = []\n",
    "    for month_folder in month_folders:\n",
    "        csv_paths.extend(Path(x) for x in glob.iglob(str(month_folder / \"**\" / f\"*{IN_SUFFIX}\"), recursive=True))\n",
    "    csv_paths = [p for p in csv_paths if p.name.endswith(IN_SUFFIX)]\n",
    "\n",
    "    if not csv_paths:\n",
    "        continue\n",
    "\n",
    "    # Compute per-fly reference angle\n",
    "    try:\n",
    "        ref_angle, ref_info = find_fly_reference_angle(csv_paths)\n",
    "        print(f\"[{fly_dir.name}] {ref_info}\")\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        errors.append(f\"{fly_dir}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure per-fly plot folders\n",
    "    fly_plot_dir_unc = fly_dir / \"angle_plots\"\n",
    "    fly_plot_dir_ctr = fly_dir / \"angle_plots\" / \"angle_plots_centered\"\n",
    "    fly_plot_dir_unc.mkdir(exist_ok=True)\n",
    "    fly_plot_dir_ctr.mkdir(exist_ok=True)\n",
    "\n",
    "    # Process each CSV with centering\n",
    "    for csv_path in csv_paths:\n",
    "        out_path = csv_path.with_name(csv_path.name.replace(IN_SUFFIX, OUT_SUFFIX, 1))\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df[\"angle_deg_c2_26_vs_anchor\"] = compute_angle_deg_at_point2(df)\n",
    "            df[\"angle_centered_deg\"] = df[\"angle_deg_c2_26_vs_anchor\"] - ref_angle\n",
    "            df.to_csv(out_path, index=False)\n",
    "            processed += 1\n",
    "\n",
    "            # Time axis\n",
    "            t = get_time_axis(df)\n",
    "\n",
    "            # File-safe plot name (flatten relative path)\n",
    "            relative_parts = csv_path.relative_to(fly_dir).with_suffix(\"\").parts\n",
    "            safe_name = \"_\".join(relative_parts)\n",
    "\n",
    "            # —— Uncentered plot ——\n",
    "            png_unc = fly_plot_dir_unc / f\"{safe_name}_angle_ARB.png\"\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(t, df[\"angle_deg_c2_26_vs_anchor\"].to_numpy(), linewidth=1.5)\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Angle (degrees)\")\n",
    "            plt.title(f\"{csv_path.stem} — angle@class2\")\n",
    "            plt.grid(True, alpha=0.25)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(png_unc, dpi=150)\n",
    "            plt.close()\n",
    "            plotted_unc += 1\n",
    "\n",
    "            # —— Centered plot ——\n",
    "            png_ctr = fly_plot_dir_ctr / f\"{safe_name}_angle_ARB_centered.png\"\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(t, df[\"angle_centered_deg\"].to_numpy(), linewidth=1.5)\n",
    "            plt.axhline(0, linestyle=\"--\", linewidth=1.0, alpha=0.6)\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Centered angle (deg)\")\n",
    "            plt.title(f\"{csv_path.stem} — centered by fly ref (angle@class2 − {ref_angle:.2f}°)\")\n",
    "            plt.grid(True, alpha=0.25)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(png_ctr, dpi=150)\n",
    "            plt.close()\n",
    "            plotted_ctr += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            errors.append(f\"{csv_path}: {e}\")\n",
    "\n",
    "print(f\"Done. CSVs updated: {processed} | Uncentered plots: {plotted_unc} | Centered plots: {plotted_ctr} | Failed: {failed}\")\n",
    "if errors:\n",
    "    print(\"\\nErrors:\")\n",
    "    for msg in errors[:12]:\n",
    "        print(\" •\", msg)\n",
    "    if len(errors) > 12:\n",
    "        print(f\" … and {len(errors)-12} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8acf2d-1d92-4a79-a4ef-8818e1e51ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Combined heatmaps of centered angle percentages per fly\n",
    "# Produces, for each fly:\n",
    "#   fly_dir/angle_plots_centered_percentage_heatmaps/<fly>_training_angle_centered_pct_heatmap.png\n",
    "#   fly_dir/angle_plots_centered_percentage_heatmaps/<fly>_testing_angle_centered_pct_heatmap.png\n",
    "#\n",
    "# Inputs (preferred): *_distance_class_2_angle_ARB.csv (with angle_centered_pct)\n",
    "\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# ───────── prerequisites ─────────\n",
    "assert 'main_directory' in globals(), \"Define main_directory = '/path/to/root' before running.\"\n",
    "main_directory = Path(main_directory)\n",
    "assert main_directory.is_dir(), f\"Not a directory: {main_directory}\"\n",
    "\n",
    "# ───────── display defaults (similar to your snippet) ─────────\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'lines.linewidth': 2,\n",
    "    'axes.linewidth': 1.5,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'legend.fontsize': 12\n",
    "})\n",
    "\n",
    "# ───────── config ─────────\n",
    "FPS = 40.0\n",
    "IN_SUFFIX_RAW = \"*merged.csv\"\n",
    "IN_SUFFIX_ANG = \"*merged.csv\"\n",
    "\n",
    "# Make this RELATIVE (no leading slash) and a Path\n",
    "OUT_HEAT_DIR = Path(\"angle_plots/angle_plots_centered_percentage_heatmaps\")\n",
    "ANCHOR_X, ANCHOR_Y = 1080.0, 540.0   # anchor used if we must recompute angles\n",
    "\n",
    "MONTHS = (\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "          \"july\",\"august\",\"september\",\"october\",\"november\",\"december\")\n",
    "\n",
    "def is_month_folder(p: Path) -> bool:\n",
    "    return p.is_dir() and p.name.lower().startswith(MONTHS)\n",
    "\n",
    "def infer_category_from_path(p: Path) -> str | None:\n",
    "    parts = [s.lower() for s in p.parts]\n",
    "    if \"testing\" in parts:\n",
    "        return \"testing\"\n",
    "    if \"training\" in parts:\n",
    "        return \"training\"\n",
    "    # If not explicit, try filename\n",
    "    name = p.name.lower()\n",
    "    if \"testing\" in name:\n",
    "        return \"testing\"\n",
    "    if \"training\" in name:\n",
    "        return \"training\"\n",
    "    return None\n",
    "\n",
    "def get_time_axis(df: pd.DataFrame) -> np.ndarray:\n",
    "    if \"time_s\" in df.columns:\n",
    "        return df[\"time_s\"].astype(float).to_numpy()\n",
    "    if \"frame\" in df.columns:\n",
    "        return (df[\"frame\"].astype(float) / FPS).to_numpy()\n",
    "    return np.arange(len(df)) / FPS\n",
    "\n",
    "def require_distance_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [\"distance_percentage\", \"distance_percentage_2_6\", \"distance_pct\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise ValueError(\"No distance percentage column found (expected one of: \"\n",
    "                     \"'distance_percentage','distance_percentage_2_6','distance_pct').\")\n",
    "\n",
    "def compute_angle_deg_at_point2(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Angle at class2 between (point1→class2) and (class2→class6), degrees [0,180].\"\"\"\n",
    "    required = [\"x_class2\", \"y_class2\", \"x_class6\", \"y_class6\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for angle computation: {missing}\")\n",
    "    p2x = df[\"x_class2\"].astype(float); p2y = df[\"y_class2\"].astype(float)\n",
    "    p1x, p1y = ANCHOR_X, ANCHOR_Y\n",
    "    p3x = df[\"x_class6\"].astype(float); p3y = df[\"y_class6\"].astype(float)\n",
    "    ux, uy = (p1x - p2x), (p1y - p2y)    # p1 - p2\n",
    "    vx, vy = (p3x - p2x), (p3y - p2y)    # p3 - p2\n",
    "    dot   = ux*vx + uy*vy\n",
    "    cross = ux*vy - uy*vx\n",
    "    n1 = np.hypot(ux, uy); n2 = np.hypot(vx, vy)\n",
    "    valid = (n1 > 0) & (n2 > 0) & np.isfinite(dot) & np.isfinite(cross)\n",
    "    angle_rad = np.full(len(df), np.nan)\n",
    "    angle_rad[valid.to_numpy()] = np.arctan2(np.abs(cross[valid]), dot[valid])\n",
    "    return pd.Series(np.degrees(angle_rad), index=df.index, name=\"angle_ARB_deg\")\n",
    "\n",
    "def find_fly_reference_angle(csv_paths_raw: list[Path]) -> float:\n",
    "    \"\"\"\n",
    "    Per-fly reference: first distance_percentage == 0 across files,\n",
    "    else row with minimal |distance_percentage_2_6|.\n",
    "    Returns the angle (deg) at that reference frame.\n",
    "    \"\"\"\n",
    "    best = None  # (priority, abs_val, angle_deg)\n",
    "    for p in csv_paths_raw:\n",
    "        df = pd.read_csv(p)\n",
    "        angle_ser = compute_angle_deg_at_point2(df)\n",
    "        dist_col  = require_distance_col(df)\n",
    "        dist      = df[dist_col].astype(float).to_numpy()\n",
    "        exact_idx = np.flatnonzero(dist == 0)\n",
    "        if exact_idx.size > 0:\n",
    "            idx = int(exact_idx[0])\n",
    "            angle_here = float(angle_ser.iloc[idx]) if np.isfinite(angle_ser.iloc[idx]) else np.nan\n",
    "            cand = (0, 0.0, angle_here)\n",
    "            if best is None or cand < best:\n",
    "                best = cand\n",
    "            continue\n",
    "        with np.errstate(invalid=\"ignore\"):\n",
    "            absdist = np.abs(dist)\n",
    "        if absdist.size == 0 or not np.isfinite(absdist).any():\n",
    "            continue\n",
    "        idx = int(np.nanargmin(absdist))\n",
    "        angle_here = float(angle_ser.iloc[idx]) if np.isfinite(angle_ser.iloc[idx]) else np.nan\n",
    "        cand = (1, float(absdist[idx]), angle_here)\n",
    "        if best is None or cand < best:\n",
    "            best = cand\n",
    "    if best is None:\n",
    "        return np.nan\n",
    "    return best[2]\n",
    "\n",
    "def compute_fly_max_abs_centered(csv_paths_raw: list[Path], ref_angle: float) -> float:\n",
    "    \"\"\"Max |angle_centered_deg| across all raw CSVs for a fly.\"\"\"\n",
    "    fly_max = 0.0\n",
    "    for p in csv_paths_raw:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            ang = compute_angle_deg_at_point2(df)\n",
    "            centered = (ang - ref_angle).to_numpy(dtype=float)\n",
    "            with np.errstate(invalid=\"ignore\"):\n",
    "                local = np.nanmax(np.abs(centered))\n",
    "            if np.isfinite(local):\n",
    "                fly_max = max(fly_max, float(local))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fly_max if np.isfinite(fly_max) and fly_max > 0 else np.nan\n",
    "\n",
    "def get_odor_window_if_available(df: pd.DataFrame) -> tuple[float, float] | None:\n",
    "    \"\"\"\n",
    "    If 'OFM State' exists, return (onset_time_s, offset_time_s) relative to the file start,\n",
    "    based on the first/last 'during' segment. Otherwise None.\n",
    "    \"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    if \"ofm state\" in cols:\n",
    "        col = cols[\"ofm state\"]\n",
    "        try:\n",
    "            # Build time_s\n",
    "            t = get_time_axis(df)\n",
    "            mask = df[col].astype(str).str.lower().eq(\"during\").to_numpy()\n",
    "            if mask.any():\n",
    "                idx = np.flatnonzero(mask)\n",
    "                return float(t[idx[0]] - t[0]), float(t[idx[-1]] - t[0])\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _persist_angle_columns(path: Path, df_raw: pd.DataFrame, ang: pd.Series, ref_angle: float, fly_max_abs: float) -> Path:\n",
    "    \"\"\"Write angle metrics (deg, centered deg, centered %) back to disk for this file.\n",
    "    Returns the output CSV path.\"\"\"\n",
    "    df_out = df_raw.copy()\n",
    "    df_out[\"angle_ARB_deg\"] = ang.to_numpy()\n",
    "    centered_deg = (ang - ref_angle).to_numpy() if np.isfinite(ref_angle) else np.full(len(ang), np.nan)\n",
    "    df_out[\"angle_centered_deg\"] = centered_deg\n",
    "    if np.isfinite(fly_max_abs) and fly_max_abs > 0:\n",
    "        df_out[\"angle_centered_pct\"] = (centered_deg / float(fly_max_abs)) * 100.0\n",
    "    else:\n",
    "        df_out[\"angle_centered_pct\"] = np.zeros(len(ang), dtype=float)\n",
    "\n",
    "    out_csv = path.with_name(path.stem + \".csv\")\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    return out_csv\n",
    "\n",
    "# ───────── collect flies ─────────\n",
    "fly_dirs = [p for p in main_directory.iterdir() if p.is_dir()]\n",
    "\n",
    "for fly_dir in sorted(fly_dirs):\n",
    "    fly_name = fly_dir.name\n",
    "\n",
    "    out_dir = fly_dir / OUT_HEAT_DIR\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)  # ← create full tree if needed\n",
    "    # optional sanity check:\n",
    "    assert out_dir.is_dir(), f\"Failed to create output directory: {out_dir}\"\n",
    "\n",
    "\n",
    "    # Find candidate CSVs beneath month-named subfolders\n",
    "    month_folders = [sub for sub in fly_dir.rglob(\"*\") if is_month_folder(sub)]\n",
    "    raw_csvs, ang_csvs = [], []\n",
    "    for month_folder in month_folders:\n",
    "        raw_csvs += [Path(x) for x in glob.iglob(str(month_folder / \"**\" / f\"*{IN_SUFFIX_RAW}\"), recursive=True)]\n",
    "        ang_csvs += [Path(x) for x in glob.iglob(str(month_folder / \"**\" / f\"*{IN_SUFFIX_ANG}\"), recursive=True)]\n",
    "\n",
    "    # Group by category (training/testing)\n",
    "    grouped = {\"training\": [], \"testing\": []}\n",
    "\n",
    "    # Prefer angle CSVs if they already exist and contain angle_centered_pct\n",
    "    for p in sorted(ang_csvs):\n",
    "        cat = infer_category_from_path(p) or \"testing\"  # default to testing if ambiguous\n",
    "        grouped[cat].append((\"angle\", p))\n",
    "\n",
    "    # Include raw CSVs as well (to compute if angle CSV missing or incomplete)\n",
    "    for p in sorted(raw_csvs):\n",
    "        cat = infer_category_from_path(p) or \"testing\"\n",
    "        grouped[cat].append((\"raw\", p))\n",
    "\n",
    "    # Compute per-fly reference & max if we need to compute from raw\n",
    "    need_compute = any(kind == \"raw\" for pairs in grouped.values() for kind, _ in pairs)\n",
    "    ref_angle = np.nan\n",
    "    fly_max_abs = np.nan\n",
    "    if need_compute:\n",
    "        ref_angle = find_fly_reference_angle(raw_csvs) if raw_csvs else np.nan\n",
    "        fly_max_abs = compute_fly_max_abs_centered(raw_csvs, ref_angle) if np.isfinite(ref_angle) else np.nan\n",
    "\n",
    "    for category in (\"training\", \"testing\"):\n",
    "        # Build trial rows (time, centered %, odor window, label)\n",
    "        trials = []\n",
    "        seen_labels = set()\n",
    "\n",
    "        for kind, path in grouped[category]:\n",
    "            try:\n",
    "                if kind == \"angle\":\n",
    "                    df = pd.read_csv(path)\n",
    "                    if \"angle_centered_pct\" not in df.columns:\n",
    "                        # Fall back to compute if missing\n",
    "                        raise RuntimeError(\"angle_centered_pct missing in angle CSV; recomputing from raw.\")\n",
    "                    time = get_time_axis(df)\n",
    "                    pct = df[\"angle_centered_pct\"].astype(float).to_numpy()\n",
    "                    # Try to recover odor window from sibling raw CSV if available\n",
    "                    odor = None\n",
    "                    # Heuristic: same name but without _angle_ARB\n",
    "                    raw_guess = path.with_name(path.name.replace(IN_SUFFIX_ANG, IN_SUFFIX_RAW))\n",
    "                    if raw_guess.exists():\n",
    "                        df_raw = pd.read_csv(raw_guess)\n",
    "                        odor = get_odor_window_if_available(df_raw)\n",
    "                    label = path.stem\n",
    "                else:\n",
    "                    # kind == \"raw\" → compute centered %\n",
    "                    df_raw = pd.read_csv(path)\n",
    "                    ang = compute_angle_deg_at_point2(df_raw)\n",
    "                    centered = ang - ref_angle if np.isfinite(ref_angle) else ang * 0.0\n",
    "                    if np.isfinite(fly_max_abs) and fly_max_abs > 0:\n",
    "                        pct = (centered.to_numpy() / fly_max_abs) * 100.0\n",
    "                    else:\n",
    "                        pct = np.zeros(len(centered), dtype=float)\n",
    "                    time = get_time_axis(df_raw)\n",
    "                    odor = get_odor_window_if_available(df_raw)\n",
    "\n",
    "                    # >>> ADD to persist the new columns to disk\n",
    "                    try:\n",
    "                        out_csv_path = _persist_angle_columns(path, df_raw, ang, ref_angle, fly_max_abs)\n",
    "                        # Optional: if you want downstream code to load the augmented file next time,\n",
    "                        # you could log it here:\n",
    "                        print(f\"Augmented CSV written → {out_csv_path}\")\n",
    "                    except Exception as _e:\n",
    "                        # Non-fatal: continue plotting even if write fails\n",
    "                        pass\n",
    "                    # <<< END ADD\n",
    "\n",
    "                    label = path.stem\n",
    "\n",
    "                # Avoid duplicates if both angle+raw contributed the same trial\n",
    "                if label in seen_labels:\n",
    "                    continue\n",
    "                seen_labels.add(label)\n",
    "\n",
    "                # Pack trial row\n",
    "                trials.append({\n",
    "                    \"label\": label,\n",
    "                    \"time\": time,\n",
    "                    \"pct\": pct,\n",
    "                    \"odor\": odor\n",
    "                })\n",
    "            except Exception:\n",
    "                # Skip problematic files silently to keep the grid building\n",
    "                continue\n",
    "\n",
    "        if not trials:\n",
    "            continue\n",
    "\n",
    "        # Normalize each trial onto a common time grid per trial for pcolormesh\n",
    "        # Use each trial's own uniform edges (no need to resample); pcolormesh accepts per-row edges.\n",
    "        n_trials = len(trials)\n",
    "        fig_h = max(2, int(2 * n_trials))  # scale height similar to your script\n",
    "        fig, axs = plt.subplots(n_trials, 1, figsize=(18, fig_h), sharex=False)\n",
    "        if n_trials == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        # Colormap: diverging, centered at 0, with fixed range -100..+100\n",
    "        cmap = mpl.colormaps['coolwarm'].copy()\n",
    "        cmap.set_bad('dimgray')\n",
    "        norm = Normalize(vmin=-100, vmax=100)\n",
    "\n",
    "        legend_handles = []\n",
    "        used_odor_line = False\n",
    "\n",
    "        for i, tr in enumerate(trials):\n",
    "            ax = axs[i]\n",
    "            t = tr[\"time\"]\n",
    "            data_row = tr[\"pct\"]\n",
    "\n",
    "            # Guard against empty/malformed sequences\n",
    "            if len(t) < 2 or len(data_row) != len(t):\n",
    "                # Skip this row visually\n",
    "                ax.text(0.5, 0.5, f\"Skipped: {tr['label']}\", transform=ax.transAxes,\n",
    "                        ha='center', va='center')\n",
    "                ax.set_yticks([])\n",
    "                continue\n",
    "\n",
    "            # Build edges for pcolormesh\n",
    "            time_edges = np.linspace(t[0], t[-1], len(t) + 1)\n",
    "            X, Y = np.meshgrid(time_edges, [0, 1])\n",
    "            row = np.asarray(data_row, dtype=float).reshape(1, -1)\n",
    "\n",
    "            pcm = ax.pcolormesh(X, Y, row, cmap=cmap, shading='auto', norm=norm)\n",
    "\n",
    "            # Odor window (if available)\n",
    "            if tr[\"odor\"] is not None:\n",
    "                odor_start, odor_end = tr[\"odor\"]\n",
    "                ax.axvline(odor_start, color='red', linewidth=2.5)\n",
    "                ax.axvline(odor_end,   color='red', linewidth=2.5)\n",
    "                used_odor_line = True\n",
    "\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlim(t[0], t[-1])\n",
    "            ax.set_title(tr[\"label\"], loc='left')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.tick_params(axis='x', direction='out')\n",
    "\n",
    "        axs[-1].set_xlabel(\"Time (seconds)\")\n",
    "        fig.suptitle(f\"{fly_name} – {category.capitalize()} Trials (Centered Angle %)\", fontsize=20)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "        # Colorbar\n",
    "        cbar = fig.colorbar(pcm, ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar.set_label(\"Centered Angle (%)\", fontsize=14)\n",
    "\n",
    "        # Legend (odor period)\n",
    "        if used_odor_line:\n",
    "            legend_handles = [Line2D([0], [0], color='red', linewidth=2.5, label='Odor Period')]\n",
    "            fig.legend(handles=legend_handles, loc='upper right', frameon=True)\n",
    "\n",
    "        out_png = out_dir / f\"{fly_name}_{category}_angle_centered_pct_heatmap.png\"\n",
    "        plt.savefig(out_png, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"[{fly_name}] {category.capitalize()} heatmap saved → {out_png}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f0d15-1b44-46ae-9f6e-86bad4d05a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Antenna Eye Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1348dd-1884-41ae-9893-8b940a1981b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Combine per-pair CSVs into one with only requested columns.\n",
    "# Looks for sfx2 in: <fly>/Eye_Prob_Dist/testing/\n",
    "# Saves to: <fly>/RMS_calculations/<...>_class_combined.csv\n",
    "# --------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# REQUIRED: define main_directory as a Path or string before running\n",
    "main_directory = Path(main_directory)\n",
    "\n",
    "sfx1    = \"merged.csv\"\n",
    "sfx2    = \"merged.csv\"\n",
    "out_sfx = \"merged.csv\"\n",
    "\n",
    "EPD_SUBPATH = (\"Eye_Prob_Dist\", \"testing\")  # where to find sfx2\n",
    "\n",
    "cols1 = [\"frame\", \"x_class1\", \"y_class1\"]\n",
    "cols2 = [\"frame\", \"x_class2\", \"y_class2\", \"x_class6\", \"y_class6\", \"distance_percentage\", \"timestamp\", \"OFM_State\"]\n",
    "\n",
    "def _find_matching_sfx2(p1_name: str, p2_dir: Path) -> Path | None:\n",
    "    # 1) Exact same basename with swapped suffix\n",
    "    candidate = p2_dir / p1_name.replace(sfx1, sfx2)\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    # 2) Match by testing/training index if present\n",
    "    m = re.search(r\"(testing|training)_(\\d+)\", p1_name)\n",
    "    if m:\n",
    "        n = m.group(2)\n",
    "        matches = list(p2_dir.glob(f\"*testing_{n}*{sfx2}\"))\n",
    "        if len(matches) == 1:\n",
    "            return matches[0]\n",
    "    # 3) Single-file fallback\n",
    "    all_sfx2 = list(p2_dir.glob(f\"*{sfx2}\"))\n",
    "    if len(all_sfx2) == 1:\n",
    "        return all_sfx2[0]\n",
    "    return None\n",
    "\n",
    "def _read_df2_robust(p2: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read df2, tolerating missing optional columns by filling with NA.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(p2, usecols=cols2)\n",
    "    except Exception:\n",
    "        base = [\"frame\", \"x_class2\", \"y_class2\"]\n",
    "        df2 = pd.read_csv(p2, usecols=base)\n",
    "        # Fill any optional columns not present\n",
    "        optional = [\"x_class6\", \"y_class6\", \"distance_percentage\", \"timestamp\", \"OFM_State\"]\n",
    "        for c in optional:\n",
    "            if c not in df2.columns:\n",
    "                df2[c] = pd.NA\n",
    "        return df2\n",
    "\n",
    "def process_pair(p1: Path, fly_root: Path):\n",
    "    \"\"\"Create merged CSV under <fly>/RMS_calculations/.\"\"\"\n",
    "    p2_dir = fly_root.joinpath(*EPD_SUBPATH)\n",
    "    if not p2_dir.is_dir():\n",
    "        print(f\"⚠️  {fly_root.name}: missing {p2_dir.relative_to(fly_root)}\")\n",
    "        return\n",
    "\n",
    "    p2 = _find_matching_sfx2(p1.name, p2_dir)\n",
    "    if p2 is None:\n",
    "        print(f\"⚠️  {p1.relative_to(main_directory)}: no matching {sfx2} in {p2_dir.relative_to(fly_root)}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df1 = pd.read_csv(p1, usecols=cols1)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed reading {p1}: {e}\")\n",
    "        return\n",
    "\n",
    "    df2 = _read_df2_robust(p2)\n",
    "    merged = pd.merge(df1, df2, on=\"frame\", how=\"inner\").sort_values(\"frame\")\n",
    "\n",
    "    out_dir = fly_root / \"Eye_Antenna_Dist\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    out_csv = out_dir / p1.name.replace(sfx1, out_sfx)\n",
    "\n",
    "    merged.to_csv(out_csv, index=False)\n",
    "    print(f\"✓ {out_csv.relative_to(fly_root)}  ⇐ merged with {p2.relative_to(fly_root)}\")\n",
    "\n",
    "# Walk each fly folder; find every *class_1_2.csv, merge with sfx2 from Eye_Prob_Dist/testing\n",
    "for fly_folder in [p for p in main_directory.iterdir() if p.is_dir()]:\n",
    "    for p1 in fly_folder.rglob(f\"*{sfx1}\"):\n",
    "        process_pair(p1, fly_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125da9cf-3bc4-4b1c-9838-3260163b6137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Add distance_class1_class2 in place (no testing/training subfolders assumed)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_distance_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    required = [\"x_class1\", \"y_class1\", \"x_class2\", \"y_class2\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    for c in required:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    dx = df[\"x_class1\"] - df[\"x_class2\"]\n",
    "    dy = df[\"y_class1\"] - df[\"y_class2\"]\n",
    "    df[\"distance_class1_class2\"] = np.sqrt(dx*dx + dy*dy)\n",
    "    return df\n",
    "\n",
    "def process_main_directory(main_directory: str | Path) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Find CSVs under <fly>/Eye_Antenna_Dist/*combined.csv (no testing/training folders),\n",
    "    add 'distance_class1_class2' if missing, and overwrite in place.\n",
    "    \"\"\"\n",
    "    root = Path(main_directory)\n",
    "    assert root.is_dir(), f\"Not a directory: {root}\"\n",
    "\n",
    "    # Relative recursive search (avoids absolute glob error)\n",
    "    pattern = \"Eye_Antenna_Dist/*combined.csv\"\n",
    "    csv_paths = sorted(root.rglob(pattern))\n",
    "\n",
    "    outputs: list[Path] = []\n",
    "    for csv_path in csv_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            if \"distance_class1_class2\" not in df.columns:\n",
    "                df = compute_distance_column(df)\n",
    "                df.to_csv(csv_path, index=False)  # overwrite in place\n",
    "                outputs.append(csv_path)\n",
    "            else:\n",
    "                # Already present; skip to avoid unnecessary writes\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipped {csv_path}: {e}\")\n",
    "    return outputs\n",
    "\n",
    "# Example:\n",
    "outputs = process_main_directory(main_directory)\n",
    "print(f\"Updated {len(outputs)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc39dc2-95d6-435e-aa7a-d9a99ff679ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Normalize distance_class1_class2 to percentage per fly\n",
    "# Bottom 5% of values (global) are trimmed; min = smallest remaining; max = global (untrimmed).\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Config ---\n",
    "DIST_COL  = \"distance_class1_class2\"\n",
    "PCT_COL   = \"distance_class1_class2_pct\"\n",
    "TRIM_FRAC = 0.05  # bottom 5%\n",
    "\n",
    "def _find_fly_dirs(main_directory: Path) -> list[Path]:\n",
    "    \"\"\"Return immediate subdirectories that look like fly folders.\"\"\"\n",
    "    return [p for p in main_directory.iterdir() if p.is_dir()]\n",
    "\n",
    "def _gather_csvs_for_fly(fly_dir: Path) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Use the updated CSVs written in place by the previous script:\n",
    "      <fly>/Eye_Antenna_Dist/*combined.csv\n",
    "    \"\"\"\n",
    "    epd = fly_dir / \"Eye_Antenna_Dist\"\n",
    "    if not epd.is_dir():\n",
    "        return []\n",
    "    return sorted(epd.glob(\"*combined.csv\"))\n",
    "\n",
    "def _compute_trimmed_min_and_global_max(csv_paths: list[Path]) -> tuple[float, float, float, int, int] | None:\n",
    "    \"\"\"\n",
    "    Collect all DIST_COL values across the fly, drop NaNs.\n",
    "    Compute p5 = 5th percentile, drop values < p5, then:\n",
    "      - trimmed_min = min(remaining)\n",
    "      - global_max  = max(all)\n",
    "    Returns (trimmed_min, global_max, p5, total_count, kept_count)\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    for p in csv_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p, usecols=[DIST_COL])\n",
    "            vals.append(pd.to_numeric(df[DIST_COL], errors=\"coerce\").to_numpy())\n",
    "        except ValueError:\n",
    "            # Column missing; skip file\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not read {p.name}: {e}\")\n",
    "    if not vals:\n",
    "        return None\n",
    "\n",
    "    all_vals = np.concatenate(vals)\n",
    "    all_vals = all_vals[np.isfinite(all_vals)]\n",
    "    total = all_vals.size\n",
    "    if total == 0:\n",
    "        return None\n",
    "\n",
    "    global_max = float(np.max(all_vals))\n",
    "    p5 = float(np.percentile(all_vals, 100 * TRIM_FRAC, method=\"linear\"))\n",
    "\n",
    "    kept = all_vals[all_vals >= p5]\n",
    "    kept_count = kept.size\n",
    "    if kept_count == 0:\n",
    "        trimmed_min = float(np.min(all_vals))  # fallback if everything trimmed\n",
    "        kept_count = 0\n",
    "    else:\n",
    "        trimmed_min = float(np.min(kept))\n",
    "\n",
    "    return trimmed_min, global_max, p5, total, kept_count\n",
    "\n",
    "def _write_with_percentage(csv_path: Path, global_min: float, global_max: float) -> Path | None:\n",
    "    \"\"\"Add percentage column based on existing DIST_COL and write alongside original.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if DIST_COL not in df.columns:\n",
    "            print(f\"[SKIP] {csv_path.name}: missing '{DIST_COL}'\")\n",
    "            return None\n",
    "\n",
    "        dist = pd.to_numeric(df[DIST_COL], errors=\"coerce\")\n",
    "        if np.isfinite(global_min) and np.isfinite(global_max) and (global_max > global_min):\n",
    "            pct = (dist - global_min) / (global_max - global_min) * 100.0\n",
    "        else:\n",
    "            pct = pd.Series(np.where(dist.notna(), 0.0, np.nan), index=df.index)\n",
    "\n",
    "        df[PCT_COL] = pct\n",
    "        out_path = csv_path.with_name(csv_path.stem + \".csv\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        return out_path\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_distances_to_percentage(main_directory: str | Path) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    For each fly:\n",
    "      - Use <fly>/Eye_Antenna_Dist/*combined.csv (updated in place by prior script).\n",
    "      - Compute 5th percentile cutoff across those CSVs.\n",
    "      - Trim values < p5; use min(remaining) as global_min; global (untrimmed) max as global_max.\n",
    "      - Write new CSVs with 'distance_class1_class2_pct' appended.\n",
    "    Returns a summary dict.\n",
    "    \"\"\"\n",
    "    main_directory = Path(main_directory)\n",
    "    assert main_directory.is_dir(), f\"Not a directory: {main_directory}\"\n",
    "\n",
    "    summary = {}\n",
    "    for fly_dir in _find_fly_dirs(main_directory):\n",
    "        csvs = _gather_csvs_for_fly(fly_dir)\n",
    "        if not csvs:\n",
    "            continue\n",
    "\n",
    "        mm = _compute_trimmed_min_and_global_max(csvs)\n",
    "        if mm is None:\n",
    "            summary[fly_dir.name] = {\"files\": len(csvs), \"written\": 0, \"note\": \"No valid distances found\"}\n",
    "            print(f\"[INFO] {fly_dir.name}: no valid '{DIST_COL}' values across files.\")\n",
    "            continue\n",
    "\n",
    "        trimmed_min, global_max, p5, total_cnt, kept_cnt = mm\n",
    "        written = 0\n",
    "        for p in csvs:\n",
    "            out = _write_with_percentage(p, trimmed_min, global_max)\n",
    "            written += int(out is not None)\n",
    "\n",
    "        summary[fly_dir.name] = {\n",
    "            \"files\": len(csvs),\n",
    "            \"written\": written,\n",
    "            \"p5_cutoff\": p5,\n",
    "            \"trimmed_min\": trimmed_min,\n",
    "            \"global_max\": global_max,\n",
    "            \"total_points\": total_cnt,\n",
    "            \"kept_points\": kept_cnt,\n",
    "        }\n",
    "        print(f\"[DONE] {fly_dir.name}: p5={p5:.6g}, trimmed_min={trimmed_min:.6g}, \"\n",
    "              f\"global_max={global_max:.6g}, kept={kept_cnt}/{total_cnt}, wrote {written}/{len(csvs)}\")\n",
    "    return summary\n",
    "\n",
    "# --- Example usage ---\n",
    "report = normalize_distances_to_percentage(main_directory)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57611846-b84c-4fac-9464-1246e952ea4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Heat maps using in-place files from Eye_Antenna_Dist/*combined.csv\n",
    "# If distance_class1_class2_pct is missing, compute it IN PLACE (no new CSVs).\n",
    "# Per-fly normalization: trim bottom 5% globally; min = smallest remaining; max = global (untrimmed).\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import Normalize\n",
    "from datetime import datetime\n",
    "\n",
    "# ───────── SETTINGS ─────────\n",
    "DEBUG = True\n",
    "FPS_DEFAULT = 40.0\n",
    "SRC_DIRNAME = \"Eye_Antenna_Dist\"\n",
    "OUT_DIRNAME = \"heat_maps_distance_pct\"     # under <fly>/<SRC_DIRNAME>/\n",
    "PCT_COL = \"distance_class1_class2_pct\"\n",
    "DIST_COL = \"distance_class1_class2\"\n",
    "TRIM_FRAC = 0.05  # bottom 5% trim per fly when computing PCT if missing\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'lines.linewidth': 2,\n",
    "    'axes.linewidth': 1.5,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'legend.fontsize': 12\n",
    "})\n",
    "\n",
    "def dprint(*args):\n",
    "    if DEBUG:\n",
    "        print(*args)\n",
    "\n",
    "# Robust timestamp parser (supports \"HH:MM:SS:MS\", \"HH:MM:SS\", \"MM:SS\", seconds)\n",
    "def timestamp_to_seconds(ts):\n",
    "    if pd.isna(ts):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(ts)\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(ts).strip()\n",
    "    parts = s.split(\":\")\n",
    "    try:\n",
    "        if len(parts) == 4:\n",
    "            hh, mm, ss, ms = parts\n",
    "            return int(hh)*3600 + int(mm)*60 + int(ss) + int(ms)/1000.0\n",
    "        if len(parts) == 3:\n",
    "            hh, mm, ss = parts\n",
    "            return int(hh)*3600 + int(mm)*60 + float(ss)\n",
    "        if len(parts) == 2:\n",
    "            mm, ss = parts\n",
    "            return int(mm)*60 + float(ss)\n",
    "        if len(parts) == 1:\n",
    "            return float(parts[0])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def collect_fly_folders(main_directory: Path) -> list[Path]:\n",
    "    return [p for p in Path(main_directory).iterdir() if p.is_dir()]\n",
    "\n",
    "def find_col(df, candidates):\n",
    "    cols = set(df.columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _derive_fps(df) -> float:\n",
    "    fps_col = find_col(df, [\"fps\",\"FPS\",\"frame_rate\",\"frameRate\"])\n",
    "    if fps_col is not None:\n",
    "        try:\n",
    "            fps_val = pd.to_numeric(df[fps_col], errors=\"coerce\").median()\n",
    "            if np.isfinite(fps_val) and fps_val > 0:\n",
    "                return float(fps_val)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return FPS_DEFAULT\n",
    "\n",
    "def _ensure_time_series(df, frame_col, ts_col, debug_lines) -> tuple[pd.Series, dict]:\n",
    "    meta = {'used': None, 'valid_ts': 0, 'total': len(df), 'fps': None}\n",
    "\n",
    "    if ts_col is not None:\n",
    "        if ts_col in [\"time_seconds\", \"relative_time\"]:\n",
    "            ts = pd.to_numeric(df[ts_col], errors=\"coerce\")\n",
    "        else:\n",
    "            ts = df[ts_col].apply(timestamp_to_seconds)\n",
    "        valid = ts.notna().sum()\n",
    "        meta['valid_ts'] = int(valid)\n",
    "        if valid >= 2 and np.nanmax(np.diff(ts.dropna().values)) > 0:\n",
    "            meta['used'] = 'timestamp'\n",
    "            return ts, meta\n",
    "        debug_lines.append(\n",
    "            f\"Timestamp parse insufficient (valid={valid}/{len(df)} or non-increasing). Falling back.\"\n",
    "        )\n",
    "\n",
    "    if frame_col is not None:\n",
    "        frames = pd.to_numeric(df[frame_col], errors=\"coerce\")\n",
    "        if frames.notna().sum() >= 2:\n",
    "            fps = _derive_fps(df)\n",
    "            meta['used'] = 'frame_fallback'\n",
    "            meta['fps'] = fps\n",
    "            f0 = int(np.nanmin(frames.values))\n",
    "            ts_fb = (frames - f0) / fps\n",
    "            return ts_fb, meta\n",
    "\n",
    "    fps = _derive_fps(df)\n",
    "    meta['used'] = 'index_fallback'\n",
    "    meta['fps'] = fps\n",
    "    ts_fb = pd.Series(np.arange(len(df), dtype=float) / fps, index=df.index)\n",
    "    return ts_fb, meta\n",
    "\n",
    "# ───────── Per-fly PCT computation (in-place if missing) ─────────\n",
    "\n",
    "def _gather_inplace_csvs(fly_dir: Path) -> list[Path]:\n",
    "    src = fly_dir / SRC_DIRNAME\n",
    "    if not src.is_dir():\n",
    "        return []\n",
    "    return sorted(src.glob(\"*combined.csv\"))\n",
    "\n",
    "def _compute_fly_trim_min_and_max(csv_paths: list[Path]) -> tuple[float, float] | None:\n",
    "    vals = []\n",
    "    for p in csv_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p, usecols=[DIST_COL])\n",
    "            v = pd.to_numeric(df[DIST_COL], errors=\"coerce\").to_numpy()\n",
    "            vals.append(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not vals:\n",
    "        return None\n",
    "    all_vals = np.concatenate(vals)\n",
    "    all_vals = all_vals[np.isfinite(all_vals)]\n",
    "    if all_vals.size == 0:\n",
    "        return None\n",
    "    global_max = float(np.max(all_vals))\n",
    "    p5 = float(np.percentile(all_vals, 100*TRIM_FRAC, method=\"linear\"))\n",
    "    kept = all_vals[all_vals >= p5]\n",
    "    trimmed_min = float(np.min(kept)) if kept.size else float(np.min(all_vals))\n",
    "    return trimmed_min, global_max\n",
    "\n",
    "def _ensure_pct_inplace(csv_paths: list[Path], fly_min: float, fly_max: float) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Add PCT_COL in place (no new files) where missing.\n",
    "    \"\"\"\n",
    "    updated = []\n",
    "    for p in csv_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if PCT_COL not in df.columns:\n",
    "                dist = pd.to_numeric(df.get(DIST_COL, np.nan), errors=\"coerce\")\n",
    "                if np.isfinite(fly_min) and np.isfinite(fly_max) and (fly_max > fly_min):\n",
    "                    pct = (dist - fly_min) / (fly_max - fly_min) * 100.0\n",
    "                else:\n",
    "                    pct = pd.Series(np.where(dist.notna(), 0.0, np.nan), index=df.index)\n",
    "                df[PCT_COL] = pct\n",
    "                df.to_csv(p, index=False)  # overwrite in place\n",
    "                updated.append(p)\n",
    "        except Exception as e:\n",
    "            dprint(f\"[WARN] Could not add {PCT_COL} to {p.name}: {e}\")\n",
    "    return updated\n",
    "\n",
    "# ───────── Heatmap building ─────────\n",
    "\n",
    "def build_trial_from_csv(csv_path: Path, debug_lines):\n",
    "    \"\"\"\n",
    "    Returns (trial_dict, reason_if_skipped)\n",
    "    trial_dict keys: label, time, data, odor_start, odor_end\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        frame_col = find_col(df, [\"frame\", \"Frame\", \"frame_num\", \"frame_index\"])\n",
    "        ts_col    = find_col(df, [\"timestamp\", \"Timestamp\", \"time\", \"Time\", \"time_seconds\", \"relative_time\"])\n",
    "        ofm_col   = find_col(df, [\"OFM State\", \"OFM_State\", \"ofm_state\", \"ofm\"])\n",
    "\n",
    "        if PCT_COL not in df.columns:\n",
    "            return None, f\"Missing '{PCT_COL}'\"\n",
    "\n",
    "        # Build time_seconds with robust fallback\n",
    "        time_seconds, meta = _ensure_time_series(df, frame_col, ts_col, debug_lines)\n",
    "        df[\"time_seconds\"] = pd.to_numeric(time_seconds, errors=\"coerce\")\n",
    "        valid_after = df[\"time_seconds\"].notna().sum()\n",
    "\n",
    "        debug_lines.append(\n",
    "            f\"{csv_path.name}: time_source={meta['used']}, valid_ts={meta['valid_ts']}/{meta['total']}, fps={meta['fps']}\"\n",
    "        )\n",
    "\n",
    "        if valid_after < 2:\n",
    "            return None, \"Too few usable time points after fallback\"\n",
    "\n",
    "        df[\"relative_time\"] = df[\"time_seconds\"] - df[\"time_seconds\"].min()\n",
    "\n",
    "        # Odor window detection (if present)\n",
    "        pre_sec, post_sec = 30.0, 90.0\n",
    "        odor_duration = 30.0\n",
    "        if ofm_col is not None:\n",
    "            ofm_series = df[ofm_col].astype(str).str.lower()\n",
    "            odor_mask = ofm_series == \"during\"\n",
    "            if odor_mask.any():\n",
    "                rt = df.loc[odor_mask, \"relative_time\"]\n",
    "                if len(rt) >= 2:\n",
    "                    odor_duration = max(float(rt.iloc[-1] - rt.iloc[0]), 0.0)\n",
    "\n",
    "        trial_label = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "        final_total_duration = pre_sec + odor_duration + post_sec\n",
    "\n",
    "        if frame_col is not None:\n",
    "            frames = pd.to_numeric(df[frame_col], errors=\"coerce\").dropna().astype(int)\n",
    "        else:\n",
    "            frames = pd.Series(np.arange(len(df), dtype=int), index=df.index)\n",
    "\n",
    "        if frames.empty:\n",
    "            return None, \"No valid frame information\"\n",
    "\n",
    "        total_frames = np.arange(frames.min(), frames.max() + 1, dtype=int)\n",
    "        full_data = np.full(total_frames.shape, np.nan, dtype=float)\n",
    "\n",
    "        idx_map = {f: i for i, f in enumerate(total_frames)}\n",
    "        present_idx = [idx_map.get(int(f)) for f in frames if int(f) in idx_map]\n",
    "        if len(present_idx) == 0:\n",
    "            return None, \"Frame alignment failed (no overlap).\"\n",
    "\n",
    "        vals = pd.to_numeric(df.loc[frames.index, PCT_COL], errors=\"coerce\").to_numpy()\n",
    "        full_data[np.array(present_idx, dtype=int)] = vals\n",
    "\n",
    "        data_for_plot = full_data.copy()\n",
    "        data_for_plot[data_for_plot == -1]  = -0.5\n",
    "        data_for_plot[data_for_plot == 101] = 101\n",
    "\n",
    "        new_time = np.linspace(0, final_total_duration, len(total_frames))\n",
    "\n",
    "        return {\n",
    "            \"label\": trial_label,\n",
    "            \"time\": new_time,\n",
    "            \"data\": data_for_plot,\n",
    "            \"odor_start\": pre_sec,\n",
    "            \"odor_end\": pre_sec + odor_duration\n",
    "        }, None\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, f\"Exception: {e}\"\n",
    "\n",
    "def write_debug_log(out_dir: Path, lines: list[str]):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_path = out_dir / \"_debug_heatmap_log.txt\"\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\\n===== {datetime.now().isoformat(timespec='seconds')} =====\\n\")\n",
    "        for ln in lines:\n",
    "            f.write(ln.rstrip() + \"\\n\")\n",
    "    dprint(f\"[LOG] {log_path}\")\n",
    "\n",
    "def plot_fly(main_directory: Path, fly_dir: Path):\n",
    "    \"\"\"\n",
    "    Build TWO heatmaps per fly from in-place files <fly>/Eye_Antenna_Dist/*combined.csv:\n",
    "      - <fly>_testing_heatmap_distance_pct.png  (files with 'testing' in name)\n",
    "      - <fly>_training_heatmap_distance_pct.png (files with 'training' in name)\n",
    "    If PCT_COL is missing, compute it in-place first (per-fly trimmed-min / global-max).\n",
    "    \"\"\"\n",
    "    src_dir = fly_dir / SRC_DIRNAME\n",
    "    out_dir = src_dir / OUT_DIRNAME\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    debug_lines = [\n",
    "        f\"Fly: {fly_dir.name}\",\n",
    "        f\"Input dir: {src_dir}\",\n",
    "        f\"Output dir: {out_dir}\"\n",
    "    ]\n",
    "\n",
    "    # 1) Gather in-place CSVs\n",
    "    csv_files = _gather_inplace_csvs(fly_dir)\n",
    "    if not csv_files:\n",
    "        write_debug_log(out_dir, debug_lines + [\"No CSVs to process.\"])\n",
    "        return\n",
    "\n",
    "    # 2) Ensure PCT exists in-place for ALL files (compute per-fly min/max once)\n",
    "    stats = _compute_fly_trim_min_and_max(csv_files)\n",
    "    if stats is None:\n",
    "        write_debug_log(out_dir, debug_lines + [f\"No valid '{DIST_COL}' values across files.\"])\n",
    "        return\n",
    "    fly_min, fly_max = stats\n",
    "    updated = _ensure_pct_inplace(csv_files, fly_min, fly_max)\n",
    "    if updated:\n",
    "        debug_lines.append(f\"Added {PCT_COL} to: {', '.join(p.name for p in updated)}\")\n",
    "\n",
    "    # 3) Split by category using filename\n",
    "    testing_files  = [p for p in csv_files if \"testing\"  in p.name.lower()]\n",
    "    training_files = [p for p in csv_files if \"training\" in p.name.lower()]\n",
    "\n",
    "    # Helper to plot one category\n",
    "    def _plot_category(cat_name: str, files: list[Path]):\n",
    "        if not files:\n",
    "            debug_lines.append(f\"No {cat_name} files; skipping {cat_name} figure.\")\n",
    "            return\n",
    "        trials, skipped_files = [], 0\n",
    "        for csv_path in sorted(files):\n",
    "            tr, reason = build_trial_from_csv(csv_path, debug_lines)\n",
    "            if tr is None:\n",
    "                skipped_files += 1\n",
    "                msg = f\"{csv_path.name}: {reason}\"\n",
    "                dprint(f\"  [SKIP FILE] {msg}\")\n",
    "                debug_lines.append(f\"SKIP FILE: {msg}\")\n",
    "            else:\n",
    "                trials.append(tr)\n",
    "        if not trials:\n",
    "            debug_lines.append(f\"All {cat_name} files skipped ({skipped_files} skipped). No figure produced.\")\n",
    "            return\n",
    "\n",
    "        # Colormap and norm (linear 0–100 %; NaN→dimgray, under/over→pink)\n",
    "        cmap = mpl.colormaps['viridis'].copy()\n",
    "        cmap.set_under('pink')\n",
    "        cmap.set_over('pink')\n",
    "        cmap.set_bad('dimgray')\n",
    "        norm = Normalize(vmin=0, vmax=100)\n",
    "\n",
    "        n = len(trials)\n",
    "        fig_h = max(2, 2 * n)\n",
    "        fig, axs = plt.subplots(n, 1, figsize=(18, fig_h), sharex=True)\n",
    "        if n == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        legend_handles = [Line2D([0], [0], color='red', linewidth=2.5, label='Odor Period')]\n",
    "        pcm_last = None\n",
    "\n",
    "        for i, tr in enumerate(trials):\n",
    "            ax = axs[i]\n",
    "            time_edges = np.linspace(tr[\"time\"][0], tr[\"time\"][-1], len(tr[\"time\"]) + 1)\n",
    "            X, Y = np.meshgrid(time_edges, [0, 1])\n",
    "            row = tr[\"data\"].reshape(1, -1)\n",
    "            pcm_last = ax.pcolormesh(X, Y, row, cmap=cmap, shading='auto', norm=norm)\n",
    "            ax.set_yticks([])\n",
    "            ax.tick_params(axis='x', direction='out')\n",
    "            ax.set_xlim(tr[\"time\"][0], tr[\"time\"][-1])\n",
    "            ax.set_title(tr[\"label\"], loc='left')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.axvline(tr[\"odor_start\"], color='red', linewidth=2.5)\n",
    "            ax.axvline(tr[\"odor_end\"],   color='red', linewidth=2.5)\n",
    "\n",
    "        axs[-1].set_xlabel(\"Time (seconds)\")\n",
    "        fly_name = fly_dir.name\n",
    "        fig.suptitle(f\"{fly_name} – {cat_name.capitalize()} Trials (distance_class1_class2_pct)\", fontsize=20)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "        cbar = fig.colorbar(pcm_last, ax=axs, orientation='vertical', fraction=0.02, pad=0.04, extend='both')\n",
    "        cbar.set_label(\"Distance Percentage (%)\", fontsize=14)\n",
    "        fig.legend(handles=legend_handles, loc='upper right', frameon=True)\n",
    "\n",
    "        out_path = out_dir / f\"{fly_name}_{cat_name}_heatmap_distance_pct.png\"\n",
    "        fig.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        dprint(f\"[SAVED] {out_path}\")\n",
    "        debug_lines.append(f\"SAVED: {out_path}\")\n",
    "\n",
    "    # 4) Emit two PNGs (if files exist)\n",
    "    _plot_category(\"testing\", testing_files)\n",
    "    _plot_category(\"training\", training_files)\n",
    "\n",
    "    # 5) Write combined debug log\n",
    "    write_debug_log(out_dir, debug_lines)\n",
    "\n",
    "def plot_all_flies(main_directory: str | Path):\n",
    "    \"\"\"\n",
    "    Scan only fly folders whose names START with a calendar month (full or 3-letter abbrev),\n",
    "    case-insensitive. Examples matched: 'August_19_fly_1', 'aug_12_fly_3', 'Dec-05_fly_2'.\n",
    "    \"\"\"\n",
    "    main_directory = Path(main_directory)\n",
    "    assert main_directory.is_dir(), f\"Not a directory: {main_directory}\"\n",
    "    dprint(f\"Scanning main_directory: {main_directory}\")\n",
    "\n",
    "    # Month prefixes to match (lowercase)\n",
    "    month_prefixes = (\n",
    "        \"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "        \"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\n",
    "        \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"\n",
    "    )\n",
    "\n",
    "    # Only include immediate subfolders starting with a month prefix\n",
    "    all_dirs = collect_fly_folders(main_directory)\n",
    "    fly_dirs = [p for p in all_dirs if p.name.lower().startswith(month_prefixes)]\n",
    "\n",
    "    if not fly_dirs:\n",
    "        print(f\"[WARN] No month-prefixed fly folders found in {main_directory}\")\n",
    "        return\n",
    "\n",
    "    for fly_dir in fly_dirs:\n",
    "        dprint(f\"\\n== Fly: {fly_dir.name} ==\")\n",
    "        plot_fly(main_directory, fly_dir)\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# main_directory = \"/path/to/root/with/fly_folders\"\n",
    "plot_all_flies(main_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fcbb6a-e38c-41bf-9384-d7e24c23cb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Collect all heatmap PNGs into <main_directory>/heat_maps/Eye_Antenna_Dist/\n",
    "# Organizes into subfolders: testing/, training/, other/\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Adjust if you want to move instead of copy\n",
    "MOVE_FILES = False  # False = copy2 (preserve originals), True = move\n",
    "\n",
    "# Source pattern produced by your plotting code\n",
    "SRC_SUBPATH = (\"Eye_Antenna_Dist\", \"heat_maps_distance_pct\")\n",
    "\n",
    "# Month-prefixed folder filter (full + common abbreviations)\n",
    "_MONTH_PREFIXES = (\n",
    "    \"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "    \"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\n",
    "    \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"sept\",\"oct\",\"nov\",\"dec\"\n",
    ")\n",
    "\n",
    "def _is_month_prefixed(name: str) -> bool:\n",
    "    return name.lower().startswith(_MONTH_PREFIXES)\n",
    "\n",
    "def collect_heatmaps(main_directory: str | Path, *, move: bool = MOVE_FILES) -> int:\n",
    "    \"\"\"\n",
    "    Gather all PNGs from <fly>/Eye_Antenna_Dist/heat_maps_distance_pct/*.png\n",
    "    where <fly> starts with a month, and copy/move them to:\n",
    "        <main_directory>/heat_maps/Eye_Antenna_Dist/{testing|training|other}/\n",
    "    Returns the number of images copied/moved.\n",
    "    \"\"\"\n",
    "    root = Path(main_directory)\n",
    "    assert root.is_dir(), f\"Not a directory: {root}\"\n",
    "\n",
    "    dest_root = root / \"heat_maps\" / \"Eye_Antenna_Dist\"\n",
    "    (dest_root / \"testing\").mkdir(parents=True, exist_ok=True)\n",
    "    (dest_root / \"training\").mkdir(parents=True, exist_ok=True)\n",
    "    (dest_root / \"other\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n = 0\n",
    "    for fly_dir in (p for p in root.iterdir() if p.is_dir() and _is_month_prefixed(p.name)):\n",
    "        src_dir = fly_dir.joinpath(*SRC_SUBPATH)\n",
    "        if not src_dir.is_dir():\n",
    "            continue\n",
    "        for img in sorted(src_dir.glob(\"*.png\")):\n",
    "            name_l = img.name.lower()\n",
    "            category = \"testing\" if \"testing\" in name_l else (\"training\" if \"training\" in name_l else \"other\")\n",
    "            dest_dir = dest_root / category\n",
    "            dest_path = dest_dir / img.name\n",
    "\n",
    "            # Avoid collisions by suffixing _1, _2, ...\n",
    "            if dest_path.exists():\n",
    "                stem, ext = dest_path.stem, dest_path.suffix\n",
    "                k = 1\n",
    "                while (dest_dir / f\"{stem}_{k}{ext}\").exists():\n",
    "                    k += 1\n",
    "                dest_path = dest_dir / f\"{stem}_{k}{ext}\"\n",
    "\n",
    "            if move:\n",
    "                shutil.move(str(img), dest_path)\n",
    "                action = \"Moved\"\n",
    "            else:\n",
    "                shutil.copy2(str(img), dest_path)\n",
    "                action = \"Copied\"\n",
    "\n",
    "            print(f\"{action}: {img} -> {dest_path}\")\n",
    "            n += 1\n",
    "\n",
    "    print(f\"Total {'moved' if move else 'copied'}: {n}\")\n",
    "    return n\n",
    "\n",
    "# --- Example usage ---\n",
    "collect_heatmaps(main_directory, move=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41e045-2a78-4b38-ace1-09170751367c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Mega Heat Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4e88d-e3cc-47ec-b5e1-fb25aca955ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Mega heatmaps per fly (training & testing), adapted to new layout\n",
    "# Row 1: Eye_Prob_Dist\n",
    "# Row 2: Centered angle % (month folders, *_distance_class_2_angle_ARB.csv preferred)\n",
    "# Row 3: Eye_Antenna_Dist (trim-5% min / global max fallback if pct missing)\n",
    "\n",
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.transforms import Bbox\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 13,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'lines.linewidth': 2,\n",
    "    'axes.linewidth': 1.25,\n",
    "    'figure.dpi': 160,\n",
    "    'savefig.dpi': 300,\n",
    "    'legend.fontsize': 11\n",
    "})\n",
    "\n",
    "# ───────── Config ─────────\n",
    "FPS_DEFAULT   = 40.0\n",
    "VIRIDIS_NORM  = Normalize(vmin=0,   vmax=100)\n",
    "COOLWARM_NORM = Normalize(vmin=-100, vmax=100)\n",
    "ANCHOR_X, ANCHOR_Y = 1080.0, 540.0\n",
    "\n",
    "PRE_SEC, POST_SEC = 30.0, 90.0\n",
    "TRIM_FRAC = 0.05  # bottom 5% trim (Eye_Antenna_Dist fallback pct normalization)\n",
    "\n",
    "MONTHS = (\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "          \"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\n",
    "          \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\")\n",
    "\n",
    "# ───────── Utilities ─────────\n",
    "def timestamp_to_seconds(ts) -> float:\n",
    "    if pd.isna(ts): return np.nan\n",
    "    try: return float(ts)\n",
    "    except Exception: pass\n",
    "    s = str(ts).strip(); parts = s.split(\":\")\n",
    "    try:\n",
    "        if len(parts) == 4: hh,mm,ss,ms = parts; return int(hh)*3600+int(mm)*60+int(ss)+int(ms)/1000.0\n",
    "        if len(parts) == 3: hh,mm,ss   = parts; return int(hh)*3600+int(mm)*60+float(ss)\n",
    "        if len(parts) == 2: mm,ss      = parts; return int(mm)*60+float(ss)\n",
    "        if len(parts) == 1: return float(parts[0])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def find_col(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    cols = set(df.columns)\n",
    "    for c in cands:\n",
    "        if c in cols: return c\n",
    "    return None\n",
    "\n",
    "def derive_fps(df: pd.DataFrame) -> float:\n",
    "    fps_col = find_col(df, [\"fps\",\"FPS\",\"frame_rate\",\"frameRate\"])\n",
    "    if fps_col is not None:\n",
    "        val = pd.to_numeric(df[fps_col], errors=\"coerce\").median()\n",
    "        if np.isfinite(val) and val > 0: return float(val)\n",
    "    return FPS_DEFAULT\n",
    "\n",
    "def ensure_time_series(df: pd.DataFrame, frame_col: Optional[str], ts_col: Optional[str]):\n",
    "    # robust: timestamp → frame → index\n",
    "    if ts_col is not None:\n",
    "        if ts_col in [\"time_seconds\",\"relative_time\",\"time_s\"]:\n",
    "            ts = pd.to_numeric(df[ts_col], errors=\"coerce\")\n",
    "        else:\n",
    "            ts = df[ts_col].apply(timestamp_to_seconds)\n",
    "        if ts.notna().sum() >= 2 and np.nanmax(np.diff(ts.dropna().values)) > 0:\n",
    "            return ts, {'used':'timestamp'}\n",
    "    if frame_col is not None:\n",
    "        frames = pd.to_numeric(df[frame_col], errors=\"coerce\")\n",
    "        if frames.notna().sum() >= 2:\n",
    "            fps = derive_fps(df); f0 = int(np.nanmin(frames.values))\n",
    "            return (frames - f0)/fps, {'used':'frame_fallback','fps':fps}\n",
    "    fps = derive_fps(df)\n",
    "    return pd.Series(np.arange(len(df), dtype=float)/fps, index=df.index), {'used':'index_fallback','fps':fps}\n",
    "\n",
    "def _extract_trial_fallback(stem: str, category: str | None) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Robust trial index extraction:\n",
    "      1) '{category}_(\\d+)' or '(training|testing)_(\\d+)'.\n",
    "      2) last numeric group in the stem.\n",
    "      3) None → caller may auto-assign.\n",
    "    \"\"\"\n",
    "    s = stem.lower()\n",
    "    if category:\n",
    "        m = re.search(rf\"{category}_(\\d+)\", s)\n",
    "        if m:\n",
    "            try: return int(m.group(1))\n",
    "            except: pass\n",
    "    m2 = re.search(r\"(training|testing)_(\\d+)\", s)\n",
    "    if m2:\n",
    "        try: return int(m2.group(2))\n",
    "        except: pass\n",
    "    m3 = re.findall(r\"(\\d+)\", s)\n",
    "    if m3:\n",
    "        try: return int(m3[-1])\n",
    "        except: pass\n",
    "    return None\n",
    "    \n",
    "def odor_window_from_ofm(df: pd.DataFrame, time_col: str = \"relative_time\"):\n",
    "    ofm_col = find_col(df, [\"OFM State\",\"OFM_State\",\"ofm_state\",\"ofm\"])\n",
    "    if ofm_col is None or time_col not in df.columns: return None\n",
    "    try:\n",
    "        mask = df[ofm_col].astype(str).str.lower().eq(\"during\")\n",
    "        if mask.any():\n",
    "            t = df[time_col]; idx = np.flatnonzero(mask.to_numpy())\n",
    "            if idx.size >= 2: return float(t.iloc[idx[0]]), float(t.iloc[idx[-1]])\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def build_row(time_seconds: np.ndarray, values: np.ndarray, odor_start: float, odor_end: float,\n",
    "              label: str, apply_sentinels=True) -> dict:\n",
    "    if len(time_seconds) < 2: return {}\n",
    "    time_edges = np.linspace(time_seconds[0], time_seconds[-1], len(time_seconds) + 1)\n",
    "    row_vals = np.array(values, dtype=float).copy()\n",
    "    if apply_sentinels:\n",
    "        row_vals[row_vals == -1]  = -0.5\n",
    "        row_vals[row_vals == 101] = 101\n",
    "    return {\"label\": label, \"time_edges\": time_edges, \"data_row\": row_vals.reshape(1,-1),\n",
    "            \"odor_start\": float(odor_start), \"odor_end\": float(odor_end)}\n",
    "\n",
    "# ───────── Angle helpers (per your angle heatmap) ─────────\n",
    "def compute_angle_deg_at_point2(df: pd.DataFrame) -> pd.Series:\n",
    "    req = [\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"]\n",
    "    if any(c not in df.columns for c in req): raise ValueError(f\"Missing angle columns: {req}\")\n",
    "    p2x = df[\"x_class2\"].astype(float).to_numpy(); p2y = df[\"y_class2\"].astype(float).to_numpy()\n",
    "    p3x = df[\"x_class6\"].astype(float).to_numpy(); p3y = df[\"y_class6\"].astype(float).to_numpy()\n",
    "    ux, uy = (ANCHOR_X - p2x), (ANCHOR_Y - p2y)\n",
    "    vx, vy = (p3x - p2x), (p3y - p2y)\n",
    "    dot = ux*vx + uy*vy; cross = ux*vy - uy*vx\n",
    "    n1 = np.hypot(ux, uy); n2 = np.hypot(vx, vy)\n",
    "    valid = (n1 > 0) & (n2 > 0) & np.isfinite(dot) & np.isfinite(cross)\n",
    "    ang = np.full(len(p2x), np.nan); ang[valid] = np.arctan2(np.abs(cross[valid]), dot[valid])\n",
    "    return pd.Series(np.degrees(ang), index=df.index, name=\"angle_ARB_deg\")\n",
    "\n",
    "def find_fly_reference_angle(csvs_raw: List[Path]) -> float:\n",
    "    best = None\n",
    "    for p in csvs_raw:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if not {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df.columns): continue\n",
    "            ang = compute_angle_deg_at_point2(df)\n",
    "            dist_col = find_col(df, [\"distance_percentage\",\"distance_percent\",\"distance_pct\",\n",
    "                                     \"distance_class1_class2_pct\"])\n",
    "            if dist_col is None: continue\n",
    "            dist = pd.to_numeric(df[dist_col], errors=\"coerce\").to_numpy()\n",
    "            exact = np.flatnonzero(dist == 0)\n",
    "            if exact.size > 0:\n",
    "                idx = int(exact[0]); angle_here = float(ang.iloc[idx]) if np.isfinite(ang.iloc[idx]) else np.nan\n",
    "                cand = (0, 0.0, angle_here)\n",
    "            else:\n",
    "                with np.errstate(invalid=\"ignore\"): absd = np.abs(dist)\n",
    "                if not np.isfinite(absd).any(): continue\n",
    "                idx = int(np.nanargmin(absd)); angle_here = float(ang.iloc[idx]) if np.isfinite(ang.iloc[idx]) else np.nan\n",
    "                cand = (1, float(absd[idx]), angle_here)\n",
    "            if best is None or cand < best: best = cand\n",
    "        except Exception:\n",
    "            pass\n",
    "    return best[2] if best is not None else np.nan\n",
    "\n",
    "def compute_fly_max_abs_centered(csvs_raw: List[Path], ref_angle: float) -> float:\n",
    "    fly_max = 0.0\n",
    "    for p in csvs_raw:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if not {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df.columns): continue\n",
    "            ang = compute_angle_deg_at_point2(df)\n",
    "            centered = ang - ref_angle if np.isfinite(ref_angle) else ang*0.0\n",
    "            local = np.nanmax(np.abs(centered.to_numpy(dtype=float)))\n",
    "            if np.isfinite(local): fly_max = max(fly_max, float(local))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fly_max if np.isfinite(fly_max) and fly_max > 0 else np.nan\n",
    "\n",
    "# ───────── Row collectors (new locations) ─────────\n",
    "def collect_rows_eye_prob_dist(fly_dir: Path, category: str) -> Dict[int, dict]:\n",
    "    \"\"\"\n",
    "    Eye_Prob_Dist/<category>/*.csv  (or fallback: Eye_Prob_Dist/*category*.csv)\n",
    "    Expects a *percentage* column (tries common names).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    base = fly_dir / \"Eye_Prob_Dist\" / category\n",
    "    candidates = []\n",
    "    if base.is_dir():\n",
    "        candidates = sorted([p for p in base.glob(\"*.csv\") if p.is_file()])\n",
    "    else:\n",
    "        root = fly_dir / \"Eye_Prob_Dist\"\n",
    "        if root.is_dir():\n",
    "            candidates = sorted([p for p in root.glob(\"*.csv\") if category in p.name.lower()])\n",
    "\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            df = pd.read_csv(p); df.columns = df.columns.str.strip()\n",
    "            frame_col = find_col(df, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "            ts_col    = find_col(df, [\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\",\"time_s\"])\n",
    "            pct_col   = find_col(df, [\n",
    "                \"distance_percentage\",\"distance_percent\",\"distance_pct\",\n",
    "                \"distance_proboscis_eye_pct\",\"proboscis_eye_distance_pct\",\n",
    "                \"eye_prob_distance_pct\",\"distance_class1_class2_pct\"  # keep broad\n",
    "            ])\n",
    "            if pct_col is None: \n",
    "                continue  # nothing sensible to plot here\n",
    "            ts, _ = ensure_time_series(df, frame_col, ts_col)\n",
    "            if ts.notna().sum() < 2: \n",
    "                continue\n",
    "            # relative timeline with default odor window if OFM missing\n",
    "            time_s = pd.to_numeric(ts, errors=\"coerce\"); t0 = float(np.nanmin(time_s))\n",
    "            rel = time_s - t0\n",
    "            df[\"relative_time\"] = rel\n",
    "            odor = odor_window_from_ofm(df, \"relative_time\")\n",
    "            if odor is None:\n",
    "                odor = (PRE_SEC, PRE_SEC + 30.0)\n",
    "            odor_start, odor_end = odor\n",
    "            total_duration = PRE_SEC + (odor_end - odor_start) + POST_SEC\n",
    "\n",
    "            # build dense frame-aligned row if frames exist; else use raw length\n",
    "            if frame_col is not None and frame_col in df.columns:\n",
    "                frames = pd.to_numeric(df[frame_col], errors=\"coerce\").dropna().astype(int)\n",
    "                total_frames = np.arange(frames.min(), frames.max()+1, dtype=int)\n",
    "                idx_map = {f:i for i,f in enumerate(total_frames)}\n",
    "                present_idx = [idx_map.get(int(f)) for f in frames if int(f) in idx_map]\n",
    "                vals = pd.to_numeric(df.loc[frames.index, pct_col], errors=\"coerce\").to_numpy()\n",
    "                full = np.full_like(total_frames, np.nan, dtype=float)\n",
    "                if present_idx: full[np.array(present_idx, dtype=int)] = vals\n",
    "                time_axis = np.linspace(0, total_duration, len(total_frames))\n",
    "            else:\n",
    "                vals = pd.to_numeric(df[pct_col], errors=\"coerce\").to_numpy()\n",
    "                time_axis = np.linspace(0, total_duration, len(vals)); full = vals\n",
    "\n",
    "            tri = _extract_trial_fallback(p.stem, category)\n",
    "            if tri is None: \n",
    "                continue\n",
    "            row = build_row(time_axis, full, PRE_SEC, PRE_SEC+(odor_end-odor_start), p.stem, True)\n",
    "            if row: out[tri] = row\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "# ───────── helpers (add if not present) ─────────\n",
    "# ───────── helpers (add if not present) ─────────\n",
    "MONTHS = (\n",
    "    \"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "    \"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\n",
    "    \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"\n",
    ")\n",
    "\n",
    "def _is_month_folder(p: Path) -> bool:\n",
    "    nm = p.name.lower()\n",
    "    return any(nm.startswith(m) for m in MONTHS)\n",
    "\n",
    "# (Optional) Stronger category inference: recognizes train/test shorthands.\n",
    "def _infer_category_from_path(p: Path) -> str | None:\n",
    "    tokens = \" \".join([*p.parts, p.stem]).lower()\n",
    "    if any(t in tokens for t in (\"training\",\"train\",\"trn\")): return \"training\"\n",
    "    if any(t in tokens for t in (\"testing\",\"test\",\"tst\")):  return \"testing\"\n",
    "    return None\n",
    "\n",
    "# REPLACE this function\n",
    "def collect_rows_angle_centered_pct(fly_dir: Path, category: str) -> Dict[int, dict]:\n",
    "    \"\"\"\n",
    "    Centered angle % for the mega heatmap:\n",
    "      - Prefer *_distance_class_2_angle_ARB.csv (use angle_centered_pct if present).\n",
    "      - Fallback to *_distance_class_2.csv and compute centered% per-fly.\n",
    "      - Searches the entire fly_dir (not limited to month folders).\n",
    "      - Robust trial inference and odor-window recovery from raw CSVs.\n",
    "    \"\"\"\n",
    "    out: Dict[int, dict] = {}\n",
    "\n",
    "    # 1) Discover candidates anywhere under the fly\n",
    "    angle_csvs = list(fly_dir.rglob(\"*_distance_class_2_angle_ARB.csv\"))\n",
    "    raw_csvs   = list(fly_dir.rglob(\"*_distance_class_2_angle_ARB.csv\"))\n",
    "\n",
    "    # Keep files that match the target category or are ambiguous\n",
    "    def _cat_match(p: Path, target: str) -> bool:\n",
    "        inf = _infer_category_from_path(p)\n",
    "        return (inf == target) or (inf is None)\n",
    "\n",
    "    angle_cat = [p for p in angle_csvs if _cat_match(p, category)]\n",
    "    raw_cat   = [p for p in raw_csvs   if _cat_match(p, category)]\n",
    "    if not angle_cat and not raw_cat:\n",
    "        return out\n",
    "\n",
    "    # 2) Per-fly reference and max from ALL raw (most stable)\n",
    "    raw_for_ref = raw_csvs if raw_csvs else raw_cat\n",
    "    ref_angle   = find_fly_reference_angle(raw_for_ref) if raw_for_ref else np.nan\n",
    "    fly_max_abs = compute_fly_max_abs_centered(raw_for_ref, ref_angle) if np.isfinite(ref_angle) else np.nan\n",
    "\n",
    "    # 3) Prefer angle files over raw when both exist for the same logical trial\n",
    "    def _base_key(p: Path) -> str:\n",
    "        s = p.stem\n",
    "        return s.replace(\"_distance_class_2_angle_ARB\",\"\").replace(\"_distance_class_2\",\"\")\n",
    "\n",
    "    chosen: Dict[str, Path] = {}\n",
    "    for p in sorted(raw_cat):\n",
    "        chosen.setdefault(_base_key(p), p)\n",
    "    for p in sorted(angle_cat):\n",
    "        chosen[_base_key(p)] = p  # overwrite with angle file\n",
    "\n",
    "    # 4) Build rows\n",
    "    for base in sorted(chosen.keys()):\n",
    "        p = chosen[base]\n",
    "        try:\n",
    "            df = pd.read_csv(p); df.columns = df.columns.str.strip()\n",
    "\n",
    "            # Trial index: explicit → last digits fallback\n",
    "            tri = _extract_trial_fallback(p.stem, category)\n",
    "            if tri is None:\n",
    "                nums = [int(x) for x in re.findall(r\"\\d+\", p.stem)]\n",
    "                if not nums: \n",
    "                    continue\n",
    "                tri = nums[-1]\n",
    "\n",
    "            # Centered %: use column if present; else compute from coords/raw\n",
    "            if \"angle_centered_pct\" in df.columns:\n",
    "                pct = pd.to_numeric(df[\"angle_centered_pct\"], errors=\"coerce\").to_numpy()\n",
    "            else:\n",
    "                if {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df.columns):\n",
    "                    ang = compute_angle_deg_at_point2(df).to_numpy(dtype=float)\n",
    "                else:\n",
    "                    partner = p.with_name(p.name.replace(\"_distance_class_2_angle_ARB.csv\",\"_distance_class_2.csv\"))\n",
    "                    if not partner.exists(): \n",
    "                        continue\n",
    "                    dfr = pd.read_csv(partner)\n",
    "                    ang = compute_angle_deg_at_point2(dfr).to_numpy(dtype=float)\n",
    "\n",
    "                if np.isfinite(fly_max_abs) and fly_max_abs > 0 and np.isfinite(ref_angle):\n",
    "                    pct = ((ang - ref_angle) / fly_max_abs) * 100.0\n",
    "                else:\n",
    "                    pct = np.zeros_like(ang, dtype=float)  # still renders; flags missing ref/max\n",
    "\n",
    "            # Odor window: try sibling/nearby raw; else default\n",
    "            odor = None\n",
    "            sib_raw = p.with_name(p.name.replace(\"_distance_class_2_angle_ARB.csv\",\"_distance_class_2.csv\"))\n",
    "            candidates = [sib_raw] if sib_raw.exists() else []\n",
    "            candidates += [q for q in p.parent.glob(\"*_distance_class_2.csv\") if _cat_match(q, category)]\n",
    "            if not candidates:\n",
    "                candidates = raw_cat  # last resort\n",
    "\n",
    "            for q in candidates:\n",
    "                try:\n",
    "                    dfr = pd.read_csv(q)\n",
    "                    fr_col = find_col(dfr, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "                    ts_col = find_col(dfr, [\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\",\"time_s\"])\n",
    "                    ts, _  = ensure_time_series(dfr, fr_col, ts_col)\n",
    "                    dfr[\"time_seconds\"] = pd.to_numeric(ts, errors=\"coerce\")\n",
    "                    dfr[\"relative_time\"] = dfr[\"time_seconds\"] - dfr[\"time_seconds\"].min()\n",
    "                    odor = odor_window_from_ofm(dfr, \"relative_time\")\n",
    "                    if odor is not None:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if odor is None:\n",
    "                odor = (PRE_SEC, PRE_SEC + 30.0)\n",
    "\n",
    "            odor_start, odor_end = odor\n",
    "            total_duration = PRE_SEC + (odor_end - odor_start) + POST_SEC\n",
    "            time_axis = np.linspace(0, total_duration, len(pct))\n",
    "\n",
    "            row = build_row(time_axis, pct, PRE_SEC, PRE_SEC + (odor_end - odor_start),\n",
    "                            p.stem, apply_sentinels=False)\n",
    "            if row:\n",
    "                out[tri] = row\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return out\n",
    "\n",
    "def _ead_compute_trim_min_max(csvs: List[Path], dist_col: str) -> Tuple[float,float] | None:\n",
    "    vals = []\n",
    "    for p in csvs:\n",
    "        try:\n",
    "            v = pd.to_numeric(pd.read_csv(p, usecols=[dist_col])[dist_col], errors=\"coerce\").to_numpy()\n",
    "            vals.append(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not vals: return None\n",
    "    allv = np.concatenate(vals); allv = allv[np.isfinite(allv)]\n",
    "    if allv.size == 0: return None\n",
    "    gmax = float(np.max(allv))\n",
    "    p5   = float(np.percentile(allv, 100*TRIM_FRAC, method=\"linear\"))\n",
    "    kept = allv[allv >= p5]\n",
    "    trimmed_min = float(np.min(kept)) if kept.size else float(np.min(allv))\n",
    "    return trimmed_min, gmax\n",
    "\n",
    "def collect_rows_eye_antenna_pct(fly_dir: Path, category: str) -> Dict[int, dict]:\n",
    "    \"\"\"\n",
    "    Eye_Antenna_Dist/*combined.csv (filtered by filename containing 'training'/'testing').\n",
    "    If distance_class1_class2_pct missing, compute in-memory using per-fly trimmed-min & global max.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    base = fly_dir / \"Eye_Antenna_Dist\"\n",
    "    if not base.is_dir(): \n",
    "        return out\n",
    "    csvs = sorted([p for p in base.glob(\"*combined.csv\") if category in p.name.lower()])\n",
    "    if not csvs:\n",
    "        return out\n",
    "\n",
    "    # Column names\n",
    "    PCT_COL  = \"distance_class1_class2_pct\"\n",
    "    DIST_COL = \"distance_class1_class2\"  # source for fallback pct\n",
    "\n",
    "    # Precompute per-fly stats if needed\n",
    "    need_stats = False\n",
    "    for p in csvs:\n",
    "        try:\n",
    "            hdr = pd.read_csv(p, nrows=0)\n",
    "            if PCT_COL not in hdr.columns: \n",
    "                need_stats = True; break\n",
    "        except Exception:\n",
    "            continue\n",
    "    stats = None\n",
    "    if need_stats:\n",
    "        stats = _ead_compute_trim_min_max(sorted(base.glob(\"*combined.csv\")), DIST_COL)\n",
    "\n",
    "    for p in csvs:\n",
    "        try:\n",
    "            df = pd.read_csv(p); df.columns = df.columns.str.strip()\n",
    "            frame_col = find_col(df, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "            ts_col    = find_col(df, [\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\",\"time_s\"])\n",
    "            ts, _     = ensure_time_series(df, frame_col, ts_col)\n",
    "            if ts.notna().sum() < 2: \n",
    "                continue\n",
    "\n",
    "            # ensure pct in-memory\n",
    "            if PCT_COL not in df.columns:\n",
    "                if stats is None:\n",
    "                    continue\n",
    "                fly_min, fly_max = stats\n",
    "                dist = pd.to_numeric(df.get(DIST_COL, np.nan), errors=\"coerce\")\n",
    "                if np.isfinite(fly_min) and np.isfinite(fly_max) and (fly_max > fly_min):\n",
    "                    pct = (dist - fly_min) / (fly_max - fly_min) * 100.0\n",
    "                else:\n",
    "                    pct = pd.Series(np.where(dist.notna(), 0.0, np.nan), index=df.index)\n",
    "                df[PCT_COL] = pct\n",
    "\n",
    "            # time + odor\n",
    "            time_s = pd.to_numeric(ts, errors=\"coerce\"); t0 = float(np.nanmin(time_s))\n",
    "            rel = time_s - t0\n",
    "            df[\"relative_time\"] = rel\n",
    "            odor = odor_window_from_ofm(df, \"relative_time\")\n",
    "            if odor is None:\n",
    "                odor = (PRE_SEC, PRE_SEC + 30.0)\n",
    "            odor_start, odor_end = odor\n",
    "            total_duration = PRE_SEC + (odor_end - odor_start) + POST_SEC\n",
    "\n",
    "            # dense alignment if frames exist\n",
    "            if frame_col is not None and frame_col in df.columns:\n",
    "                frames = pd.to_numeric(df[frame_col], errors=\"coerce\").dropna().astype(int)\n",
    "                total_frames = np.arange(frames.min(), frames.max()+1, dtype=int)\n",
    "                idx_map = {f:i for i,f in enumerate(total_frames)}\n",
    "                present_idx = [idx_map.get(int(f)) for f in frames if int(f) in idx_map]\n",
    "                vals = pd.to_numeric(df.loc[frames.index, PCT_COL], errors=\"coerce\").to_numpy()\n",
    "                full = np.full_like(total_frames, np.nan, dtype=float)\n",
    "                if present_idx: full[np.array(present_idx, dtype=int)] = vals\n",
    "                time_axis = np.linspace(0, total_duration, len(total_frames))\n",
    "            else:\n",
    "                vals = pd.to_numeric(df[PCT_COL], errors=\"coerce\").to_numpy()\n",
    "                time_axis = np.linspace(0, total_duration, len(vals)); full = vals\n",
    "\n",
    "            tri = _extract_trial_fallback(p.stem, category)\n",
    "            if tri is None:\n",
    "                continue\n",
    "            row = build_row(time_axis, full, PRE_SEC, PRE_SEC+(odor_end-odor_start), p.stem, True)\n",
    "            if row: out[tri] = row\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "# ───────── Plotting ─────────\n",
    "def union_bbox(ax_list):\n",
    "    boxes = [ax.get_position() for ax in ax_list]\n",
    "    if not boxes: return None\n",
    "    bb = boxes[0]\n",
    "    for b in boxes[1:]:\n",
    "        bb = Bbox.union([bb, b])\n",
    "    return bb\n",
    "\n",
    "def plot_category_mega(fly_dir: Path, category: str):\n",
    "    fly_name = fly_dir.name\n",
    "    out_dir = fly_dir / \"mega_heatmaps\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Row collectors (new layout)\n",
    "    rows_top   = collect_rows_eye_prob_dist(fly_dir, category)     # Eye_Prob_Dist\n",
    "    rows_angle = collect_rows_angle_centered_pct(fly_dir, category) # Centered angle %\n",
    "    rows_pct   = collect_rows_eye_antenna_pct(fly_dir, category)    # Eye_Antenna_Dist robust %\n",
    "\n",
    "    all_trials = sorted(set(rows_top.keys()) | set(rows_pct.keys()) | set(rows_angle.keys()))\n",
    "    if not all_trials:\n",
    "        print(f\"[WARN] No trials found for {fly_name} / {category}. Skipping.\"); return\n",
    "\n",
    "    n_cols, n_rows = len(all_trials), 3\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(4.6*n_cols, 2.0*n_rows + 1.0), squeeze=False)\n",
    "\n",
    "    cmap_v = mpl.colormaps['viridis'].copy(); cmap_v.set_under('pink'); cmap_v.set_over('pink'); cmap_v.set_bad('dimgray')\n",
    "    cmap_a = mpl.colormaps['coolwarm'].copy(); cmap_a.set_bad('dimgray')\n",
    "\n",
    "    for j, tri in enumerate(all_trials):\n",
    "        axs[0, j].set_title(f\"{category}_{tri}\", fontsize=12, pad=6)\n",
    "\n",
    "    row_labels = [\"Top distance % (Eye_Prob_Dist)\", \"Centered angle %\", \"Class1–Class2 % (Eye_Antenna_Dist)\"]\n",
    "    for i in range(n_rows):\n",
    "        axs[i,0].text(-0.06, 0.5, row_labels[i], transform=axs[i,0].transAxes,\n",
    "                      rotation=90, va='center', ha='right', fontsize=11)\n",
    "\n",
    "    pcm_top = pcm_ang = pcm_pct = None\n",
    "    for j, tri in enumerate(all_trials):\n",
    "        # Row 0: Eye_Prob_Dist\n",
    "        ax = axs[0, j]; r = rows_top.get(tri)\n",
    "        if r:\n",
    "            pcm_top = ax.pcolormesh(r[\"time_edges\"], [0,1], r[\"data_row\"], cmap=cmap_v, norm=VIRIDIS_NORM, shading='auto')\n",
    "            ax.axvline(r[\"odor_start\"], color='red', linewidth=2.0); ax.axvline(r[\"odor_end\"], color='red', linewidth=2.0)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Missing\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_yticks([]); ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "\n",
    "        # Row 1: Centered angle %\n",
    "        ax = axs[1, j]; r = rows_angle.get(tri)\n",
    "        if r:\n",
    "            pcm_ang = ax.pcolormesh(r[\"time_edges\"], [0,1], r[\"data_row\"], cmap=cmap_a, norm=COOLWARM_NORM, shading='auto')\n",
    "            ax.axvline(r[\"odor_start\"], color='red', linewidth=2.0); ax.axvline(r[\"odor_end\"], color='red', linewidth=2.0)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Missing\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_yticks([]); ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "\n",
    "        # Row 2: Eye_Antenna_Dist robust %\n",
    "        ax = axs[2, j]; r = rows_pct.get(tri)\n",
    "        if r:\n",
    "            pcm_pct = ax.pcolormesh(r[\"time_edges\"], [0,1], r[\"data_row\"], cmap=cmap_v, norm=VIRIDIS_NORM, shading='auto')\n",
    "            ax.axvline(r[\"odor_start\"], color='red', linewidth=2.0); ax.axvline(r[\"odor_end\"], color='red', linewidth=2.0)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Missing\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_yticks([]); ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "\n",
    "    for j in range(n_cols):\n",
    "        axs[2, j].set_xlabel(\"Time (s)\")\n",
    "\n",
    "    # Make room on the right for two colorbars\n",
    "    fig.tight_layout(rect=[0.08, 0.02, 0.86, 0.94])\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "    grid_bb = union_bbox([ax for row in axs for ax in row])\n",
    "    y0, h = (grid_bb.y0, grid_bb.height) if grid_bb is not None else (0.12, 0.76)\n",
    "\n",
    "    cbar_w = 0.015\n",
    "    gap    = 0.02\n",
    "    x_vir  = 0.89\n",
    "    x_ang  = x_vir + cbar_w + gap\n",
    "\n",
    "    if (pcm_top or pcm_pct):\n",
    "        cax_vir = fig.add_axes([x_vir, y0, cbar_w, h])\n",
    "        fig.colorbar(pcm_pct if pcm_pct else pcm_top, cax=cax_vir, orientation='vertical', extend='both')\n",
    "        cax_vir.set_ylabel(\"Distance Percentage (%)\")\n",
    "\n",
    "    if pcm_ang:\n",
    "        cax_ang = fig.add_axes([x_ang, y0, cbar_w, h])\n",
    "        fig.colorbar(pcm_ang, cax=cax_ang, orientation='vertical')\n",
    "        cax_ang.set_ylabel(\"Centered Angle (%)\")\n",
    "\n",
    "    odor_handle = [Line2D([0],[0], color='red', linewidth=2.0, label='Odor Period')]\n",
    "    fig.legend(handles=odor_handle, loc='upper left', frameon=True)\n",
    "\n",
    "    fig.suptitle(f\"{fly_name} — {category.capitalize()} (All trials grouped by index)\", fontsize=14)\n",
    "\n",
    "    out_path = (fly_dir / \"mega_heatmaps\") / f\"{fly_name}_{category}_MEGA_heatmaps.png\"\n",
    "    fig.savefig(out_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"[SAVED] {out_path}\")\n",
    "\n",
    "def plot_all_flies_mega(main_directory: Path | str):\n",
    "    md = Path(main_directory)\n",
    "    assert md.is_dir(), f\"Not a directory: {md}\"\n",
    "\n",
    "    # Only month-prefixed fly folders (full or 3-letter), case-insensitive\n",
    "    fly_dirs = [p for p in md.iterdir() if p.is_dir() and any(p.name.lower().startswith(m) for m in MONTHS)]\n",
    "    if not fly_dirs:\n",
    "        print(f\"[WARN] No month-prefixed fly folders found in {md}\"); return\n",
    "\n",
    "    for fly_dir in sorted(fly_dirs):\n",
    "        for category in (\"training\",\"testing\"):\n",
    "            plot_category_mega(fly_dir, category)\n",
    "\n",
    "# Run (expects `main_directory` defined)\n",
    "plot_all_flies_mega(main_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf6fb6-4ed8-47d6-969c-dafb4a6f6f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Collect all MEGA heatmaps into main_directory/MEGA_Heatmap\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# --- Config ---\n",
    "assert 'main_directory' in globals(), \"Define main_directory = '/path/to/root' first.\"\n",
    "MAIN_DIR   = Path(main_directory).resolve()\n",
    "DEST_DIR   = MAIN_DIR / \"MEGA_Heatmap\"\n",
    "MOVE_FILES = False  # set True to move instead of copy\n",
    "\n",
    "assert MAIN_DIR.is_dir(), f\"Not a directory: {MAIN_DIR}\"\n",
    "DEST_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Find all mega heatmaps created by the previous code\n",
    "mega_pngs = sorted(p for p in MAIN_DIR.rglob(\"*_MEGA_heatmaps.png\") if p.is_file())\n",
    "\n",
    "if not mega_pngs:\n",
    "    print(f\"[INFO] No MEGA heatmaps found under {MAIN_DIR}\")\n",
    "else:\n",
    "    copied, skipped = 0, 0\n",
    "    for src in mega_pngs:\n",
    "        dst = DEST_DIR / src.name\n",
    "        if dst.exists():\n",
    "            # Avoid collision by appending a numeric suffix\n",
    "            stem, suf = dst.stem, dst.suffix\n",
    "            k = 1\n",
    "            while True:\n",
    "                alt = DEST_DIR / f\"{stem} ({k}){suf}\"\n",
    "                if not alt.exists():\n",
    "                    dst = alt\n",
    "                    break\n",
    "                k += 1\n",
    "        try:\n",
    "            if MOVE_FILES:\n",
    "                shutil.move(str(src), str(dst))\n",
    "            else:\n",
    "                shutil.copy2(str(src), str(dst))\n",
    "            copied += 1\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            print(f\"[WARN] Could not {'move' if MOVE_FILES else 'copy'} {src} → {dst}: {e}\")\n",
    "\n",
    "    action = \"moved\" if MOVE_FILES else \"copied\"\n",
    "    print(f\"[DONE] {copied} file(s) {action} to {DEST_DIR} | {skipped} skipped.\")\n",
    "    print(f\"[DEST] {DEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d7e00-7da0-45c5-9a5c-9c3b04cc63ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Videos With 3 Line Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee09b6-f194-4965-be9d-c557430b2a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Move trial videos into per-fly three_line_videos/{testing,training}\n",
    "# Assumes trial folders are named like: /.../august_06_fly_3/august_06_fly_3_testing_1/\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# --- Configure this to your root that contains the fly folders ---\n",
    "ROOT = Path(main_directory).expanduser().resolve()\n",
    "\n",
    "# Destination folder name under each fly\n",
    "DEST_FOLDER = \"videos_with_rms\"\n",
    "\n",
    "# Video extensions to collect (lowercase, with dot)\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".mpg\", \".mpeg\", \".m4v\"}\n",
    "\n",
    "# Set to True for a dry run (no files moved)\n",
    "DRY_RUN = False\n",
    "\n",
    "TRIAL_DIR_RE = re.compile(r\"^(?P<fly>.+)_(?P<phase>testing|training)_(?P<idx>\\d+)$\", re.IGNORECASE)\n",
    "\n",
    "def is_video(p: Path) -> bool:\n",
    "    return p.is_file() and p.suffix.lower() in VIDEO_EXTS\n",
    "\n",
    "def ensure_unique_path(dst: Path) -> Path:\n",
    "    \"\"\"If dst exists, append _dupN before the extension.\"\"\"\n",
    "    if not dst.exists():\n",
    "        return dst\n",
    "    stem, suffix = dst.stem, dst.suffix\n",
    "    n = 1\n",
    "    while True:\n",
    "        candidate = dst.with_name(f\"{stem}_dup{n}{suffix}\")\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        n += 1\n",
    "\n",
    "def move_videos_from_trial(trial_dir: Path, fly_dir: Path, phase: str):\n",
    "    # destination under each fly\n",
    "    dest_dir = fly_dir / DEST_FOLDER / phase.lower()\n",
    "    if not DRY_RUN:\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    moved = 0\n",
    "    for vid in sorted(trial_dir.iterdir()):\n",
    "        if not is_video(vid):\n",
    "            continue\n",
    "        dst = ensure_unique_path(dest_dir / vid.name)\n",
    "        print(f\"{'DRY-RUN: ' if DRY_RUN else ''}Moving {vid} -> {dst}\")\n",
    "        if not DRY_RUN:\n",
    "            shutil.move(str(vid), str(dst))\n",
    "        moved += 1\n",
    "    return moved\n",
    "\n",
    "def main():\n",
    "    if not ROOT.exists():\n",
    "        raise SystemExit(f\"ROOT does not exist: {ROOT}\")\n",
    "\n",
    "    # Iterate fly folders directly under ROOT\n",
    "    for fly_dir in sorted([p for p in ROOT.iterdir() if p.is_dir()]):\n",
    "        fly_name = fly_dir.name\n",
    "\n",
    "        # Skip non-fly folders (heuristic: must contain '_fly_')\n",
    "        if \"_fly_\" not in fly_name:\n",
    "            continue\n",
    "\n",
    "        # Skip destination-looking roots (defensive)\n",
    "        if fly_name.lower().endswith((\"testing\", \"training\")):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Fly: {fly_name} ===\")\n",
    "\n",
    "        # Iterate immediate subfolders of the fly directory\n",
    "        for trial_dir in sorted([p for p in fly_dir.iterdir() if p.is_dir()]):\n",
    "            trial_name = trial_dir.name\n",
    "\n",
    "            # Skip the destination folder itself and other generated outputs\n",
    "            if trial_name.lower().startswith(DEST_FOLDER.lower()):\n",
    "                continue\n",
    "\n",
    "            m = TRIAL_DIR_RE.match(trial_name)\n",
    "            if not m:\n",
    "                # Not a trial folder in the expected pattern\n",
    "                continue\n",
    "\n",
    "            phase = m.group(\"phase\").lower()\n",
    "\n",
    "            # Extra guard: ensure the prefix (fly name) matches this fly\n",
    "            if m.group(\"fly\") != fly_name:\n",
    "                continue\n",
    "\n",
    "            moved = move_videos_from_trial(trial_dir, fly_dir, phase)\n",
    "            if moved == 0:\n",
    "                print(f\"  (No videos found in {trial_dir})\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5eca25-cdb7-4142-8c34-f7d9a379bb3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Line-plot panel under the video (Top %, Centered angle %, Class1–Class2 %)\n",
    "# Sources & normalization mirror the FIXED MEGA heatmap code.\n",
    "# Videos are read from: {fly}/three_line_videos/{training,testing}/\n",
    "# Outputs go to:        {fly}/three_line_videos/with_line_plots/{fly}_{category}_{trial}_LINES_three_series.mp4\n",
    "\n",
    "import os, re, glob, io\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip, VideoClip, CompositeVideoClip\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# REQUIRED: set main_directory (Path or str)\n",
    "assert 'main_directory' in globals(), \"Define main_directory = '/path/to/root' before running.\"\n",
    "ROOT = Path(main_directory).expanduser().resolve()\n",
    "assert ROOT.is_dir(), f\"Not a directory: {ROOT}\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Visual defaults\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 13,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'lines.linewidth': 2,\n",
    "    'axes.linewidth': 1.25,\n",
    "    'figure.dpi': 160,\n",
    "    'savefig.dpi': 300,\n",
    "    'legend.fontsize': 11\n",
    "})\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Constants (aligned with MEGA)\n",
    "FPS_DEFAULT   = 40.0\n",
    "ANCHOR_X, ANCHOR_Y = 1080.0, 540.0\n",
    "PCT_COL_ROBUST = \"distance_class1_class2_pct\"\n",
    "DIST_COL_ROBUST = \"distance_class1_class2\"\n",
    "PRE_SEC, POST_SEC = 30.0, 90.0\n",
    "TRIM_FRAC = 0.05  # bottom 5% trimmed-min for Eye_Antenna_Dist fallback\n",
    "MONTHS = (\n",
    "    \"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "    \"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\n",
    "    \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"\n",
    ")\n",
    "\n",
    "# ── Panel rendering options ─────────────────────────────────────\n",
    "PANEL_HEIGHT_FRACTION = 0.24\n",
    "YLIM = (-100, 100)  # angle is [-100,100], distance lines are [0,100] → keep unified\n",
    "VIDEO_INPUT_DIR = \"three_line_videos\"              # where copied trial videos live\n",
    "VIDEO_OUTPUT_SUBDIR = \"with_line_plots\"            # under three_line_videos/\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Helpers (same behavior as MEGA)\n",
    "# Delete the source videos after rendering new ones\n",
    "DELETE_SOURCE_AFTER_RENDER = True\n",
    "# Optionally remove the input category folder if it becomes empty\n",
    "DELETE_EMPTY_INPUT_DIRS = True\n",
    "\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".mpg\", \".mpeg\", \".m4v\"}  # keep as in your script\n",
    "\n",
    "def _safe_unlink(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(f\"    Deleted source video: {p.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [warn] Could not delete {p}: {e}\")\n",
    "\n",
    "def _maybe_rmdir_empty(dir_path: Path):\n",
    "    try:\n",
    "        # remove folder if it contains no files/dirs\n",
    "        if dir_path.exists() and not any(dir_path.iterdir()):\n",
    "            dir_path.rmdir()\n",
    "            print(f\"    Removed empty folder: {dir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [warn] Could not remove folder {dir_path}: {e}\")\n",
    "\n",
    "def timestamp_to_seconds(ts) -> float:\n",
    "    if pd.isna(ts): return np.nan\n",
    "    try: return float(ts)\n",
    "    except Exception:\n",
    "        s = str(ts).strip(); parts = s.split(\":\")\n",
    "        try:\n",
    "            if len(parts)==4: hh,mm,ss,ms=parts; return int(hh)*3600+int(mm)*60+int(ss)+int(ms)/1000.0\n",
    "            if len(parts)==3: hh,mm,ss=parts;   return int(hh)*3600+int(mm)*60+float(ss)\n",
    "            if len(parts)==2: mm,ss=parts;      return int(mm)*60+float(ss)\n",
    "            if len(parts)==1: return float(parts[0])\n",
    "        except Exception: return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def find_col(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    cols = set(df.columns)\n",
    "    for c in cands:\n",
    "        if c in cols: return c\n",
    "    return None\n",
    "\n",
    "def derive_fps(df: pd.DataFrame) -> float:\n",
    "    fps_col = find_col(df, [\"fps\",\"FPS\",\"frame_rate\",\"frameRate\"])\n",
    "    if fps_col is not None:\n",
    "        fps_val = pd.to_numeric(df[fps_col], errors=\"coerce\").median()\n",
    "        if np.isfinite(fps_val) and fps_val > 0: return float(fps_val)\n",
    "    return FPS_DEFAULT\n",
    "\n",
    "def ensure_time_series(df: pd.DataFrame, frame_col: Optional[str], ts_col: Optional[str]):\n",
    "    if ts_col is not None:\n",
    "        ts = pd.to_numeric(df[ts_col], errors=\"coerce\") if ts_col in [\"time_seconds\",\"relative_time\",\"time_s\"] \\\n",
    "             else df[ts_col].apply(timestamp_to_seconds)\n",
    "        if ts.notna().sum() >= 2 and np.nanmax(np.diff(ts.dropna().values)) > 0:\n",
    "            return ts, {'used':'timestamp'}\n",
    "    if frame_col is not None:\n",
    "        frames = pd.to_numeric(df[frame_col], errors=\"coerce\")\n",
    "        if frames.notna().sum() >= 2:\n",
    "            fps = derive_fps(df); f0 = int(np.nanmin(frames.values))\n",
    "            return (frames - f0)/fps, {'used':'frame_fallback','fps':fps}\n",
    "    fps = derive_fps(df)\n",
    "    return pd.Series(np.arange(len(df), dtype=float)/fps, index=df.index), {'used':'index_fallback','fps':fps}\n",
    "\n",
    "def extract_trial_index(name: str, category: str) -> Optional[int]:\n",
    "    m = re.search(rf\"{category}_(\\d+)\", name.lower())\n",
    "    if m:\n",
    "        try: return int(m.group(1))\n",
    "        except: return None\n",
    "    m2 = re.search(r\"(training|testing)_(\\d+)\", name.lower())\n",
    "    if m2:\n",
    "        try: return int(m2.group(2))\n",
    "        except: return None\n",
    "    # fallback: last number in stem\n",
    "    nums = re.findall(r\"\\d+\", name)\n",
    "    if nums:\n",
    "        try: return int(nums[-1])\n",
    "        except: return None\n",
    "    return None\n",
    "\n",
    "def odor_window_from_ofm(df: pd.DataFrame, time_col: str = \"relative_time\"):\n",
    "    ofm_col = find_col(df, [\"OFM State\",\"OFM_State\",\"ofm_state\",\"ofm\"])\n",
    "    if ofm_col is None or time_col not in df.columns: return None\n",
    "    try:\n",
    "        mask = df[ofm_col].astype(str).str.lower().eq(\"during\")\n",
    "        if mask.any():\n",
    "            t = df[time_col]; idx = np.flatnonzero(mask.to_numpy())\n",
    "            if idx.size >= 2: return float(t.iloc[idx[0]]), float(t.iloc[idx[-1]])\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def compute_angle_deg_at_point2(df: pd.DataFrame) -> pd.Series:\n",
    "    req = [\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"]\n",
    "    if any(c not in df.columns for c in req): raise ValueError(\"Missing angle columns\")\n",
    "    p2x = df[\"x_class2\"].astype(float).to_numpy(); p2y = df[\"y_class2\"].astype(float).to_numpy()\n",
    "    p3x = df[\"x_class6\"].astype(float).to_numpy(); p3y = df[\"y_class6\"].astype(float).to_numpy()\n",
    "    ux, uy = (ANCHOR_X - p2x), (ANCHOR_Y - p2y)\n",
    "    vx, vy = (p3x - p2x), (p3y - p2y)\n",
    "    dot = ux*vx + uy*vy; cross = ux*vy - uy*vx\n",
    "    n1 = np.hypot(ux, uy); n2 = np.hypot(vx, vy)\n",
    "    valid = (n1 > 0) & (n2 > 0) & np.isfinite(dot) & np.isfinite(cross)\n",
    "    ang = np.full(len(p2x), np.nan); ang[valid] = np.degrees(np.arctan2(np.abs(cross[valid]), dot[valid]))\n",
    "    return pd.Series(ang, index=df.index, name=\"angle_ARB_deg\")\n",
    "\n",
    "def find_fly_reference_angle(csvs_raw: List[Path]) -> float:\n",
    "    best = None\n",
    "    for p in csvs_raw:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if not {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df.columns): continue\n",
    "            ang = compute_angle_deg_at_point2(df)\n",
    "            dist_col = find_col(df, [\"distance_percentage\",\"distance_percent\",\"distance_pct\",\"distance_class1_class2_pct\"])\n",
    "            if dist_col is None: continue\n",
    "            dist = pd.to_numeric(df[dist_col], errors=\"coerce\").to_numpy()\n",
    "            exact = np.flatnonzero(dist == 0)\n",
    "            if exact.size > 0:\n",
    "                idx = int(exact[0]); angle_here = float(ang.iloc[idx]) if np.isfinite(ang.iloc[idx]) else np.nan\n",
    "                cand = (0, 0.0, angle_here)\n",
    "            else:\n",
    "                with np.errstate(invalid=\"ignore\"): absd = np.abs(dist)\n",
    "                if not np.isfinite(absd).any(): continue\n",
    "                idx = int(np.nanargmin(absd)); angle_here = float(ang.iloc[idx]) if np.isfinite(ang.iloc[idx]) else np.nan\n",
    "                cand = (1, float(absd[idx]), angle_here)\n",
    "            if best is None or cand < best: best = cand\n",
    "        except Exception:\n",
    "            pass\n",
    "    return best[2] if best is not None else np.nan\n",
    "\n",
    "def compute_fly_max_abs_centered(csvs_raw: List[Path], ref_angle: float) -> float:\n",
    "    fly_max = 0.0\n",
    "    for p in csvs_raw:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if not {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df.columns): continue\n",
    "            ang = compute_angle_deg_at_point2(df)\n",
    "            centered = ang - ref_angle if np.isfinite(ref_angle) else ang*0.0\n",
    "            local = np.nanmax(np.abs(centered.to_numpy(dtype=float)))\n",
    "            if np.isfinite(local): fly_max = max(fly_max, float(local))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fly_max if np.isfinite(fly_max) and fly_max > 0 else np.nan\n",
    "\n",
    "def _infer_category_from_path(p: Path) -> Optional[str]:\n",
    "    tokens = \" \".join([*p.parts, p.stem]).lower()\n",
    "    if any(t in tokens for t in (\"training\",\"train\",\"trn\")): return \"training\"\n",
    "    if any(t in tokens for t in (\"testing\",\"test\",\"tst\")):  return \"testing\"\n",
    "    return None\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Series collectors (mirror MEGA). Return (t, y, odor_on, odor_off).\n",
    "\n",
    "def _series_top_distance(fly_dir: Path, category: str, trial_index: int):\n",
    "    base = fly_dir / \"Eye_Prob_Dist\" / category\n",
    "    candidates = []\n",
    "    if base.is_dir():\n",
    "        candidates = sorted(base.glob(\"*.csv\"))\n",
    "    else:\n",
    "        root = fly_dir / \"Eye_Prob_Dist\"\n",
    "        if root.is_dir():\n",
    "            candidates = sorted([p for p in root.glob(\"*.csv\") if category in p.name.lower()])\n",
    "    chosen = None\n",
    "    for p in candidates:\n",
    "        if extract_trial_index(p.stem, category) == trial_index:\n",
    "            chosen = p; break\n",
    "    if chosen is None: return None\n",
    "\n",
    "    df = pd.read_csv(chosen); df.columns = df.columns.str.strip()\n",
    "    frame_col = find_col(df, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "    ts_col    = find_col(df, [\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\",\"time_s\"])\n",
    "    pct_col   = find_col(df, [\n",
    "        \"distance_percentage\",\"distance_percent\",\"distance_pct\",\n",
    "        \"distance_proboscis_eye_pct\",\"proboscis_eye_distance_pct\",\n",
    "        \"eye_prob_distance_pct\",\"distance_class1_class2_pct\"\n",
    "    ])\n",
    "    if pct_col is None: return None\n",
    "\n",
    "    ts, _ = ensure_time_series(df, frame_col, ts_col)\n",
    "    if ts.notna().sum() < 2: return None\n",
    "    time_s = pd.to_numeric(ts, errors=\"coerce\"); t0 = float(np.nanmin(time_s))\n",
    "    df[\"relative_time\"] = time_s - t0\n",
    "    odor = odor_window_from_ofm(df, \"relative_time\") or (PRE_SEC, PRE_SEC + 30.0)\n",
    "    odor_start, odor_end = odor\n",
    "    total_duration = PRE_SEC + (odor_end - odor_start) + POST_SEC\n",
    "\n",
    "    if frame_col is not None and frame_col in df.columns:\n",
    "        frames = pd.to_numeric(df[frame_col], errors=\"coerce\").dropna().astype(int)\n",
    "        total_frames = np.arange(frames.min(), frames.max()+1, dtype=int)\n",
    "        idx_map = {f:i for i,f in enumerate(total_frames)}\n",
    "        present_idx = [idx_map.get(int(f)) for f in frames if int(f) in idx_map]\n",
    "        vals = pd.to_numeric(df.loc[frames.index, pct_col], errors=\"coerce\").to_numpy()\n",
    "        full = np.full_like(total_frames, np.nan, dtype=float)\n",
    "        if present_idx: full[np.array(present_idx, dtype=int)] = vals\n",
    "        t = np.linspace(0, total_duration, len(total_frames)); y = full\n",
    "    else:\n",
    "        vals = pd.to_numeric(df[pct_col], errors=\"coerce\").to_numpy()\n",
    "        t = np.linspace(0, total_duration, len(vals)); y = vals\n",
    "    return t, y.astype(float), float(PRE_SEC), float(PRE_SEC + (odor_end - odor_start))\n",
    "\n",
    "# Cache per-fly trimmed-min/global-max for Eye_Antenna_Dist\n",
    "_ead_stats_cache: Dict[Path, Tuple[float,float]] = {}\n",
    "\n",
    "def _ead_compute_trim_min_max(fly_dir: Path) -> Optional[Tuple[float,float]]:\n",
    "    if fly_dir in _ead_stats_cache:\n",
    "        return _ead_stats_cache[fly_dir]\n",
    "    base = fly_dir / \"Eye_Antenna_Dist\"\n",
    "    if not base.is_dir(): return None\n",
    "    vals = []\n",
    "    for p in sorted(base.glob(\"*combined.csv\")):\n",
    "        try:\n",
    "            v = pd.to_numeric(pd.read_csv(p, usecols=[DIST_COL_ROBUST])[DIST_COL_ROBUST], errors=\"coerce\").to_numpy()\n",
    "            vals.append(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not vals: return None\n",
    "    allv = np.concatenate(vals); allv = allv[np.isfinite(allv)]\n",
    "    if allv.size == 0: return None\n",
    "    gmax = float(np.max(allv))\n",
    "    p5   = float(np.percentile(allv, 100*TRIM_FRAC, method=\"linear\"))\n",
    "    kept = allv[allv >= p5]\n",
    "    trimmed_min = float(np.min(kept)) if kept.size else float(np.min(allv))\n",
    "    _ead_stats_cache[fly_dir] = (trimmed_min, gmax)\n",
    "    return _ead_stats_cache[fly_dir]\n",
    "\n",
    "def _series_robust_pct(fly_dir: Path, category: str, trial_index: int):\n",
    "    base = fly_dir / \"Eye_Antenna_Dist\"\n",
    "    if not base.is_dir(): return None\n",
    "    csvs = sorted([p for p in base.glob(\"*combined.csv\") if category in p.name.lower()])\n",
    "    chosen = None\n",
    "    for p in csvs:\n",
    "        if extract_trial_index(p.stem, category) == trial_index:\n",
    "            chosen = p; break\n",
    "    if chosen is None: return None\n",
    "\n",
    "    df = pd.read_csv(chosen); df.columns = df.columns.str.strip()\n",
    "    frame_col = find_col(df, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "    ts_col    = find_col(df, [\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\",\"time_s\"])\n",
    "    ts, _     = ensure_time_series(df, frame_col, ts_col)\n",
    "    if ts.notna().sum() < 2: return None\n",
    "\n",
    "    # Ensure percentage in-memory if missing\n",
    "    if PCT_COL_ROBUST not in df.columns:\n",
    "        stats = _ead_compute_trim_min_max(fly_dir)\n",
    "        if stats is None: return None\n",
    "        fly_min, fly_max = stats\n",
    "        dist = pd.to_numeric(df.get(DIST_COL_ROBUST, np.nan), errors=\"coerce\")\n",
    "        if np.isfinite(fly_min) and np.isfinite(fly_max) and (fly_max > fly_min):\n",
    "            df[PCT_COL_ROBUST] = (dist - fly_min) / (fly_max - fly_min) * 100.0\n",
    "        else:\n",
    "            df[PCT_COL_ROBUST] = np.where(dist.notna(), 0.0, np.nan)\n",
    "\n",
    "    df[\"time_seconds\"] = pd.to_numeric(ts, errors=\"coerce\")\n",
    "    df[\"relative_time\"] = df[\"time_seconds\"] - df[\"time_seconds\"].min()\n",
    "    odor = odor_window_from_ofm(df, \"relative_time\") or (PRE_SEC, PRE_SEC + 30.0)\n",
    "    odor_start, odor_end = odor\n",
    "    total_duration = PRE_SEC + (odor_end - odor_start) + POST_SEC\n",
    "\n",
    "    if frame_col is not None and frame_col in df.columns:\n",
    "        frames = pd.to_numeric(df[frame_col], errors=\"coerce\").dropna().astype(int)\n",
    "        total_frames = np.arange(frames.min(), frames.max()+1, dtype=int)\n",
    "        idx_map = {f:i for i,f in enumerate(total_frames)}\n",
    "        present_idx = [idx_map.get(int(f)) for f in frames if int(f) in idx_map]\n",
    "        vals = pd.to_numeric(df.loc[frames.index, PCT_COL_ROBUST], errors=\"coerce\").to_numpy()\n",
    "        full = np.full_like(total_frames, np.nan, dtype=float)\n",
    "        if present_idx: full[np.array(present_idx, dtype=int)] = vals\n",
    "        t = np.linspace(0, total_duration, len(total_frames)); y = full\n",
    "    else:\n",
    "        vals = pd.to_numeric(df[PCT_COL_ROBUST], errors=\"coerce\").to_numpy()\n",
    "        t = np.linspace(0, total_duration, len(vals)); y = vals\n",
    "    return t, y.astype(float), float(PRE_SEC), float(PRE_SEC + (odor_end - odor_start))\n",
    "\n",
    "def _series_angle_centered_pct(fly_dir: Path, category: str, trial_index: int):\n",
    "    # Discover angle/raw files across the entire fly folder (mirrors robust MEGA logic)\n",
    "    angle_csvs = list(fly_dir.rglob(\"*_distance_class_2_angle_ARB.csv\"))\n",
    "    raw_csvs   = list(fly_dir.rglob(\"*distance_class_2_angle_ARB.csv\"))\n",
    "\n",
    "    def _cat_match(p: Path) -> bool:\n",
    "        inf = _infer_category_from_path(p)\n",
    "        return (inf == category) or (inf is None)\n",
    "\n",
    "    angle_cat = [p for p in angle_csvs if _cat_match(p)]\n",
    "    raw_cat   = [p for p in raw_csvs   if _cat_match(p)]\n",
    "\n",
    "    # Choose the file that corresponds to this trial index (prefer angle over raw)\n",
    "    chosen = None\n",
    "    for p in sorted(angle_cat):\n",
    "        if extract_trial_index(p.stem, category) == trial_index:\n",
    "            chosen = p; break\n",
    "    if chosen is None:\n",
    "        for p in sorted(raw_cat):\n",
    "            if extract_trial_index(p.stem, category) == trial_index:\n",
    "                chosen = p; break\n",
    "    if chosen is None:\n",
    "        return None\n",
    "\n",
    "    # Build per-fly reference/max from ALL raw CSVs for stable centering\n",
    "    raw_for_ref = raw_csvs if raw_csvs else raw_cat\n",
    "    ref_angle   = find_fly_reference_angle(raw_for_ref) if raw_for_ref else np.nan\n",
    "    fly_max_abs = compute_fly_max_abs_centered(raw_for_ref, ref_angle) if np.isfinite(ref_angle) else np.nan\n",
    "\n",
    "    df = pd.read_csv(chosen); df.columns = df.columns.str.strip()\n",
    "    frame_col = find_col(df, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "    ts_col    = find_col(df, [\"time_s\",\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\"])\n",
    "    if ts_col == \"time_s\":\n",
    "        ts = pd.to_numeric(df[\"time_s\"], errors=\"coerce\")\n",
    "    else:\n",
    "        ts, _ = ensure_time_series(df, frame_col, ts_col)\n",
    "    if ts.notna().sum() < 2: return None\n",
    "\n",
    "    # Odor from sibling/raw if possible\n",
    "    odor = None\n",
    "    raw_guess = None\n",
    "    if \"_distance_class_2_angle_ARB.csv\" in chosen.name:\n",
    "        raw_guess = chosen.with_name(chosen.name.replace(\"_distance_class_2_angle_ARB.csv\",\"_distance_class_2.csv\"))\n",
    "    if raw_guess and raw_guess.exists():\n",
    "        dfr = pd.read_csv(raw_guess)\n",
    "        fr_col = find_col(dfr, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "        ts_r, _ = ensure_time_series(dfr, fr_col, find_col(dfr, [\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\",\"time_s\"]))\n",
    "        dfr[\"time_seconds\"] = pd.to_numeric(ts_r, errors=\"coerce\")\n",
    "        dfr[\"relative_time\"] = dfr[\"time_seconds\"] - dfr[\"time_seconds\"].min()\n",
    "        odor = odor_window_from_ofm(dfr, \"relative_time\")\n",
    "    if odor is None:\n",
    "        tmp = df.copy()\n",
    "        ts_series = pd.to_numeric(ts, errors=\"coerce\")\n",
    "        tmp[\"relative_time\"] = ts_series - ts_series.min()\n",
    "        odor = odor_window_from_ofm(tmp, \"relative_time\")\n",
    "    if odor is None: odor = (PRE_SEC, PRE_SEC + 30.0)\n",
    "    odor_start, odor_end = odor\n",
    "    total_duration = PRE_SEC + (odor_end - odor_start) + POST_SEC\n",
    "\n",
    "    # Percent line\n",
    "    if \"angle_centered_pct\" in df.columns:\n",
    "        vals = pd.to_numeric(df[\"angle_centered_pct\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    else:\n",
    "        df_pos = df\n",
    "        if not {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df_pos.columns):\n",
    "            if raw_guess and raw_guess.exists(): df_pos = pd.read_csv(raw_guess)\n",
    "        if not {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df_pos.columns):\n",
    "            return None\n",
    "        ang = compute_angle_deg_at_point2(df_pos).to_numpy(dtype=float)\n",
    "        if np.isfinite(fly_max_abs) and fly_max_abs > 0 and np.isfinite(ref_angle):\n",
    "            centered = ang - ref_angle; vals = (centered / fly_max_abs) * 100.0\n",
    "        else:\n",
    "            vals = np.zeros_like(ang, dtype=float)\n",
    "\n",
    "    t = np.linspace(0, total_duration, len(vals))\n",
    "    return t, vals.astype(float), float(PRE_SEC), float(PRE_SEC + (odor_end - odor_start))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Video discovery & matching in three_line_videos/<category>\n",
    "\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".mpg\", \".mpeg\", \".m4v\"}\n",
    "\n",
    "def _find_video_for_trial(fly_dir: Path, category: str, tri: int) -> Optional[Path]:\n",
    "    vid_dir = fly_dir / VIDEO_INPUT_DIR / category\n",
    "    if not vid_dir.is_dir(): return None\n",
    "    vids = [p for p in vid_dir.iterdir() if p.is_file() and p.suffix.lower() in VIDEO_EXTS]\n",
    "    # Heuristics: exact token \"<category>_<tri>\", then \"_<tri>.\", then single-file fallback\n",
    "    token = f\"{category}_{tri}\"\n",
    "    cand = [v for v in vids if token in v.stem.lower()]\n",
    "    if cand: return cand[0]\n",
    "    cand = [v for v in vids if re.search(rf\"[_\\-]{tri}(\\D|$)\", v.stem)]\n",
    "    if cand: return cand[0]\n",
    "    if len(vids) == 1: return vids[0]\n",
    "    return None\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Line-panel rendering & composition\n",
    "\n",
    "def _render_line_panel_png(series_list: List[dict],\n",
    "                           width_px: int,\n",
    "                           height_px: int,\n",
    "                           xlim: Tuple[float,float],\n",
    "                           ylim: Tuple[float,float],\n",
    "                           odor_on: float | None,\n",
    "                           odor_off: float | None) -> np.ndarray:\n",
    "    \"\"\"Render a single axes with up to 3 lines, legend, and odor window; axes hidden.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(width_px/100, height_px/100), dpi=100)\n",
    "\n",
    "    for s in series_list:\n",
    "        ax.plot(s[\"t\"], s[\"y\"], label=s[\"label\"], linewidth=1)\n",
    "\n",
    "    ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "\n",
    "    if odor_on is not None and odor_off is not None and np.isfinite(odor_on) and np.isfinite(odor_off):\n",
    "        ax.axvline(odor_on, color='red', linewidth=1.0)\n",
    "        ax.axvline(odor_off, color='red', linewidth=1.0)\n",
    "\n",
    "    if series_list:\n",
    "        leg = ax.legend(loc='upper right', frameon=True, framealpha=0.9)\n",
    "        leg.get_frame().set_facecolor('white')\n",
    "        leg.get_frame().set_edgecolor('black')\n",
    "        leg.get_frame().set_linewidth(0.8)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    buf = io.BytesIO(); plt.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0); plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    arr = np.array(Image.open(buf).convert(\"RGB\"))\n",
    "    return np.array(Image.fromarray(arr).resize((width_px, height_px), resample=Image.BILINEAR))\n",
    "\n",
    "def _compose_lineplot_video(video_path: Path,\n",
    "                            series_list: List[dict],\n",
    "                            xlim: Tuple[float,float],\n",
    "                            odor_on: float | None,\n",
    "                            odor_off: float | None,\n",
    "                            out_mp4: Path,\n",
    "                            panel_height_fraction: float = PANEL_HEIGHT_FRACTION,\n",
    "                            ylim: Tuple[float,float] = YLIM) -> bool:\n",
    "    \"\"\"Compose a single panel with up to 3 lines under the video; add a moving cursor.\n",
    "       Returns True on successful write.\"\"\"\n",
    "    clip = VideoFileClip(str(video_path))\n",
    "    vw, vh = clip.size\n",
    "    ph = max(1, int(vh * panel_height_fraction))\n",
    "\n",
    "    bg = _render_line_panel_png(series_list, vw, ph, xlim, ylim, odor_on, odor_off)\n",
    "\n",
    "    def _add_cursor(img: np.ndarray, t_cur: float, xlim: Tuple[float,float]) -> np.ndarray:\n",
    "        img = img.copy()\n",
    "        t0, t1 = float(xlim[0]), float(xlim[1])\n",
    "        if not (np.isfinite(t0) and np.isfinite(t1)) or t1 <= t0: return img\n",
    "        frac = float(np.clip((t_cur - t0) / (t1 - t0), 0.0, 0.9999))\n",
    "        x = int(frac * (img.shape[1] - 1))\n",
    "        img[:, x:x+2, 0] = 255; img[:, x:x+2, 1:] = 0\n",
    "        return img\n",
    "\n",
    "    def panel_frame(t_cur: float) -> np.ndarray:\n",
    "        return _add_cursor(bg, t_cur, xlim)\n",
    "\n",
    "    panel_clip = VideoClip(panel_frame, duration=clip.duration)\n",
    "    comp = CompositeVideoClip([clip.set_position((\"center\", 0)),\n",
    "                               panel_clip.set_position((\"center\", vh))],\n",
    "                              size=(vw, vh + ph))\n",
    "\n",
    "    out_mp4.parent.mkdir(parents=True, exist_ok=True)\n",
    "    comp.write_videofile(str(out_mp4), fps=clip.fps, codec=\"libx264\", audio=False, preset=\"ultrafast\")\n",
    "\n",
    "    # cleanup\n",
    "    for c in (clip, panel_clip, comp):\n",
    "        try: c.close()\n",
    "        except: pass\n",
    "\n",
    "    try:\n",
    "        return out_mp4.exists() and out_mp4.stat().st_size > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Trial discovery (union across sources, per category), then compose videos\n",
    "\n",
    "def _discover_trials(fly_dir: Path, category: str) -> List[int]:\n",
    "    trials = set()\n",
    "\n",
    "    # Eye_Prob_Dist\n",
    "    base = fly_dir / \"Eye_Prob_Dist\" / category\n",
    "    if base.is_dir():\n",
    "        for p in base.glob(\"*.csv\"):\n",
    "            ti = extract_trial_index(p.stem, category); \n",
    "            if ti is not None: trials.add(ti)\n",
    "    else:\n",
    "        root = fly_dir / \"Eye_Prob_Dist\"\n",
    "        if root.is_dir():\n",
    "            for p in root.glob(\"*.csv\"):\n",
    "                if category in p.name.lower():\n",
    "                    ti = extract_trial_index(p.stem, category); \n",
    "                    if ti is not None: trials.add(ti)\n",
    "\n",
    "    # Eye_Antenna_Dist\n",
    "    ead = fly_dir / \"Eye_Antenna_Dist\"\n",
    "    if ead.is_dir():\n",
    "        for p in ead.glob(\"*combined.csv\"):\n",
    "            if category in p.name.lower():\n",
    "                ti = extract_trial_index(p.stem, category); \n",
    "                if ti is not None: trials.add(ti)\n",
    "\n",
    "    # Angle files anywhere\n",
    "    for p in fly_dir.rglob(\"*_distance_class_2_angle_ARB.csv\"):\n",
    "        if (_infer_category_from_path(p) in (category, None)):\n",
    "            ti = extract_trial_index(p.stem, category)\n",
    "            if ti is not None: trials.add(ti)\n",
    "\n",
    "    # Raw as last resort\n",
    "    for p in fly_dir.rglob(\"*_distance_class_2.csv\"):\n",
    "        if (_infer_category_from_path(p) in (category, None)):\n",
    "            ti = extract_trial_index(p.stem, category)\n",
    "            if ti is not None: trials.add(ti)\n",
    "\n",
    "    return sorted(trials)\n",
    "\n",
    "def _is_month_fly(p: Path) -> bool:\n",
    "    return p.is_dir() and any(p.name.lower().startswith(m) for m in MONTHS)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Main\n",
    "\n",
    "for fly_dir in sorted([p for p in ROOT.iterdir() if _is_month_fly(p)]):\n",
    "    fly_name = fly_dir.name\n",
    "    print(f\"\\n=== Fly: {fly_name} ===\")\n",
    "\n",
    "    for category in (\"training\", \"testing\"):\n",
    "        trials = _discover_trials(fly_dir, category)\n",
    "        if not trials:\n",
    "            print(f\"  [{category}] No trials discovered.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare output directory under three_line_videos\n",
    "        out_root = fly_dir / VIDEO_INPUT_DIR / VIDEO_OUTPUT_SUBDIR\n",
    "        out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for tri in trials:\n",
    "            # Find a video for this trial\n",
    "            video_path = _find_video_for_trial(fly_dir, category, tri)\n",
    "            if not video_path:\n",
    "                print(f\"  [{category} {tri}] ⤫ No matching video in {VIDEO_INPUT_DIR}/{category}/\")\n",
    "                continue\n",
    "\n",
    "            # Build up to three series\n",
    "            s_top   = _series_top_distance(fly_dir, category, tri)\n",
    "            s_angle = _series_angle_centered_pct(fly_dir, category, tri)\n",
    "            s_pct   = _series_robust_pct(fly_dir, category, tri)\n",
    "\n",
    "            series_list = []\n",
    "            xmins, xmaxs = [], []\n",
    "            odor_candidates = []\n",
    "\n",
    "            if s_top is not None:\n",
    "                t, y, on, off = s_top\n",
    "                series_list.append({\"t\": t, \"y\": y, \"label\": \"Proboscis Distance %\"})\n",
    "                xmins.append(np.nanmin(t)); xmaxs.append(np.nanmax(t))\n",
    "                odor_candidates.append((on, off))\n",
    "            if s_angle is not None:\n",
    "                t, y, on, off = s_angle\n",
    "                series_list.append({\"t\": t, \"y\": y, \"label\": \"Centered Proboscis Angle %\"})\n",
    "                xmins.append(np.nanmin(t)); xmaxs.append(np.nanmax(t))\n",
    "                odor_candidates.append((on, off))\n",
    "            if s_pct is not None:\n",
    "                t, y, on, off = s_pct\n",
    "                series_list.append({\"t\": t, \"y\": y, \"label\": \"Antenna Distance %\"})\n",
    "                xmins.append(np.nanmin(t)); xmaxs.append(np.nanmax(t))\n",
    "                odor_candidates.append((on, off))\n",
    "\n",
    "            if not series_list:\n",
    "                print(f\"  [{category} {tri}] ⤫ No data series found; skipping.\")\n",
    "                continue\n",
    "\n",
    "            xlim = (float(np.nanmin(xmins)), float(np.nanmax(xmaxs)))\n",
    "            # Prefer robust’s odor window if present; else first available\n",
    "            odor_on = odor_candidates[-1][0] if s_pct is not None else odor_candidates[0][0]\n",
    "            odor_off = odor_candidates[-1][1] if s_pct is not None else odor_candidates[0][1]\n",
    "\n",
    "            out_mp4 = out_root / f\"{fly_name}_{category}_{tri}_LINES_three_series.mp4\"\n",
    "            if out_mp4.exists():\n",
    "                print(f\"  [{category} {tri}] ⤫ Exists, skipping: {out_mp4.name}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  [{category} {tri}] ✓ Video: {video_path.name} → {out_mp4.name}\")\n",
    "            ok = _compose_lineplot_video(video_path, series_list, xlim, odor_on, odor_off, out_mp4,\n",
    "                             panel_height_fraction=PANEL_HEIGHT_FRACTION,\n",
    "                             ylim=YLIM)\n",
    "\n",
    "            if ok:\n",
    "                print(f\"  [{category} {tri}] [SAVED] {out_mp4.name}\")\n",
    "                if DELETE_SOURCE_AFTER_RENDER:\n",
    "                    _safe_unlink(video_path)\n",
    "            else:\n",
    "                print(f\"  [{category} {tri}] ⤫ Render failed; source retained.\")\n",
    "                \n",
    "    if DELETE_SOURCE_AFTER_RENDER and DELETE_EMPTY_INPUT_DIRS:\n",
    "        cat_input_dir = fly_dir / VIDEO_INPUT_DIR / category\n",
    "        # If all videos for this category were deleted and the folder is empty, remove it\n",
    "        if cat_input_dir.exists():\n",
    "            # Only consider known video types to decide emptiness\n",
    "            remaining = [p for p in cat_input_dir.iterdir()\n",
    "                         if p.is_file() and p.suffix.lower() in VIDEO_EXTS]\n",
    "            if not remaining:\n",
    "                _maybe_rmdir_empty(cat_input_dir)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9aa3a1-b6f3-48d0-8e8c-1dcfdceaadba",
   "metadata": {},
   "source": [
    "## Just RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b325639-1fe2-4cf9-a8a3-73fd53d366aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Line-plot panel under the video (Top %, Centered angle %, Class1–Class2 %)\n",
    "# Sources & normalization mirror the FIXED MEGA heatmap code.\n",
    "# Videos are read from: {fly}/three_line_videos/{training,testing}/\n",
    "# Outputs go to:        {fly}/three_line_videos/with_line_plots/{fly}_{category}_{trial}_LINES_three_series.mp4\n",
    "\n",
    "import os, re, glob, io\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip, VideoClip, CompositeVideoClip\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# REQUIRED: set main_directory (Path or str)\n",
    "assert 'main_directory' in globals(), \"Define main_directory = '/path/to/root' before running.\"\n",
    "ROOT = Path(main_directory).expanduser().resolve()\n",
    "assert ROOT.is_dir(), f\"Not a directory: {ROOT}\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Visual defaults\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 13,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'lines.linewidth': 2,\n",
    "    'axes.linewidth': 1.25,\n",
    "    'figure.dpi': 160,\n",
    "    'savefig.dpi': 300,\n",
    "    'legend.fontsize': 11\n",
    "})\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Constants (aligned with MEGA)\n",
    "FPS_DEFAULT   = 40.0\n",
    "ANCHOR_X, ANCHOR_Y = 1080.0, 540.0\n",
    "PCT_COL_ROBUST = \"distance_class1_class2_pct\"\n",
    "DIST_COL_ROBUST = \"distance_class1_class2\"\n",
    "PRE_SEC, POST_SEC = 30.0, 90.0\n",
    "TRIM_FRAC = 0.05  # bottom 5% trimmed-min for Eye_Antenna_Dist fallback\n",
    "MONTHS = (\n",
    "    \"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "    \"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\n",
    "    \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"\n",
    ")\n",
    "RMS_WINDOW_S = 1.0   # seconds for rolling RMS window\n",
    "THRESH_K     = 4.0   # threshold = mean_pre + K * std_pre\n",
    "\n",
    "# ── Panel rendering options ─────────────────────────────────────\n",
    "PANEL_HEIGHT_FRACTION = 0.24\n",
    "YLIM = (-100, 100)  # angle is [-100,100], distance lines are [0,100] → keep unified\n",
    "VIDEO_INPUT_DIR = \"videos_with_rms\"              # where copied trial videos live\n",
    "VIDEO_OUTPUT_SUBDIR = \"videos_with_rms\"            # under three_line_videos/\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Helpers (same behavior as MEGA)\n",
    "# Delete the source videos after rendering new ones\n",
    "DELETE_SOURCE_AFTER_RENDER = True\n",
    "# Optionally remove the input category folder if it becomes empty\n",
    "DELETE_EMPTY_INPUT_DIRS = True\n",
    "\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".mpg\", \".mpeg\", \".m4v\"}  # keep as in your script\n",
    "\n",
    "def _safe_unlink(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(f\"    Deleted source video: {p.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [warn] Could not delete {p}: {e}\")\n",
    "\n",
    "def _maybe_rmdir_empty(dir_path: Path):\n",
    "    try:\n",
    "        # remove folder if it contains no files/dirs\n",
    "        if dir_path.exists() and not any(dir_path.iterdir()):\n",
    "            dir_path.rmdir()\n",
    "            print(f\"    Removed empty folder: {dir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [warn] Could not remove folder {dir_path}: {e}\")\n",
    "\n",
    "def timestamp_to_seconds(ts) -> float:\n",
    "    if pd.isna(ts): return np.nan\n",
    "    try: return float(ts)\n",
    "    except Exception:\n",
    "        s = str(ts).strip(); parts = s.split(\":\")\n",
    "        try:\n",
    "            if len(parts)==4: hh,mm,ss,ms=parts; return int(hh)*3600+int(mm)*60+int(ss)+int(ms)/1000.0\n",
    "            if len(parts)==3: hh,mm,ss=parts;   return int(hh)*3600+int(mm)*60+float(ss)\n",
    "            if len(parts)==2: mm,ss=parts;      return int(mm)*60+float(ss)\n",
    "            if len(parts)==1: return float(parts[0])\n",
    "        except Exception: return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def find_col(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    cols = set(df.columns)\n",
    "    for c in cands:\n",
    "        if c in cols: return c\n",
    "    return None\n",
    "\n",
    "def derive_fps(df: pd.DataFrame) -> float:\n",
    "    fps_col = find_col(df, [\"fps\",\"FPS\",\"frame_rate\",\"frameRate\"])\n",
    "    if fps_col is not None:\n",
    "        fps_val = pd.to_numeric(df[fps_col], errors=\"coerce\").median()\n",
    "        if np.isfinite(fps_val) and fps_val > 0: return float(fps_val)\n",
    "    return FPS_DEFAULT\n",
    "\n",
    "def ensure_time_series(df: pd.DataFrame, frame_col: Optional[str], ts_col: Optional[str]):\n",
    "    if ts_col is not None:\n",
    "        ts = pd.to_numeric(df[ts_col], errors=\"coerce\") if ts_col in [\"time_seconds\",\"relative_time\",\"time_s\"] \\\n",
    "             else df[ts_col].apply(timestamp_to_seconds)\n",
    "        if ts.notna().sum() >= 2 and np.nanmax(np.diff(ts.dropna().values)) > 0:\n",
    "            return ts, {'used':'timestamp'}\n",
    "    if frame_col is not None:\n",
    "        frames = pd.to_numeric(df[frame_col], errors=\"coerce\")\n",
    "        if frames.notna().sum() >= 2:\n",
    "            fps = derive_fps(df); f0 = int(np.nanmin(frames.values))\n",
    "            return (frames - f0)/fps, {'used':'frame_fallback','fps':fps}\n",
    "    fps = derive_fps(df)\n",
    "    return pd.Series(np.arange(len(df), dtype=float)/fps, index=df.index), {'used':'index_fallback','fps':fps}\n",
    "\n",
    "def extract_trial_index(name: str, category: str) -> Optional[int]:\n",
    "    m = re.search(rf\"{category}_(\\d+)\", name.lower())\n",
    "    if m:\n",
    "        try: return int(m.group(1))\n",
    "        except: return None\n",
    "    m2 = re.search(r\"(training|testing)_(\\d+)\", name.lower())\n",
    "    if m2:\n",
    "        try: return int(m2.group(2))\n",
    "        except: return None\n",
    "    # fallback: last number in stem\n",
    "    nums = re.findall(r\"\\d+\", name)\n",
    "    if nums:\n",
    "        try: return int(nums[-1])\n",
    "        except: return None\n",
    "    return None\n",
    "\n",
    "def odor_window_from_ofm(df: pd.DataFrame, time_col: str = \"relative_time\"):\n",
    "    ofm_col = find_col(df, [\"OFM State\",\"OFM_State\",\"ofm_state\",\"ofm\"])\n",
    "    if ofm_col is None or time_col not in df.columns: return None\n",
    "    try:\n",
    "        mask = df[ofm_col].astype(str).str.lower().eq(\"during\")\n",
    "        if mask.any():\n",
    "            t = df[time_col]; idx = np.flatnonzero(mask.to_numpy())\n",
    "            if idx.size >= 2: return float(t.iloc[idx[0]]), float(t.iloc[idx[-1]])\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def compute_angle_deg_at_point2(df: pd.DataFrame) -> pd.Series:\n",
    "    req = [\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"]\n",
    "    if any(c not in df.columns for c in req): raise ValueError(\"Missing angle columns\")\n",
    "    p2x = df[\"x_class2\"].astype(float).to_numpy(); p2y = df[\"y_class2\"].astype(float).to_numpy()\n",
    "    p3x = df[\"x_class6\"].astype(float).to_numpy(); p3y = df[\"y_class6\"].astype(float).to_numpy()\n",
    "    ux, uy = (ANCHOR_X - p2x), (ANCHOR_Y - p2y)\n",
    "    vx, vy = (p3x - p2x), (p3y - p2y)\n",
    "    dot = ux*vx + uy*vy; cross = ux*vy - uy*vx\n",
    "    n1 = np.hypot(ux, uy); n2 = np.hypot(vx, vy)\n",
    "    valid = (n1 > 0) & (n2 > 0) & np.isfinite(dot) & np.isfinite(cross)\n",
    "    ang = np.full(len(p2x), np.nan); ang[valid] = np.degrees(np.arctan2(np.abs(cross[valid]), dot[valid]))\n",
    "    return pd.Series(ang, index=df.index, name=\"angle_ARB_deg\")\n",
    "\n",
    "def find_fly_reference_angle(csvs_raw: List[Path]) -> float:\n",
    "    best = None\n",
    "    for p in csvs_raw:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if not {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df.columns): continue\n",
    "            ang = compute_angle_deg_at_point2(df)\n",
    "            dist_col = find_col(df, [\"distance_percentage\",\"distance_percent\",\"distance_pct\",\"distance_class1_class2_pct\"])\n",
    "            if dist_col is None: continue\n",
    "            dist = pd.to_numeric(df[dist_col], errors=\"coerce\").to_numpy()\n",
    "            exact = np.flatnonzero(dist == 0)\n",
    "            if exact.size > 0:\n",
    "                idx = int(exact[0]); angle_here = float(ang.iloc[idx]) if np.isfinite(ang.iloc[idx]) else np.nan\n",
    "                cand = (0, 0.0, angle_here)\n",
    "            else:\n",
    "                with np.errstate(invalid=\"ignore\"): absd = np.abs(dist)\n",
    "                if not np.isfinite(absd).any(): continue\n",
    "                idx = int(np.nanargmin(absd)); angle_here = float(ang.iloc[idx]) if np.isfinite(ang.iloc[idx]) else np.nan\n",
    "                cand = (1, float(absd[idx]), angle_here)\n",
    "            if best is None or cand < best: best = cand\n",
    "        except Exception:\n",
    "            pass\n",
    "    return best[2] if best is not None else np.nan\n",
    "\n",
    "def compute_fly_max_abs_centered(csvs_raw: List[Path], ref_angle: float) -> float:\n",
    "    fly_max = 0.0\n",
    "    for p in csvs_raw:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if not {\"x_class2\",\"y_class2\",\"x_class6\",\"y_class6\"}.issubset(df.columns): continue\n",
    "            ang = compute_angle_deg_at_point2(df)\n",
    "            centered = ang - ref_angle if np.isfinite(ref_angle) else ang*0.0\n",
    "            local = np.nanmax(np.abs(centered.to_numpy(dtype=float)))\n",
    "            if np.isfinite(local): fly_max = max(fly_max, float(local))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fly_max if np.isfinite(fly_max) and fly_max > 0 else np.nan\n",
    "\n",
    "def _infer_category_from_path(p: Path) -> Optional[str]:\n",
    "    tokens = \" \".join([*p.parts, p.stem]).lower()\n",
    "    if any(t in tokens for t in (\"training\",\"train\",\"trn\")): return \"training\"\n",
    "    if any(t in tokens for t in (\"testing\",\"test\",\"tst\")):  return \"testing\"\n",
    "    return None\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Series collectors (mirror MEGA). Return (t, y, odor_on, odor_off).\n",
    "\n",
    "def _series_rms_from_rmscalc(fly_dir: Path, category: str, trial_index: int):\n",
    "    \"\"\"\n",
    "    Load the combined CSV for this trial from <fly>/RMS_calculations/,\n",
    "    build a timebase, compute rolling RMS of a distance-% column, and return:\n",
    "      (t, rms, odor_on, odor_off, threshold)\n",
    "    \"\"\"\n",
    "    # Candidate filenames like: september_08_fly_1_testing_1_*_class_combined.csv\n",
    "    rdir = fly_dir / \"RMS_calculations\"\n",
    "    if not rdir.is_dir():\n",
    "        return None\n",
    "\n",
    "    cands = sorted([p for p in rdir.glob(\"*merged.csv\")\n",
    "                    if category in p.name.lower() and extract_trial_index(p.stem, category) == trial_index])\n",
    "    if not cands:\n",
    "        return None\n",
    "    csv_path = cands[0]\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Time\n",
    "    frame_col = find_col(df, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "    ts_col    = find_col(df, [\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\",\"time_s\"])\n",
    "    ts, _meta = ensure_time_series(df, frame_col, ts_col)\n",
    "    if ts.notna().sum() < 2:\n",
    "        return None\n",
    "    time_s = pd.to_numeric(ts, errors=\"coerce\")\n",
    "    t0 = float(np.nanmin(time_s))\n",
    "    df[\"relative_time\"] = time_s - t0\n",
    "\n",
    "    # Odor window from OFM_State if available; else default [PRE_SEC, PRE_SEC+30]\n",
    "    odor = odor_window_from_ofm(df, \"relative_time\") or (PRE_SEC, PRE_SEC + 30.0)\n",
    "    odor_on, odor_off = map(float, odor)\n",
    "    total_duration = PRE_SEC + (odor_off - odor_on) + POST_SEC\n",
    "\n",
    "    # Pick a distance-% column (common names across your pipeline)\n",
    "    pct_col = find_col(df, [\n",
    "        \"distance_class1_class2_pct\",\n",
    "        \"distance_percentage\",\"distance_percent\",\"distance_pct\",\n",
    "        \"distance_proboscis_eye_pct\",\"proboscis_eye_distance_pct\",\"eye_prob_distance_pct\",\n",
    "        \"distance_percentage_2_6\"  # if you prefer this, move it to the top of the list\n",
    "    ])\n",
    "    if pct_col is None:\n",
    "        return None\n",
    "\n",
    "    vals = pd.to_numeric(df[pct_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "    # Rolling RMS (centered, 1 s window by FPS)\n",
    "    fps = derive_fps(df)\n",
    "    win = max(1, int(round(fps * RMS_WINDOW_S)))\n",
    "    s = pd.Series(vals)\n",
    "    rms = s.rolling(win, min_periods=max(1, win // 2), center=True).apply(\n",
    "        lambda x: float(np.sqrt(np.nanmean(np.square(x)))), raw=False\n",
    "    ).to_numpy()\n",
    "\n",
    "    # Time axis sized to the series\n",
    "    t = np.linspace(0.0, total_duration, len(rms))\n",
    "\n",
    "    # Threshold from pre-odor region\n",
    "    pre_mask = (df[\"relative_time\"].to_numpy(dtype=float) < PRE_SEC)\n",
    "    # align mask length to rms length if needed\n",
    "    if pre_mask.size != rms.size:\n",
    "        # fallback: approximate using fps\n",
    "        pre_vals = rms[:max(1, int(PRE_SEC * fps))]\n",
    "    else:\n",
    "        pre_vals = rms[pre_mask]\n",
    "    mu = float(np.nanmean(pre_vals)) if np.isfinite(pre_vals).any() else np.nan\n",
    "    sd = float(np.nanstd(pre_vals))  if np.isfinite(pre_vals).any() else np.nan\n",
    "    threshold = mu + THRESH_K * sd if np.isfinite(mu) and np.isfinite(sd) else np.nan\n",
    "\n",
    "    return t, rms.astype(float), odor_on, odor_off, threshold\n",
    "\n",
    "# Cache per-fly trimmed-min/global-max for Eye_Antenna_Dist\n",
    "_ead_stats_cache: Dict[Path, Tuple[float,float]] = {}\n",
    "\n",
    "def _ead_compute_trim_min_max(fly_dir: Path) -> Optional[Tuple[float,float]]:\n",
    "    if fly_dir in _ead_stats_cache:\n",
    "        return _ead_stats_cache[fly_dir]\n",
    "    base = fly_dir / \"RMS_calculations\"\n",
    "    if not base.is_dir(): return None\n",
    "    vals = []\n",
    "    for p in sorted(base.glob(\"*merged.csv\")):\n",
    "        try:\n",
    "            v = pd.to_numeric(pd.read_csv(p, usecols=[DIST_COL_ROBUST])[DIST_COL_ROBUST], errors=\"coerce\").to_numpy()\n",
    "            vals.append(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not vals: return None\n",
    "    allv = np.concatenate(vals); allv = allv[np.isfinite(allv)]\n",
    "    if allv.size == 0: return None\n",
    "    gmax = float(np.max(allv))\n",
    "    p5   = float(np.percentile(allv, 100*TRIM_FRAC, method=\"linear\"))\n",
    "    kept = allv[allv >= p5]\n",
    "    trimmed_min = float(np.min(kept)) if kept.size else float(np.min(allv))\n",
    "    _ead_stats_cache[fly_dir] = (trimmed_min, gmax)\n",
    "    return _ead_stats_cache[fly_dir]\n",
    "\n",
    "def _series_robust_pct(fly_dir: Path, category: str, trial_index: int):\n",
    "    base = fly_dir / \"RMS_calculations\"\n",
    "    if not base.is_dir(): return None\n",
    "    csvs = sorted([p for p in base.glob(\"*merged.csv\") if category in p.name.lower()])\n",
    "    chosen = None\n",
    "    for p in csvs:\n",
    "        if extract_trial_index(p.stem, category) == trial_index:\n",
    "            chosen = p; break\n",
    "    if chosen is None: return None\n",
    "\n",
    "    df = pd.read_csv(chosen); df.columns = df.columns.str.strip()\n",
    "    frame_col = find_col(df, [\"frame\",\"Frame\",\"frame_num\",\"frame_index\"])\n",
    "    ts_col    = find_col(df, [\"timestamp\",\"Timestamp\",\"time\",\"Time\",\"time_seconds\",\"relative_time\",\"time_s\"])\n",
    "    ts, _     = ensure_time_series(df, frame_col, ts_col)\n",
    "    if ts.notna().sum() < 2: return None\n",
    "\n",
    "    # Ensure percentage in-memory if missing\n",
    "    if PCT_COL_ROBUST not in df.columns:\n",
    "        stats = _ead_compute_trim_min_max(fly_dir)\n",
    "        if stats is None: return None\n",
    "        fly_min, fly_max = stats\n",
    "        dist = pd.to_numeric(df.get(DIST_COL_ROBUST, np.nan), errors=\"coerce\")\n",
    "        if np.isfinite(fly_min) and np.isfinite(fly_max) and (fly_max > fly_min):\n",
    "            df[PCT_COL_ROBUST] = (dist - fly_min) / (fly_max - fly_min) * 100.0\n",
    "        else:\n",
    "            df[PCT_COL_ROBUST] = np.where(dist.notna(), 0.0, np.nan)\n",
    "\n",
    "    df[\"time_seconds\"] = pd.to_numeric(ts, errors=\"coerce\")\n",
    "    df[\"relative_time\"] = df[\"time_seconds\"] - df[\"time_seconds\"].min()\n",
    "    odor = odor_window_from_ofm(df, \"relative_time\") or (PRE_SEC, PRE_SEC + 30.0)\n",
    "    odor_start, odor_end = odor\n",
    "    total_duration = PRE_SEC + (odor_end - odor_start) + POST_SEC\n",
    "\n",
    "    if frame_col is not None and frame_col in df.columns:\n",
    "        frames = pd.to_numeric(df[frame_col], errors=\"coerce\").dropna().astype(int)\n",
    "        total_frames = np.arange(frames.min(), frames.max()+1, dtype=int)\n",
    "        idx_map = {f:i for i,f in enumerate(total_frames)}\n",
    "        present_idx = [idx_map.get(int(f)) for f in frames if int(f) in idx_map]\n",
    "        vals = pd.to_numeric(df.loc[frames.index, PCT_COL_ROBUST], errors=\"coerce\").to_numpy()\n",
    "        full = np.full_like(total_frames, np.nan, dtype=float)\n",
    "        if present_idx: full[np.array(present_idx, dtype=int)] = vals\n",
    "        t = np.linspace(0, total_duration, len(total_frames)); y = full\n",
    "    else:\n",
    "        vals = pd.to_numeric(df[PCT_COL_ROBUST], errors=\"coerce\").to_numpy()\n",
    "        t = np.linspace(0, total_duration, len(vals)); y = vals\n",
    "    return t, y.astype(float), float(PRE_SEC), float(PRE_SEC + (odor_end - odor_start))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Video discovery & matching in three_line_videos/<category>\n",
    "\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".mpg\", \".mpeg\", \".m4v\"}\n",
    "\n",
    "def _find_video_for_trial(fly_dir: Path, category: str, tri: int) -> Optional[Path]:\n",
    "    vid_dir = fly_dir / VIDEO_INPUT_DIR / category\n",
    "    if not vid_dir.is_dir(): return None\n",
    "    vids = [p for p in vid_dir.iterdir() if p.is_file() and p.suffix.lower() in VIDEO_EXTS]\n",
    "    # Heuristics: exact token \"<category>_<tri>\", then \"_<tri>.\", then single-file fallback\n",
    "    token = f\"{category}_{tri}\"\n",
    "    cand = [v for v in vids if token in v.stem.lower()]\n",
    "    if cand: return cand[0]\n",
    "    cand = [v for v in vids if re.search(rf\"[_\\-]{tri}(\\D|$)\", v.stem)]\n",
    "    if cand: return cand[0]\n",
    "    if len(vids) == 1: return vids[0]\n",
    "    return None\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Line-panel rendering & composition\n",
    "\n",
    "def _render_line_panel_png(series_list: List[dict],\n",
    "                           width_px: int,\n",
    "                           height_px: int,\n",
    "                           xlim: Tuple[float,float],\n",
    "                           ylim: Tuple[float,float],\n",
    "                           odor_on: float | None,\n",
    "                           odor_off: float | None,\n",
    "                           threshold: float | None) -> np.ndarray:\n",
    "    fig, ax = plt.subplots(figsize=(width_px/100, height_px/100), dpi=100)\n",
    "\n",
    "    # Expect one series: RMS\n",
    "    if series_list:\n",
    "        s = series_list[0]\n",
    "        ax.plot(s[\"t\"], s[\"y\"], label=\"RMS\", linewidth=1.2, color=\"blue\")\n",
    "\n",
    "    # Horizontal threshold in red\n",
    "    if threshold is not None and np.isfinite(threshold):\n",
    "        ax.axhline(threshold, color=\"red\", linewidth=1.2, label=\"Threshold\")\n",
    "\n",
    "    # Odor-on/off markers (vertical red)\n",
    "    if odor_on is not None and odor_off is not None and np.isfinite(odor_on) and np.isfinite(odor_off):\n",
    "        ax.axvline(odor_on, color='red', linewidth=1.0)\n",
    "        ax.axvline(odor_off, color='red', linewidth=1.0)\n",
    "\n",
    "    ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "\n",
    "    if series_list or (threshold is not None and np.isfinite(threshold)):\n",
    "        leg = ax.legend(loc='upper right', frameon=True, framealpha=0.9)\n",
    "        leg.get_frame().set_facecolor('white')\n",
    "        leg.get_frame().set_edgecolor('black')\n",
    "        leg.get_frame().set_linewidth(0.8)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    buf = io.BytesIO(); plt.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0); plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    arr = np.array(Image.open(buf).convert(\"RGB\"))\n",
    "    return np.array(Image.fromarray(arr).resize((width_px, height_px), resample=Image.BILINEAR))\n",
    "\n",
    "def _compose_lineplot_video(video_path: Path,\n",
    "                            series_list: List[dict],\n",
    "                            xlim: Tuple[float,float],\n",
    "                            odor_on: float | None,\n",
    "                            odor_off: float | None,\n",
    "                            out_mp4: Path,\n",
    "                            panel_height_fraction: float = PANEL_HEIGHT_FRACTION,\n",
    "                            ylim: Tuple[float,float] = YLIM,\n",
    "                            threshold: float | None = None) -> bool:\n",
    "    clip = VideoFileClip(str(video_path))\n",
    "    vw, vh = clip.size\n",
    "    ph = max(1, int(vh * panel_height_fraction))\n",
    "\n",
    "    bg = _render_line_panel_png(series_list, vw, ph, xlim, ylim, odor_on, odor_off, threshold)\n",
    "    \n",
    "    def _add_cursor(img: np.ndarray, t_cur: float, xlim: Tuple[float,float]) -> np.ndarray:\n",
    "        img = img.copy()\n",
    "        t0, t1 = float(xlim[0]), float(xlim[1])\n",
    "        if not (np.isfinite(t0) and np.isfinite(t1)) or t1 <= t0: return img\n",
    "        frac = float(np.clip((t_cur - t0) / (t1 - t0), 0.0, 0.9999))\n",
    "        x = int(frac * (img.shape[1] - 1))\n",
    "        img[:, x:x+2, 0] = 255; img[:, x:x+2, 1:] = 0\n",
    "        return img\n",
    "\n",
    "    def panel_frame(t_cur: float) -> np.ndarray:\n",
    "        return _add_cursor(bg, t_cur, xlim)\n",
    "\n",
    "    panel_clip = VideoClip(panel_frame, duration=clip.duration)\n",
    "    comp = CompositeVideoClip([clip.set_position((\"center\", 0)),\n",
    "                               panel_clip.set_position((\"center\", vh))],\n",
    "                              size=(vw, vh + ph))\n",
    "\n",
    "    out_mp4.parent.mkdir(parents=True, exist_ok=True)\n",
    "    comp.write_videofile(str(out_mp4), fps=clip.fps, codec=\"libx264\", audio=False, preset=\"ultrafast\")\n",
    "\n",
    "    # cleanup\n",
    "    for c in (clip, panel_clip, comp):\n",
    "        try: c.close()\n",
    "        except: pass\n",
    "\n",
    "    try:\n",
    "        return out_mp4.exists() and out_mp4.stat().st_size > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Trial discovery (union across sources, per category), then compose videos\n",
    "\n",
    "def _discover_trials(fly_dir: Path, category: str) -> List[int]:\n",
    "    trials = set()\n",
    "\n",
    "    # RMS_calculations\n",
    "    rdir = fly_dir / \"RMS_calculations\"\n",
    "    if rdir.is_dir():\n",
    "        for p in rdir.glob(\"*merged.csv\"):\n",
    "            if category in p.name.lower():\n",
    "                ti = extract_trial_index(p.stem, category)\n",
    "                if ti is not None:\n",
    "                    trials.add(ti)\n",
    "\n",
    "    # keep any other sources you want (optional)...\n",
    "\n",
    "    return sorted(trials)\n",
    "\n",
    "def _is_month_fly(p: Path) -> bool:\n",
    "    return p.is_dir() and any(p.name.lower().startswith(m) for m in MONTHS)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Main\n",
    "\n",
    "for fly_dir in sorted([p for p in ROOT.iterdir() if _is_month_fly(p)]):\n",
    "    fly_name = fly_dir.name\n",
    "    print(f\"\\n=== Fly: {fly_name} ===\")\n",
    "\n",
    "    for category in (\"training\", \"testing\"):\n",
    "        trials = _discover_trials(fly_dir, category)\n",
    "        if not trials:\n",
    "            print(f\"  [{category}] No trials discovered.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare output directory under three_line_videos\n",
    "        out_root = fly_dir / VIDEO_INPUT_DIR / VIDEO_OUTPUT_SUBDIR\n",
    "        out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for tri in trials:\n",
    "            # Find a video for this trial\n",
    "            video_path = _find_video_for_trial(fly_dir, category, tri)\n",
    "            if not video_path:\n",
    "                print(f\"  [{category} {tri}] ⤫ No matching video in {VIDEO_INPUT_DIR}/{category}/\")\n",
    "                continue\n",
    "\n",
    "            # Only RMS series\n",
    "            s_rms = _series_rms_from_rmscalc(fly_dir, category, tri)\n",
    "            if s_rms is None:\n",
    "                print(f\"  [{category} {tri}] ⤫ No RMS series (RMS_calculations); skipping.\")\n",
    "                continue\n",
    "\n",
    "            t, y, on, off, thr = s_rms\n",
    "            series_list = [{\"t\": t, \"y\": y, \"label\": \"RMS\"}]\n",
    "            xlim = (float(np.nanmin(t)), float(np.nanmax(t)))\n",
    "            odor_on, odor_off = on, off\n",
    "\n",
    "            out_mp4 = out_root / f\"{fly_name}_{category}_{tri}_LINES_rms.mp4\"\n",
    "            if out_mp4.exists():\n",
    "                print(f\"  [{category} {tri}] ⤫ Exists, skipping: {out_mp4.name}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  [{category} {tri}] ✓ Video: {video_path.name} → {out_mp4.name}\")\n",
    "            ok = _compose_lineplot_video(\n",
    "                video_path, series_list, xlim, odor_on, odor_off, out_mp4,\n",
    "                panel_height_fraction=PANEL_HEIGHT_FRACTION, ylim=YLIM, threshold=thr\n",
    "            )\n",
    "\n",
    "            if ok:\n",
    "                print(f\"  [{category} {tri}] [SAVED] {out_mp4.name}\")\n",
    "                if DELETE_SOURCE_AFTER_RENDER:\n",
    "                    _safe_unlink(video_path)\n",
    "            else:\n",
    "                print(f\"  [{category} {tri}] ⤫ Render failed; source retained.\")\n",
    "                \n",
    "    if DELETE_SOURCE_AFTER_RENDER and DELETE_EMPTY_INPUT_DIRS:\n",
    "        cat_input_dir = fly_dir / VIDEO_INPUT_DIR / category\n",
    "        # If all videos for this category were deleted and the folder is empty, remove it\n",
    "        if cat_input_dir.exists():\n",
    "            # Only consider known video types to decide emptiness\n",
    "            remaining = [p for p in cat_input_dir.iterdir()\n",
    "                         if p.is_file() and p.suffix.lower() in VIDEO_EXTS]\n",
    "            if not remaining:\n",
    "                _maybe_rmdir_empty(cat_input_dir)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad082ef-800c-4678-bc5b-9b52265be41e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Single CSV File --> Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7578df-4268-4b4d-9c86-e801780e962e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Wide CSV of per-frame ENVELOPE across MANY main_directories\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "# ───────── INPUT: add as many roots as you need ─────────\n",
    "ROOTS = [\n",
    "    Path(\"/home/ramanlab/Documents/cole/Data/flys/opto_benz/\"),\n",
    "    Path(\"/home/ramanlab/Documents/cole/Data/flys/opto_EB/\"),\n",
    "    Path(\"/home/ramanlab/Documents/cole/Data/flys/opto_benz_1/\"),\n",
    "]\n",
    "\n",
    "# ───────── CONFIG (mirrors your envelope logic) ─────────\n",
    "MEASURE_COLS  = [\"distance_percentage\", \"distance_percentage_2_6\"]\n",
    "FPS_DEFAULT   = 40\n",
    "WINDOW_SEC    = 0.25\n",
    "WINDOW_FRAMES = max(int(WINDOW_SEC * FPS_DEFAULT), 1)\n",
    "\n",
    "# Output file (combined for all datasets)\n",
    "OUT_WIDE_CSV = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/all_envelope_rows_wide.csv\")\n",
    "\n",
    "# AFTER\n",
    "TRIAL_REGEX = re.compile(r\"(testing|training)_(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "# Timestamp + frame columns we’ll look for\n",
    "TIMESTAMP_CANDIDATES = [\"UTC_ISO\", \"Timestamp\", \"Number\", \"MonoNs\"]\n",
    "FRAME_CANDIDATES     = [\"Frame\", \"FrameNumber\", \"Frame Number\"]\n",
    "\n",
    "# Fallback when FPS can’t be inferred from timestamps (no video here, so use constant 50 like prior script)\n",
    "FALLBACK_FPS = 40\n",
    "\n",
    "def _pick_timestamp_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in TIMESTAMP_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _pick_frame_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in FRAME_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _to_seconds_series(df: pd.DataFrame, ts_col: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return float seconds aligned to rows (t=0 at first valid).\n",
    "    - UTC_ISO / Timestamp: ISO-8601 strings → seconds\n",
    "    - Number: numeric seconds\n",
    "    - MonoNs: numeric nanoseconds → seconds\n",
    "    \"\"\"\n",
    "    s = df[ts_col]\n",
    "    if ts_col in (\"UTC_ISO\", \"Timestamp\"):\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\", utc=(ts_col == \"UTC_ISO\"))\n",
    "        secs = dt.astype(\"int64\") / 1e9  # NaT -> NaN\n",
    "        t0 = np.nanmin(secs.values)\n",
    "        return (secs - t0).astype(float)\n",
    "\n",
    "    if ts_col == \"Number\":\n",
    "        vals = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "        t0 = np.nanmin(vals.values)\n",
    "        return vals - t0\n",
    "\n",
    "    if ts_col == \"MonoNs\":\n",
    "        vals = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "        secs = vals / 1e9\n",
    "        t0 = np.nanmin(secs.values)\n",
    "        return secs - t0\n",
    "\n",
    "    raise ValueError(f\"Unsupported timestamp column: {ts_col}\")\n",
    "\n",
    "def _estimate_fps_from_seconds(seconds_series: pd.Series) -> Optional[float]:\n",
    "    mask = seconds_series.notna()\n",
    "    if mask.sum() < 2:\n",
    "        return None\n",
    "    duration = seconds_series[mask].iloc[-1] - seconds_series[mask].iloc[0]\n",
    "    if duration <= 0:\n",
    "        return None\n",
    "    return mask.sum() / duration\n",
    "\n",
    "def _resolve_measure_column(df: pd.DataFrame) -> str | None:\n",
    "    return next((c for c in MEASURE_COLS if c in df.columns), None)\n",
    "\n",
    "def _compute_envelope(series: pd.Series, win_frames: int) -> np.ndarray:\n",
    "    \"\"\"Clip to [0,100] → Hilbert analytic envelope → centered rolling mean; length preserved.\"\"\"\n",
    "    series = pd.to_numeric(series, errors=\"coerce\").fillna(0.0).clip(lower=0, upper=100)\n",
    "    analytic = hilbert(series.to_numpy())\n",
    "    env = np.abs(analytic)\n",
    "    return (\n",
    "        pd.Series(env, index=series.index)\n",
    "          .rolling(window=win_frames, center=True, min_periods=1)\n",
    "          .mean()\n",
    "          .to_numpy()\n",
    "    )\n",
    "\n",
    "def _infer_trial_type(p: Path) -> str:\n",
    "    s = (p.stem + \"/\" + \"/\".join(q.name for q in p.parents)).lower()\n",
    "    if \"testing\" in s:  return \"testing\"\n",
    "    if \"training\" in s: return \"training\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def _trial_label(p: Path) -> str:\n",
    "    \"\"\"\n",
    "    Return 'testing_<n>' or 'training_<n>' if found anywhere in the filename;\n",
    "    otherwise fall back to a clean, short label.\n",
    "    \"\"\"\n",
    "    m = TRIAL_REGEX.search(p.stem)\n",
    "    if not m:\n",
    "        # also look up the directory chain in case the file stem lacks the token\n",
    "        chain = (p.stem + \"/\" + \"/\".join(q.name for q in p.parents)).lower()\n",
    "        m = TRIAL_REGEX.search(chain)\n",
    "    if m:\n",
    "        kind, num = m.group(1).lower(), m.group(2)\n",
    "        return f\"{kind}_{num}\"\n",
    "\n",
    "    # Fallback: compress a long stem into something readable\n",
    "    stem = p.stem\n",
    "    # try to extract a trailing integer like *_3\n",
    "    m2 = re.search(r\"(\\d+)$\", stem)\n",
    "    if m2:\n",
    "        return f\"{_infer_trial_type(p)}_{m2.group(1)}\"\n",
    "    return stem  # last resort\n",
    "\n",
    "def _find_trial_csvs(fly_dir: Path):\n",
    "    \"\"\"Prefer RMS_calculations subtree; fallback to fly root; include testing/training CSVs.\"\"\"\n",
    "    search_root = fly_dir / \"RMS_calculations\"\n",
    "    if not search_root.is_dir():\n",
    "        search_root = fly_dir\n",
    "    patterns = [\"**/*testing*.csv\", \"**/*training*.csv\"]\n",
    "    seen = set()\n",
    "    for pat in patterns:\n",
    "        for csv in search_root.glob(pat):\n",
    "            if csv.is_file():\n",
    "                rp = csv.resolve()\n",
    "                if rp not in seen:\n",
    "                    seen.add(rp)\n",
    "                    yield rp\n",
    "\n",
    "# ───────── PASS 1: discover items and determine max row length ─────────\n",
    "items = []   # [{dataset, fly, csv_path, trial_type, trial_label, measure_col, n_frames}]\n",
    "max_len = 0\n",
    "\n",
    "for root in ROOTS:\n",
    "    root = root.expanduser().resolve()\n",
    "    assert root.is_dir(), f\"Not a directory: {root}\"\n",
    "    dataset = root.name  # e.g., \"ACV\"\n",
    "\n",
    "    for fly_dir in sorted(p for p in root.iterdir() if p.is_dir()):\n",
    "        fly = fly_dir.name\n",
    "        for csv_path in _find_trial_csvs(fly_dir):\n",
    "            # Resolve measure column from header\n",
    "            try:\n",
    "                header_df = pd.read_csv(csv_path, nrows=0)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skip {csv_path.name}: header read error: {e}\")\n",
    "                continue\n",
    "            col = _resolve_measure_column(header_df)\n",
    "            if col is None:\n",
    "                print(f\"[SKIP] {csv_path.name}: none of {MEASURE_COLS} present.\")\n",
    "                continue\n",
    "            # Count frames cheaply\n",
    "            try:\n",
    "                n_frames = pd.read_csv(csv_path, usecols=[col]).shape[0]\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skip {csv_path.name}: count error: {e}\")\n",
    "                continue\n",
    "\n",
    "            items.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"fly\": fly,\n",
    "                \"csv_path\": csv_path,\n",
    "                \"trial_type\": _infer_trial_type(csv_path),\n",
    "                \"trial_label\": _trial_label(csv_path),\n",
    "                \"measure_col\": col,\n",
    "                \"n_frames\": n_frames\n",
    "            })\n",
    "            max_len = max(max_len, n_frames)\n",
    "\n",
    "if not items:\n",
    "    raise RuntimeError(\"No eligible testing/training CSVs found in provided roots.\")\n",
    "\n",
    "print(f\"[INFO] Datasets: {[r.name for r in ROOTS]}\")\n",
    "print(f\"[INFO] Discovered {len(items)} videos. Max frames = {max_len}\")\n",
    "\n",
    "# ───────── PASS 2: compute envelope and write combined wide CSV ─────────\n",
    "cols = [\"dataset\", \"fly\", \"trial_type\", \"trial_label\", \"fps\"] + [f\"env_{i}\" for i in range(max_len)]\n",
    "pd.DataFrame(columns=cols).to_csv(OUT_WIDE_CSV, index=False)\n",
    "\n",
    "for it in items:\n",
    "    dataset     = it[\"dataset\"]\n",
    "    fly         = it[\"fly\"]\n",
    "    csv_path    = it[\"csv_path\"]\n",
    "    trial_type  = it[\"trial_type\"]\n",
    "    label       = it[\"trial_label\"]\n",
    "    measure_col = it[\"measure_col\"]\n",
    "\n",
    "    # --- Determine FPS from timestamps, if possible ---  ← INSERT HERE\n",
    "    try:\n",
    "        hdr2 = pd.read_csv(csv_path, nrows=0)\n",
    "    except Exception:\n",
    "        hdr2 = pd.DataFrame()\n",
    "\n",
    "    frame_col = _pick_frame_column(hdr2) if not hdr2.empty else None\n",
    "    ts_col    = _pick_timestamp_column(hdr2) if not hdr2.empty else None\n",
    "\n",
    "    fps = np.nan\n",
    "    if frame_col is not None and ts_col is not None:\n",
    "        try:\n",
    "            # Read just the needed columns\n",
    "            df_ts = pd.read_csv(csv_path, usecols=[frame_col, ts_col])\n",
    "            secs  = _to_seconds_series(df_ts, ts_col)\n",
    "            fps_from_csv = _estimate_fps_from_seconds(secs)\n",
    "            if fps_from_csv and np.isfinite(fps_from_csv) and fps_from_csv > 0:\n",
    "                fps = float(fps_from_csv)\n",
    "            else:\n",
    "                fps = float(FALLBACK_FPS)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] FPS inference failed for {csv_path.name}: {e}\")\n",
    "            fps = float(FALLBACK_FPS)\n",
    "    else:\n",
    "        # Couldn’t find both a frame and a timestamp column; mimic prior script’s constant fallback\n",
    "        fps = float(FALLBACK_FPS)\n",
    "    # --- END FPS BLOCK ---\n",
    "\n",
    "    # Now read the measure column and compute envelope\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[measure_col])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Read failed {csv_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    env = _compute_envelope(df[measure_col], WINDOW_FRAMES).astype(float)\n",
    "\n",
    "    # Include fps in the output row\n",
    "    row = [dataset, fly, trial_type, label, fps] + list(env)\n",
    "\n",
    "    # pad/truncate to max_len\n",
    "    if len(env) < max_len:\n",
    "        row += [np.nan] * (max_len - len(env))\n",
    "    elif len(env) > max_len:\n",
    "        row = row[:5 + max_len]  # account for 5 metadata cols now\n",
    "\n",
    "    pd.DataFrame([row], columns=cols).to_csv(OUT_WIDE_CSV, mode=\"a\", header=False, index=False)\n",
    "\n",
    "print(f\"[OK] Wrote combined envelope table: {OUT_WIDE_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f0ae5-03fa-4ae6-a4e7-aee456392086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Convert wide envelope CSV → 16-bit numeric matrix + code key\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ===== INPUT / OUTPUT =====\n",
    "INPUT_CSV = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/all_envelope_rows_wide.csv\")  # change if your file lives elsewhere\n",
    "OUT_DIR   = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/\")                           # change if desired\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MATRIX_NPY = OUT_DIR / \"envelope_matrix_float16.npy\"   # 16-bit floating matrix\n",
    "CODE_KEY   = OUT_DIR / \"code_key.txt\"                  # human-readable mapping & schema\n",
    "CODES_JSON = OUT_DIR / \"code_maps.json\"                # machine-readable mappings (optional)\n",
    "\n",
    "# ===== LOAD =====\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Identify metadata columns (present subset)\n",
    "meta_cols_all = [\"dataset\", \"fly\", \"trial_type\", \"trial_label\", \"fps\"]\n",
    "meta_cols = [c for c in meta_cols_all if c in df.columns]\n",
    "assert meta_cols, \"No metadata columns found. Expected at least one of: dataset, fly, trial_type, trial_label.\"\n",
    "\n",
    "# Envelope columns (everything else)\n",
    "env_cols = [c for c in df.columns if c not in meta_cols]\n",
    "assert len(env_cols) > 0, \"No envelope columns found.\"\n",
    "\n",
    "# ===== BUILD INTEGER CODES FOR METADATA =====\n",
    "# Codes start at 1; 0 is reserved for 'unknown'\n",
    "code_maps = {}\n",
    "for col in meta_cols:\n",
    "    uniques = pd.Series(df[col].astype(str).fillna(\"UNKNOWN\")).unique().tolist()\n",
    "    mapping = {\"UNKNOWN\": 0}\n",
    "    next_code = 1\n",
    "    for u in uniques:\n",
    "        if u not in mapping:\n",
    "            mapping[u] = next_code\n",
    "            next_code += 1\n",
    "    code_maps[col] = mapping\n",
    "\n",
    "# Apply codes to a copy\n",
    "df_num = df.copy()\n",
    "for col, mapping in code_maps.items():\n",
    "    df_num[col] = df_num[col].astype(str).map(mapping).fillna(0).astype(np.int32)\n",
    "\n",
    "# Ensure envelope columns are numeric and NaN-free\n",
    "df_num[env_cols] = df_num[env_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df_num[env_cols] = df_num[env_cols].fillna(0.0)\n",
    "\n",
    "# ===== BUILD THE MATRIX (float16) =====\n",
    "# Order: [meta_cols...] + [env_0...env_N]\n",
    "ordered_cols = meta_cols + env_cols\n",
    "matrix_f16 = df_num[ordered_cols].to_numpy(dtype=np.float16)\n",
    "\n",
    "# ===== SAVE ARTIFACTS =====\n",
    "np.save(MATRIX_NPY, matrix_f16)\n",
    "\n",
    "# Human-readable key file\n",
    "with CODE_KEY.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Envelope matrix schema (float16), row-wise\\n\")\n",
    "    f.write(\"# Columns (in order):\\n\")\n",
    "    for i, col in enumerate(ordered_cols):\n",
    "        f.write(f\"{i:>5}: {col}\\n\")\n",
    "    f.write(\"\\n# Metadata code maps (string → integer code)\\n\")\n",
    "    for col in meta_cols:\n",
    "        f.write(f\"\\n[{col}]\\n\")\n",
    "        # Sort by numeric code\n",
    "        inv = sorted(((code, name) for name, code in code_maps[col].items()), key=lambda x: x[0])\n",
    "        for code, name in inv:\n",
    "            f.write(f\"{code:>5} : {name}\\n\")\n",
    "    f.write(\"\\nNotes:\\n\")\n",
    "    f.write(\"- Matrix dtype is float16 (16-bit). Metadata codes are stored as float16 numbers in the matrix.\\n\")\n",
    "    f.write(\"- Envelope NaNs (shorter videos) were replaced with 0.0.\\n\")\n",
    "    f.write(\"- Code '0' means UNKNOWN for the metadata fields.\\n\")\n",
    "\n",
    "# Optional: machine-readable mappings\n",
    "with CODES_JSON.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "    json.dump({\"column_order\": ordered_cols, \"code_maps\": code_maps}, jf, indent=2)\n",
    "\n",
    "print(f\"[OK] Saved 16-bit matrix: {MATRIX_NPY}  (shape={matrix_f16.shape}, dtype={matrix_f16.dtype})\")\n",
    "print(f\"[OK] Saved key:           {CODE_KEY}\")\n",
    "print(f\"[OK] Saved JSON maps:     {CODES_JSON}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba9bbe-4757-44af-96c3-861c2bdf4824",
   "metadata": {},
   "source": [
    "## Matrix's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9002e8-37b5-4514-b327-172e9168ae37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Reaction matrices per odor + fly-category counts (During & After)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# ───────── USER KNOB: spacing between rows (increase this to add space)\n",
    "ROW_GAP = 0.6\n",
    "HEIGHT_PER_GAP_IN = 3.0\n",
    "BOTTOM_SHIFT_IN = 0.50\n",
    "\n",
    "# ───────── PARAMETERS ─────────\n",
    "MATRIX_NPY        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/envelope_matrix_float16.npy\")\n",
    "CODES_JSON        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/code_maps.json\")\n",
    "FPS_DEFAULT       = 40\n",
    "BEFORE_SEC        = 30.0\n",
    "DURING_SEC        = 30.0\n",
    "AFTER_WINDOW_SEC  = 30.0\n",
    "THRESH_STD_MULT   = 4\n",
    "MIN_SAMPLES_OVER  = 20\n",
    "\n",
    "ODOR_TRANSIT_LAT_S = overall_mean_latency_s\n",
    "\n",
    "OUT_DIR           = Path(\"/home/ramanlab/Documents/cole/Results/Opto/Matrixs_DIST\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Canon keys for grouping\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"Ethyl Butyrate\": \"EB\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz\",\n",
    "    \"Optogenetics Ethyl Butyrate\": \"opto_EB\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz_1\",\n",
    "}\n",
    "DISPLAY_LABEL = {\n",
    "    \"ACV\": \"ACV\",\n",
    "    \"3-octonol\": \"3-Octonol\",\n",
    "    \"Benz\": \"Benzaldehyde\",\n",
    "    \"10s_Odor_Benz\": \"Benzaldehyde\",\n",
    "    \"EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz\": \"Benzaldehyde\",\n",
    "    \"opto_EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz_1\": \"Benzaldehyde\",\n",
    "}\n",
    "ODOR_ORDER = [\"ACV\", \"3-octonol\", \"Benz\", \"EB\", \"10s_Odor_Benz\", \"opto_benz\", \"opto_EB\", \"opto_benz_1\"]\n",
    "\n",
    "# ───────── LOAD + DECODE ─────────\n",
    "matrix = np.load(MATRIX_NPY)\n",
    "with open(CODES_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "ordered_cols = meta[\"column_order\"]\n",
    "code_maps    = meta[\"code_maps\"]\n",
    "rev_maps     = {c: {v:k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "decode_cols = [c for c in [\"dataset\", \"fly\", \"trial_type\", \"trial_label\"] if c in ordered_cols]\n",
    "meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "for c in decode_cols:\n",
    "    df[c] = df[c].astype(int).map(rev_maps[c]).fillna(\"UNKNOWN\")\n",
    "\n",
    "if \"fps\" in df.columns:\n",
    "    if \"fps\" in rev_maps:\n",
    "        df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "    df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"fps\"] = np.nan\n",
    "\n",
    "df = df[df[\"trial_type\"].str.lower() == \"testing\"].copy()\n",
    "\n",
    "FPS_FALLBACK = FPS_DEFAULT\n",
    "df[\"fps\"] = df[\"fps\"].fillna(FPS_FALLBACK).replace([np.inf, -np.inf], FPS_FALLBACK)\n",
    "\n",
    "env_cols = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "def _canon_odor(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"UNKNOWN\"\n",
    "    return ODOR_CANON.get(s.strip().lower(), s.strip())\n",
    "df[\"dataset_canon\"] = df[\"dataset\"].apply(_canon_odor)\n",
    "\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def display_odor_for_trial(dataset_canon: str, trial_label: str) -> str:\n",
    "    n = _trial_num(trial_label)\n",
    "    if n in (1, 3):  # hexanol controls\n",
    "        return \"Hexanol\"\n",
    "    if n in (2, 4, 5):  # trained odor\n",
    "        return DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "\n",
    "    if dataset_canon == \"ACV\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"3-octonol\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Citral\"\n",
    "        if n == 8: return \"Linalool\"\n",
    "    elif dataset_canon == \"Benz\":\n",
    "        if n == 6: return \"Citral\"\n",
    "        if n == 7: return \"Linalool\"\n",
    "    elif dataset_canon == \"EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"10s_Odor_Benz\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "    elif dataset_canon == \"opto_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz_1\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Ethyl Butyrate\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    return trial_label\n",
    "\n",
    "def score_trial_from_env(env_row: pd.Series, fps: float) -> tuple[int, int]:\n",
    "    env = env_row.to_numpy(dtype=float)\n",
    "    env = env[np.isfinite(env) & (env > 0)]\n",
    "    if env.size == 0:\n",
    "        return (0, 0)\n",
    "    total = env.size\n",
    "    b_end   = int(round(BEFORE_SEC * fps))\n",
    "    shift   = int(round(ODOR_TRANSIT_LAT_S * fps))\n",
    "    d_start = b_end + shift\n",
    "    d_end   = b_end + int(round(DURING_SEC * fps)) + shift\n",
    "    a_end   = d_end + int(round(AFTER_WINDOW_SEC * fps))\n",
    "\n",
    "    b_end   = max(0, min(b_end, total))\n",
    "    d_start = max(b_end, min(d_start, total))\n",
    "    d_end   = max(d_start, min(d_end, total))\n",
    "    a_end   = max(d_end, min(a_end, total))\n",
    "\n",
    "    before = env[:b_end]\n",
    "    during = env[d_start:d_end]\n",
    "    after  = env[d_end:a_end]\n",
    "\n",
    "    if before.size == 0:\n",
    "        return (0, 0)\n",
    "\n",
    "    theta = float(np.nanmean(before)) + THRESH_STD_MULT * float(np.nanstd(before))\n",
    "    during_hit = int(np.sum(during > theta) >= MIN_SAMPLES_OVER) if during.size else 0\n",
    "    after_hit  = int(np.sum(after  > theta) >= MIN_SAMPLES_OVER) if after.size  else 0\n",
    "    return during_hit, after_hit\n",
    "\n",
    "# ───────── Score all rows ─────────\n",
    "scores = []\n",
    "for _, row in df.iterrows():\n",
    "    row_fps = float(row.get(\"fps\", FPS_FALLBACK))\n",
    "    d_hit, a_hit = score_trial_from_env(row[env_cols], row_fps)\n",
    "    scores.append({\n",
    "        \"dataset\": row[\"dataset_canon\"],\n",
    "        \"fly\": row[\"fly\"],\n",
    "        \"trial\": row[\"trial_label\"],\n",
    "        \"trial_num\": _trial_num(row[\"trial_label\"]),\n",
    "        \"during_hit\": d_hit,\n",
    "        \"after_hit\": a_hit\n",
    "    })\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# ───────── Colormaps and helpers ─────────\n",
    "cmap = ListedColormap([\"0.7\", \"1.0\", \"0.0\"])  # gray, white, black\n",
    "norm = BoundaryNorm([-1.5, -0.5, 0.5, 1.5], cmap.N)\n",
    "\n",
    "def style_trained_xticks_vertical(ax, labels, trained_disp: str, fontsize: int):\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90, ha=\"center\", va=\"top\", fontsize=fontsize)\n",
    "    txts = []\n",
    "    for tick in ax.get_xticklabels():\n",
    "        txt = tick.get_text()\n",
    "        if txt.strip().lower() == trained_disp.lower():\n",
    "            tick.set_text(trained_disp.upper())\n",
    "            tick.set_color(\"tab:blue\")\n",
    "        txts.append(tick.get_text())\n",
    "    ax.set_xticklabels(txts, rotation=90, ha=\"center\", va=\"top\", fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"x\", pad=2)\n",
    "\n",
    "def compute_fly_category_counts(mat: np.ndarray, labels: list[str], trained_disp: str, include_hexanol: bool = False):\n",
    "    if mat.size == 0:\n",
    "        return {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "    trained_idx = [j for j, lab in enumerate(labels)\n",
    "                   if lab.strip().lower() == trained_disp.lower()]\n",
    "    other_idx = [j for j, lab in enumerate(labels)\n",
    "                 if lab.strip().lower() != trained_disp.lower()\n",
    "                 and (include_hexanol or lab.strip().lower() != \"hexanol\")]\n",
    "    if len(trained_idx) == 0:\n",
    "        return {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "    counts = {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "    for i in range(mat.shape[0]):\n",
    "        row = mat[i, :]\n",
    "        row = np.where(row < 0, 0, row)\n",
    "        t_hit = np.any(row[trained_idx] == 1)\n",
    "        o_hit = np.any(row[other_idx]   == 1) if len(other_idx) else False\n",
    "        if t_hit and not o_hit:\n",
    "            counts[\"Trained only\"] += 1\n",
    "        elif t_hit and o_hit:\n",
    "            counts[\"Trained + Others\"] += 1\n",
    "        elif (not t_hit) and o_hit:\n",
    "            counts[\"Others only\"] += 1\n",
    "    return counts\n",
    "\n",
    "def plot_category_counts(ax, counts: dict, n_flies: int, title: str):\n",
    "    cats = [\"Trained only\", \"Trained + Others\", \"Others only\"]\n",
    "    raw = np.array([counts.get(c, 0) for c in cats], dtype=float)\n",
    "    vals_pct = 100.0 * raw / float(n_flies) if n_flies > 0 else np.zeros_like(raw)\n",
    "    x = np.arange(len(cats))\n",
    "    bars = ax.bar(x, vals_pct, width=0.75, edgecolor=\"black\", linewidth=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cats, rotation=15, ha=\"right\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.set_ylabel(\"% of flies\")\n",
    "    ax.set_title(title, fontsize=12, weight=\"bold\")\n",
    "    ax.margins(x=0.05)\n",
    "    for b, pct in zip(bars, vals_pct):\n",
    "        ax.text(b.get_x() + b.get_width()/2, b.get_height() + 1.5, f\"{pct:.0f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "def shade_latency_on_timeseries(ax, before_sec: float = BEFORE_SEC, latency_s: float = ODOR_TRANSIT_LAT_S):\n",
    "    x0 = before_sec\n",
    "    x1 = before_sec + latency_s\n",
    "    ax.axvspan(x0, x1, color=\"red\", alpha=0.30, lw=0)\n",
    "\n",
    "# ──────── helper: safe dir name\n",
    "def _safe_dirname(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', str(s)).strip('_')\n",
    "\n",
    "# ───────── Build & save per-odor figures (per-odor subfolders) ─────────\n",
    "present = scores_df[\"dataset\"].unique().tolist()\n",
    "ordered_present = [o for o in ODOR_ORDER if o in present]\n",
    "extras = sorted([o for o in present if o not in ODOR_ORDER])\n",
    "for odor in ordered_present + extras:\n",
    "    sub = scores_df[scores_df[\"dataset\"] == odor].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"[WARN] No testing trials for {odor}\")\n",
    "        continue\n",
    "\n",
    "    # per-odor output directory\n",
    "    odir = OUT_DIR / _safe_dirname(odor)\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    flies  = sorted(sub[\"fly\"].unique())\n",
    "    trials = sorted(sub[\"trial\"].unique(), key=_trial_num)\n",
    "    pretty_cols = [display_odor_for_trial(odor, t) for t in trials]\n",
    "\n",
    "    D = -np.ones((len(flies), len(trials)), dtype=int)\n",
    "    A = -np.ones((len(flies), len(trials)), dtype=int)\n",
    "    for i, fly in enumerate(flies):\n",
    "        fly_rows = sub[sub[\"fly\"] == fly]\n",
    "        for j, t in enumerate(trials):\n",
    "            s = fly_rows[fly_rows[\"trial\"] == t]\n",
    "            if s.empty: continue\n",
    "            D[i, j] = int(s[\"during_hit\"].iloc[0])\n",
    "            A[i, j] = int(s[\"after_hit\"].iloc[0])\n",
    "\n",
    "    odor_label   = DISPLAY_LABEL.get(odor, odor)\n",
    "    trained_disp = DISPLAY_LABEL.get(odor, odor)\n",
    "    n_flies = len(flies)\n",
    "    n_trials = len(trials)\n",
    "\n",
    "    base_fig_w = max(10.0, 0.70 * n_trials + 6.0)\n",
    "    base_fig_h = max(5.0, n_flies * 0.26 + 3.8)\n",
    "    fig_w = base_fig_w\n",
    "    fig_h = base_fig_h + ROW_GAP * HEIGHT_PER_GAP_IN\n",
    "    fig_h += BOTTOM_SHIFT_IN\n",
    "    xtick_fs = 9 if n_trials <= 10 else (8 if n_trials <= 16 else 7)\n",
    "\n",
    "    during_counts = compute_fly_category_counts(D, pretty_cols, trained_disp, include_hexanol=True)\n",
    "    after_counts  = compute_fly_category_counts(A, pretty_cols, trained_disp, include_hexanol=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_w, fig_h), constrained_layout=False)\n",
    "    gs  = gridspec.GridSpec(2, 2, height_ratios=[3.0, 1.25], width_ratios=[1, 1], hspace=ROW_GAP, wspace=0.10)\n",
    "\n",
    "    axD  = fig.add_subplot(gs[0, 0])\n",
    "    axA  = fig.add_subplot(gs[0, 1])\n",
    "    axDc = fig.add_subplot(gs[1, 0])\n",
    "    axAc = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    imD = axD.imshow(D, cmap=cmap, norm=norm, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axD.set_title(f\"{odor_label} — During\\n(DURING shifted by +{ODOR_TRANSIT_LAT_S:.2f} s)\", fontsize=14, weight=\"bold\", linespacing=1.1)\n",
    "    style_trained_xticks_vertical(axD, pretty_cols, trained_disp, fontsize=xtick_fs)\n",
    "    axD.set_yticks([]); axD.set_ylabel(f\"{n_flies} Flies\", fontsize=11)\n",
    "\n",
    "    imA = axA.imshow(A, cmap=cmap, norm=norm, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axA.set_title(f\"{odor_label} — After (first {int(AFTER_WINDOW_SEC)} s)\", fontsize=14, weight=\"bold\")\n",
    "    style_trained_xticks_vertical(axA, pretty_cols, trained_disp, fontsize=xtick_fs)\n",
    "    axA.set_yticks([]); axA.set_ylabel(f\"{n_flies} Flies\", fontsize=11)\n",
    "\n",
    "    plot_category_counts(axDc, during_counts, n_flies, title=\"During — Fly Reaction Categories\")\n",
    "    plot_category_counts(axAc, after_counts,  n_flies, title=f\"After (first {int(AFTER_WINDOW_SEC)} s) — Fly Reaction Categories\")\n",
    "\n",
    "    red_patch = Patch(facecolor=\"red\", edgecolor=\"red\", alpha=0.30, label=f\"Odor transit {ODOR_TRANSIT_LAT_S:.2f} s (pre-DURING)\")\n",
    "    axD.legend(handles=[red_patch], loc=\"upper left\", frameon=True, fontsize=9)\n",
    "\n",
    "    shift_frac = BOTTOM_SHIFT_IN / fig_h\n",
    "    for ax in (axDc, axAc):\n",
    "        pos = ax.get_position()\n",
    "        new_y0 = max(0.05, pos.y0 - shift_frac)\n",
    "        ax.set_position([pos.x0, new_y0, pos.width, pos.height])\n",
    "\n",
    "    # Save into per-odor folder\n",
    "    out_png = odir / f\"reaction_matrix_{odor.replace(' ', '_')}_{AFTER_WINDOW_SEC}_latency_{ODOR_TRANSIT_LAT_S:.3f}s.png\"\n",
    "    fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] saved {out_png}\")\n",
    "\n",
    "    key_path = odir / f\"row_key_{odor.replace(' ', '_')}_{AFTER_WINDOW_SEC}.txt\"\n",
    "    with key_path.open(\"w\") as fh:\n",
    "        for i, fly in enumerate(flies):\n",
    "            fh.write(f\"Row {i}: {fly}\\n\")\n",
    "    print(f\"[OK] saved {key_path}\")\n",
    "\n",
    "print(\"[DONE] Per-odor exports saved into subfolders under OUT_DIR.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1cd35-47e4-4dd9-b15c-1f4aa456bb32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Matrix's not in testing trial order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c035b7-abe6-43b5-959c-213ff644b63d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Reaction matrices per odor + fly-category counts (During & After)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# ───────── USER KNOB: spacing between rows (increase this to add space)\n",
    "ROW_GAP = 0.6            # try 0.10 … 0.60 (higher = more space between rows)\n",
    "HEIGHT_PER_GAP_IN = 3.0  # how many inches of figure height to add per 1.0 ROW_GAP\n",
    "BOTTOM_SHIFT_IN = 0.50   # inches to lower the bottom row; increase to move further down\n",
    "\n",
    "# ───────── PARAMETERS ─────────\n",
    "MATRIX_NPY        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/envelope_matrix_float16.npy\")\n",
    "CODES_JSON        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/code_maps.json\")\n",
    "FPS_DEFAULT       = 40\n",
    "BEFORE_SEC        = 30.0\n",
    "DURING_SEC        = 30.0\n",
    "AFTER_WINDOW_SEC  = 30.0\n",
    "THRESH_STD_MULT   = 4\n",
    "MIN_SAMPLES_OVER  = 20\n",
    "\n",
    "# Shift DURING window by latency at both start and end:\n",
    "# e.g., DURING [30,60] → [30+lat, 60+lat]\n",
    "ODOR_TRANSIT_LAT_S = overall_mean_latency_s\n",
    "\n",
    "OUT_DIR           = Path(\"/home/ramanlab/Documents/cole/Results/Opto/Matrixs_DIST\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Canon keys for grouping\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"Ethyl Butyrate\": \"EB\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz_1\",\n",
    "    \"Optogenetics Ethyl Butyrate\": \"opto_EB\",\n",
    "}\n",
    "DISPLAY_LABEL = {\n",
    "    \"ACV\": \"ACV\",\n",
    "    \"3-octonol\": \"3-Octonol\",\n",
    "    \"Benz\": \"Benzaldehyde\",\n",
    "    \"10s_Odor_Benz\": \"Benzaldehyde\",\n",
    "    \"EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz\": \"Benzaldehyde\",\n",
    "    \"opto_EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz_1\": \"Benzaldehyde\",\n",
    "}\n",
    "ODOR_ORDER = [\"ACV\", \"3-octonol\", \"Benz\", \"EB\", \"10s_Odor_Benz\", \"opto_benz\", \"opto_EB\", \"opto_benz_1\"]\n",
    "\n",
    "# ───────── LOAD + DECODE ─────────\n",
    "matrix = np.load(MATRIX_NPY)\n",
    "with open(CODES_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "ordered_cols = meta[\"column_order\"]\n",
    "code_maps    = meta[\"code_maps\"]\n",
    "rev_maps     = {c: {v:k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "decode_cols = [c for c in [\"dataset\", \"fly\", \"trial_type\", \"trial_label\"] if c in ordered_cols]\n",
    "meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "# decode label-coded columns\n",
    "for c in decode_cols:\n",
    "    df[c] = df[c].astype(int).map(rev_maps[c]).fillna(\"UNKNOWN\")\n",
    "\n",
    "# ensure fps numeric\n",
    "if \"fps\" in df.columns:\n",
    "    if \"fps\" in rev_maps:\n",
    "        df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "    df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"fps\"] = np.nan\n",
    "\n",
    "# testing only\n",
    "df = df[df[\"trial_type\"].str.lower() == \"testing\"].copy()\n",
    "\n",
    "# fill missing fps\n",
    "FPS_FALLBACK = FPS_DEFAULT\n",
    "df[\"fps\"] = df[\"fps\"].fillna(FPS_FALLBACK).replace([np.inf, -np.inf], FPS_FALLBACK)\n",
    "\n",
    "# envelope columns exclude meta\n",
    "env_cols = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "def _canon_odor(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"UNKNOWN\"\n",
    "    return ODOR_CANON.get(s.strip().lower(), s.strip())\n",
    "df[\"dataset_canon\"] = df[\"dataset\"].apply(_canon_odor)\n",
    "\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "# ───────── Custom trial→display-odor mapping per dataset ─────────\n",
    "def display_odor_for_trial(dataset_canon: str, trial_label: str) -> str:\n",
    "    n = _trial_num(trial_label)\n",
    "    if n in (1, 3):  # hexanol controls\n",
    "        return \"Hexanol\"\n",
    "    if n in (2, 4, 5):  # trained odor\n",
    "        return DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "\n",
    "    if dataset_canon == \"ACV\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"3-octonol\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Citral\"\n",
    "        if n == 8: return \"Linalool\"\n",
    "    elif dataset_canon == \"Benz\":\n",
    "        if n == 6: return \"Citral\"\n",
    "        if n == 7: return \"Linalool\"\n",
    "    elif dataset_canon == \"EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"10s_Odor_Benz\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "    elif dataset_canon == \"opto_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz_1\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Ethyl Butyrate\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    return trial_label\n",
    "\n",
    "# ───────── Scoring on envelope row ─────────\n",
    "def score_trial_from_env(env_row: pd.Series, fps: float) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Compute During/After hits using a baseline from BEFORE.\n",
    "    DURING is fully shifted by ODOR_TRANSIT_LAT_S at start and end:\n",
    "      DURING: [BEFORE_SEC + ODOR_TRANSIT_LAT_S, BEFORE_SEC + DURING_SEC + ODOR_TRANSIT_LAT_S]\n",
    "      AFTER:  the next AFTER_WINDOW_SEC immediately following DURING.\n",
    "    \"\"\"\n",
    "    env = env_row.to_numpy(dtype=float)\n",
    "    env = env[np.isfinite(env) & (env > 0)]\n",
    "    if env.size == 0:\n",
    "        return (0, 0)\n",
    "\n",
    "    total = env.size\n",
    "\n",
    "    # Indices (in samples)\n",
    "    b_end   = int(round(BEFORE_SEC * fps))  # end of BEFORE\n",
    "    shift   = int(round(ODOR_TRANSIT_LAT_S * fps))\n",
    "    d_start = b_end + shift\n",
    "    d_end   = b_end + int(round(DURING_SEC * fps)) + shift\n",
    "    a_end   = d_end + int(round(AFTER_WINDOW_SEC * fps))\n",
    "\n",
    "    # Clip/guard\n",
    "    b_end   = max(0, min(b_end, total))\n",
    "    d_start = max(b_end, min(d_start, total))\n",
    "    d_end   = max(d_start, min(d_end, total))\n",
    "    a_end   = max(d_end, min(a_end, total))\n",
    "\n",
    "    # Windows\n",
    "    before = env[:b_end]\n",
    "    during = env[d_start:d_end]   # fully shifted DURING window\n",
    "    after  = env[d_end:a_end]\n",
    "\n",
    "    if before.size == 0:\n",
    "        return (0, 0)\n",
    "\n",
    "    # Threshold from BEFORE baseline\n",
    "    theta = float(np.nanmean(before)) + THRESH_STD_MULT * float(np.nanstd(before))\n",
    "\n",
    "    # Hits (require at least MIN_SAMPLES_OVER above theta)\n",
    "    during_hit = int(np.sum(during > theta) >= MIN_SAMPLES_OVER) if during.size else 0\n",
    "    after_hit  = int(np.sum(after  > theta) >= MIN_SAMPLES_OVER) if after.size  else 0\n",
    "    return during_hit, after_hit\n",
    "\n",
    "# ───────── Score all rows ─────────\n",
    "scores = []\n",
    "for _, row in df.iterrows():\n",
    "    row_fps = float(row.get(\"fps\", FPS_FALLBACK))\n",
    "    d_hit, a_hit = score_trial_from_env(row[env_cols], row_fps)\n",
    "    scores.append({\n",
    "        \"dataset\": row[\"dataset_canon\"],\n",
    "        \"fly\": row[\"fly\"],\n",
    "        \"trial\": row[\"trial_label\"],\n",
    "        \"trial_num\": _trial_num(row[\"trial_label\"]),\n",
    "        \"during_hit\": d_hit,\n",
    "        \"after_hit\": a_hit\n",
    "    })\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# ───────── Colormaps and helpers ─────────\n",
    "cmap = ListedColormap([\"0.7\", \"1.0\", \"0.0\"])  # gray, white, black\n",
    "norm = BoundaryNorm([-1.5, -0.5, 0.5, 1.5], cmap.N)\n",
    "\n",
    "def style_trained_xticks_vertical(ax, labels, trained_disp: str, fontsize: int):\n",
    "    \"\"\"Vertical x labels; trained odor BLUE + UPPERCASE.\"\"\"\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90, ha=\"center\", va=\"top\", fontsize=fontsize)\n",
    "    txts = []\n",
    "    for tick in ax.get_xticklabels():\n",
    "        txt = tick.get_text()\n",
    "        if txt.strip().lower() == trained_disp.lower():\n",
    "            tick.set_text(trained_disp.upper())\n",
    "            tick.set_color(\"tab:blue\")\n",
    "        txts.append(tick.get_text())\n",
    "    ax.set_xticklabels(txts, rotation=90, ha=\"center\", va=\"top\", fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"x\", pad=2)\n",
    "\n",
    "def compute_fly_category_counts(mat: np.ndarray, labels: list[str], trained_disp: str, include_hexanol: bool = False):\n",
    "    if mat.size == 0:\n",
    "        return {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "\n",
    "    trained_idx = [j for j, lab in enumerate(labels)\n",
    "                   if lab.strip().lower() == trained_disp.lower()]\n",
    "\n",
    "    other_idx = [j for j, lab in enumerate(labels)\n",
    "                 if lab.strip().lower() != trained_disp.lower()\n",
    "                 and (include_hexanol or lab.strip().lower() != \"hexanol\")]\n",
    "    if len(trained_idx) == 0:\n",
    "        return {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "\n",
    "    counts = {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "    for i in range(mat.shape[0]):\n",
    "        row = mat[i, :]\n",
    "        row = np.where(row < 0, 0, row)  # treat missing (-1) as 0 for categorization\n",
    "        t_hit = np.any(row[trained_idx] == 1)\n",
    "        o_hit = np.any(row[other_idx]   == 1) if len(other_idx) else False\n",
    "\n",
    "        if t_hit and not o_hit:\n",
    "            counts[\"Trained only\"] += 1\n",
    "        elif t_hit and o_hit:\n",
    "            counts[\"Trained + Others\"] += 1\n",
    "        elif (not t_hit) and o_hit:\n",
    "            counts[\"Others only\"] += 1\n",
    "    return counts\n",
    "\n",
    "def plot_category_counts(ax, counts: dict, n_flies: int, title: str):\n",
    "    cats = [\"Trained only\", \"Trained + Others\", \"Others only\"]\n",
    "    raw = np.array([counts.get(c, 0) for c in cats], dtype=float)\n",
    "    vals_pct = 100.0 * raw / float(n_flies) if n_flies > 0 else np.zeros_like(raw)\n",
    "\n",
    "    x = np.arange(len(cats))\n",
    "    bars = ax.bar(x, vals_pct, width=0.75, edgecolor=\"black\", linewidth=0.8)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cats, rotation=15, ha=\"right\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.set_ylabel(\"% of flies\")\n",
    "    ax.set_title(title, fontsize=12, weight=\"bold\")\n",
    "    ax.margins(x=0.05)\n",
    "\n",
    "    for b, pct in zip(bars, vals_pct):\n",
    "        ax.text(b.get_x() + b.get_width()/2,\n",
    "                b.get_height() + 1.5,\n",
    "                f\"{pct:.0f}%\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "# ─────── helper: safe dir name\n",
    "def _safe_dirname(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', str(s)).strip('_')\n",
    "\n",
    "# ───────── Build & save per-odor figures (into per-odor subfolders) ─────────\n",
    "# Only iterate over odors present; preserve preferred order, then extras\n",
    "present = scores_df[\"dataset\"].unique().tolist()\n",
    "ordered_present = [o for o in ODOR_ORDER if o in present]\n",
    "extras = sorted([o for o in present if o not in ODOR_ORDER])\n",
    "\n",
    "for odor in ordered_present + extras:\n",
    "    sub = scores_df[scores_df[\"dataset\"] == odor].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"[WARN] No testing trials for {odor}\")\n",
    "        continue\n",
    "\n",
    "    # per-odor output directory\n",
    "    odir = OUT_DIR / _safe_dirname(odor)\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    flies  = sorted(sub[\"fly\"].unique())\n",
    "    # Build trial order: trained odor first (2,4,5), then 1,3,6,7,8,9\n",
    "    existing_trials = list(sub[\"trial\"].unique())\n",
    "\n",
    "    def _tnum(lbl):\n",
    "        m = re.search(r\"(\\d+)\", str(lbl))\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    desired_order = [2, 4, 5, 1, 3, 6, 7, 8, 9]\n",
    "\n",
    "    by_num = {}\n",
    "    for t in existing_trials:\n",
    "        n = _tnum(t)\n",
    "        if n not in by_num:\n",
    "            by_num[n] = t\n",
    "\n",
    "    ordered_trials = [by_num[n] for n in desired_order if n in by_num]\n",
    "    leftovers = sorted([n for n in by_num.keys() if n not in set(desired_order) and n >= 0])\n",
    "    ordered_trials += [by_num[n] for n in leftovers]\n",
    "\n",
    "    trials = ordered_trials\n",
    "    pretty_cols = [display_odor_for_trial(odor, t) for t in trials]\n",
    "\n",
    "    # Matrices with sentinel -1 for missing, else 0/1\n",
    "    D = -np.ones((len(flies), len(trials)), dtype=int)\n",
    "    A = -np.ones((len(flies), len(trials)), dtype=int)\n",
    "    for i, fly in enumerate(flies):\n",
    "        fly_rows = sub[sub[\"fly\"] == fly]\n",
    "        for j, t in enumerate(trials):\n",
    "            s = fly_rows[fly_rows[\"trial\"] == t]\n",
    "            if s.empty: continue\n",
    "            D[i, j] = int(s[\"during_hit\"].iloc[0])\n",
    "            A[i, j] = int(s[\"after_hit\"].iloc[0])\n",
    "\n",
    "    odor_label   = DISPLAY_LABEL.get(odor, odor)\n",
    "    trained_disp = DISPLAY_LABEL.get(odor, odor)\n",
    "    n_flies = len(flies)\n",
    "    n_trials = len(trials)\n",
    "\n",
    "    # Figure size (height grows with flies and with ROW_GAP)\n",
    "    base_fig_w = max(10.0, 0.70 * n_trials + 6.0)\n",
    "    base_fig_h = max(5.0, n_flies * 0.26 + 3.8)\n",
    "    fig_w = base_fig_w\n",
    "    fig_h = base_fig_h + ROW_GAP * HEIGHT_PER_GAP_IN\n",
    "    fig_h += BOTTOM_SHIFT_IN   # keep layout comfortable while lowering bottom row\n",
    "\n",
    "    xtick_fs = 9 if n_trials <= 10 else (8 if n_trials <= 16 else 7)\n",
    "\n",
    "    # NEW: Compute fly-category counts for During & After\n",
    "    during_counts = compute_fly_category_counts(D, pretty_cols, trained_disp, include_hexanol=True)\n",
    "    after_counts  = compute_fly_category_counts(A, pretty_cols, trained_disp, include_hexanol=True)\n",
    "\n",
    "    # Create figure (manual layout)\n",
    "    fig = plt.figure(figsize=(fig_w, fig_h), constrained_layout=False)\n",
    "    gs  = gridspec.GridSpec(\n",
    "        2, 2,\n",
    "        height_ratios=[3.0, 1.25],\n",
    "        width_ratios=[1, 1],\n",
    "        hspace=ROW_GAP,\n",
    "        wspace=0.10\n",
    "    )\n",
    "\n",
    "    axD  = fig.add_subplot(gs[0, 0])   # top-left  (During matrix)\n",
    "    axA  = fig.add_subplot(gs[0, 1])   # top-right (After matrix)\n",
    "    axDc = fig.add_subplot(gs[1, 0])   # bottom-left  (During categories)\n",
    "    axAc = fig.add_subplot(gs[1, 1])   # bottom-right (After categories)\n",
    "\n",
    "    # Top: matrices — vertical x labels, trained odor in blue\n",
    "    imD = axD.imshow(D, cmap=cmap, norm=norm, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axD.set_title(\n",
    "        f\"{odor_label} — During (shifted +{ODOR_TRANSIT_LAT_S:.2f}s)\",\n",
    "        fontsize=14, weight=\"bold\"\n",
    "    )\n",
    "    style_trained_xticks_vertical(axD, pretty_cols, trained_disp, fontsize=xtick_fs)\n",
    "    axD.set_yticks([]); axD.set_ylabel(f\"{n_flies} Flies\", fontsize=11)\n",
    "\n",
    "    imA = axA.imshow(A, cmap=cmap, norm=norm, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axA.set_title(f\"{odor_label} — After (first {int(AFTER_WINDOW_SEC)} s)\", fontsize=14, weight=\"bold\")\n",
    "    style_trained_xticks_vertical(axA, pretty_cols, trained_disp, fontsize=xtick_fs)\n",
    "    axA.set_yticks([]); axA.set_ylabel(f\"{n_flies} Flies\", fontsize=11)\n",
    "\n",
    "    # Bottom: category count bars\n",
    "    plot_category_counts(axDc, during_counts, n_flies, title=\"During — Fly Reaction Categories\")\n",
    "    plot_category_counts(axAc, after_counts,  n_flies, title=f\"After (first {int(AFTER_WINDOW_SEC)} s) — Fly Reaction Categories\")\n",
    "\n",
    "    # Lower the bottom row by BOTTOM_SHIFT_IN (inches)\n",
    "    shift_frac = BOTTOM_SHIFT_IN / fig_h\n",
    "    for ax in (axDc, axAc):\n",
    "        pos = ax.get_position()\n",
    "        new_y0 = max(0.05, pos.y0 - shift_frac)\n",
    "        ax.set_position([pos.x0, new_y0, pos.width, pos.height])\n",
    "\n",
    "    # Save — into per-odor folder\n",
    "    out_png = odir / f\"reaction_matrix_{odor.replace(' ', '_')}_{AFTER_WINDOW_SEC}_latency_{ODOR_TRANSIT_LAT_S:.3f}s_unordered.png\"\n",
    "    fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] saved {out_png}\")\n",
    "\n",
    "    # Row index → fly key — into per-odor folder\n",
    "    key_path = odir / f\"row_key_{odor.replace(' ', '_')}_{AFTER_WINDOW_SEC}.txt\"\n",
    "    with key_path.open(\"w\") as fh:\n",
    "        for i, fly in enumerate(flies):\n",
    "            fh.write(f\"Row {i}: {fly}\\n\")\n",
    "    print(f\"[OK] saved {key_path}\")\n",
    "\n",
    "    # ───────── CSV per odor with actual odor names — into per-odor folder ─────────\n",
    "    sub_for_csv = sub.copy()\n",
    "    sub_for_csv[\"odor_sent\"] = sub_for_csv[\"trial\"].apply(lambda t: display_odor_for_trial(odor, t))\n",
    "    order_map = {t: i for i, t in enumerate(trials)}\n",
    "    sub_for_csv[\"trial_ord\"] = sub_for_csv[\"trial\"].map(order_map).fillna(10**9).astype(int)\n",
    "    sub_for_csv = sub_for_csv.sort_values([\"fly\", \"trial_ord\", \"trial_num\", \"trial\"])\n",
    "\n",
    "    export_cols = [\"dataset\", \"fly\", \"trial_num\", \"odor_sent\", \"during_hit\", \"after_hit\"]\n",
    "    out_csv = odir / f\"binary_reactions_{odor.replace(' ', '_')}.csv\"\n",
    "    sub_for_csv[export_cols].to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] saved {out_csv}\")\n",
    "\n",
    "print(\"[DONE] Per-odor exports saved into subfolders under OUT_DIR.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cef96d-3a69-4862-9e0a-a8a612b500c2",
   "metadata": {},
   "source": [
    "## Envople"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855d13f-84f8-4f80-b33a-8e96a97072c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Per-fly envelope plots from MATRIX with trained-odor styling\n",
    "# (after-period limited to 30 s) + per-trial threshold line\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= PARAMETERS =========\n",
    "MATRIX_NPY        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/envelope_matrix_float16.npy\")\n",
    "CODES_JSON        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/code_maps.json\")\n",
    "\n",
    "FPS_DEFAULT       = 40.0       # fallback if fps missing/invalid\n",
    "ODOR_ON_S         = 30.0\n",
    "ODOR_OFF_S        = 60.0\n",
    "AFTER_SHOW_S      = 30.0       # show only first 30 s after odor OFF\n",
    "\n",
    "# Threshold params — matches your scoring code design (baseline is [0, ODOR_ON_S))\n",
    "THRESH_STD_MULT   = 4.0        # θ = μ_before + k·σ\n",
    "\n",
    "# Odor transit latency (mean time for plume to reach the fly)\n",
    "ODOR_TRANSIT_LAT_S = overall_mean_latency_s\n",
    "\n",
    "# IMPORTANT: extend visible window so AFTER is measured from shifted OFF\n",
    "X_MAX_LIMIT       = ODOR_OFF_S + ODOR_TRANSIT_LAT_S + AFTER_SHOW_S\n",
    "\n",
    "OUT_DIR = Path(\"/home/ramanlab/Documents/cole/Results/Opto/Envlope_DIST\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= LOAD MATRIX + METADATA =========\n",
    "matrix = np.load(MATRIX_NPY, allow_pickle=False)\n",
    "with open(CODES_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ordered_cols = meta[\"column_order\"]\n",
    "code_maps    = meta[\"code_maps\"]\n",
    "rev_maps     = {c: {v:k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "decode_cols = [c for c in [\"dataset\",\"fly\",\"trial_type\",\"trial_label\"] if c in ordered_cols]\n",
    "meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "\n",
    "df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "# Decode labels -> strings\n",
    "for c in decode_cols:\n",
    "    df[c] = df[c].astype(int).map(rev_maps[c]).fillna(\"UNKNOWN\")\n",
    "\n",
    "# Ensure fps exists and is numeric\n",
    "if \"fps\" in df.columns:\n",
    "    if \"fps\" in rev_maps:  # if coded (rare)\n",
    "        df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "    df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"fps\"] = np.nan\n",
    "\n",
    "# Keep only testing trials\n",
    "df = df[df[\"trial_type\"].str.lower()==\"testing\"].copy()\n",
    "\n",
    "# Fill missing/invalid fps with fallback\n",
    "df[\"fps\"] = df[\"fps\"].replace([np.inf, -np.inf], np.nan).fillna(FPS_DEFAULT)\n",
    "\n",
    "# env_* columns exclude meta (including fps)\n",
    "env_cols  = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "# ========= Canon keys & display names =========\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"Ethyl Butyrate\": \"EB\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz_1\",\n",
    "    \"Optogenetics Ethyl Butyrate\": \"opto_EB\",\n",
    "}\n",
    "DISPLAY_LABEL = {\n",
    "    \"ACV\": \"ACV\",\n",
    "    \"3-octonol\": \"3-Octonol\",\n",
    "    \"Benz\": \"Benzaldehyde\",\n",
    "    \"10s_Odor_Benz\": \"Benzaldehyde\",\n",
    "    \"EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz\": \"Benzaldehyde\",\n",
    "    \"opto_benz_1\": \"Benzaldehyde\",\n",
    "    \"opto_EB\": \"Ethyl Butyrate\",\n",
    "}\n",
    "\n",
    "def _canon_dataset(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"UNKNOWN\"\n",
    "    return ODOR_CANON.get(s.strip().lower(), s.strip())\n",
    "\n",
    "df[\"dataset_canon\"] = df[\"dataset\"].apply(_canon_dataset)\n",
    "\n",
    "# helper: safe dir name\n",
    "def _safe_dirname(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', str(s)).strip('_')\n",
    "\n",
    "# ========= Helpers =========\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def display_odor_for_trial(dataset_canon: str, trial_label: str) -> str:\n",
    "    n = _trial_num(trial_label)\n",
    "    if n in (1, 3):  # controls\n",
    "        return \"Hexanol\"\n",
    "    if n in (2, 4, 5):  # trained odor\n",
    "        return DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "\n",
    "    if dataset_canon == \"ACV\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"3-octonol\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Citral\"\n",
    "        if n == 8: return \"Linalool\"\n",
    "    elif dataset_canon == \"Benz\":\n",
    "        if n == 6: return \"Citral\"\n",
    "        if n == 7: return \"Linalool\"\n",
    "    elif dataset_canon == \"EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"10s_Odor_Benz\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "    elif dataset_canon == \"opto_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz_1\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Ethyl Butyrate\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    return trial_label\n",
    "\n",
    "def _extract_env(row: pd.Series) -> np.ndarray:\n",
    "    env = row[env_cols].to_numpy(dtype=float)\n",
    "    env = env[np.isfinite(env) & (env > 0)]\n",
    "    return env\n",
    "\n",
    "def _compute_theta(env_full: np.ndarray, fps: float) -> float:\n",
    "    \"\"\"θ = mean(before) + k*std(before), where before = [0, ODOR_ON_S).\"\"\"\n",
    "    if env_full.size == 0 or fps <= 0:\n",
    "        return np.nan\n",
    "    b_end = int(ODOR_ON_S * fps)\n",
    "    b_end = min(b_end, env_full.size)\n",
    "    before = env_full[:b_end]\n",
    "    if before.size == 0:\n",
    "        return np.nan\n",
    "    mu = float(np.nanmean(before))\n",
    "    sd = float(np.nanstd(before))\n",
    "    return mu + THRESH_STD_MULT * sd\n",
    "\n",
    "def _is_trained_odor(dataset_canon: str, odor_name: str) -> bool:\n",
    "    trained = DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "    return str(odor_name).strip().lower() == str(trained).strip().lower()\n",
    "\n",
    "def style_trained_title(ax, odor_label: str):\n",
    "    ax.set_title(\n",
    "        odor_label.upper(),\n",
    "        loc=\"left\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "        pad=2,\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "\n",
    "# ========= MAKE FIGURES PER FLY =========\n",
    "for fly, g in df.groupby(\"fly\"):\n",
    "    g = g.sort_values(\"trial_label\", key=lambda s: s.map(_trial_num))\n",
    "    dataset_canon = _canon_dataset(g[\"dataset\"].iloc[0])\n",
    "\n",
    "    # (odor_name, t_visible, env_visible, theta, is_trained)\n",
    "    trial_curves = []\n",
    "    y_max = 0.0\n",
    "\n",
    "    for _, row in g.iterrows():\n",
    "        env_full = _extract_env(row)\n",
    "        if env_full.size == 0:\n",
    "            continue\n",
    "\n",
    "        row_fps = float(row.get(\"fps\", FPS_DEFAULT)) if np.isfinite(row.get(\"fps\", np.nan)) else FPS_DEFAULT\n",
    "        t_full = np.arange(env_full.size, dtype=float) / max(row_fps, 1e-9)\n",
    "\n",
    "        theta = _compute_theta(env_full, row_fps)\n",
    "\n",
    "        # Clip to [0, X_MAX_LIMIT] for visualization\n",
    "        mask = (t_full <= X_MAX_LIMIT + 1e-9)\n",
    "        t = t_full[mask]\n",
    "        env = env_full[mask]\n",
    "        if t.size == 0:\n",
    "            continue\n",
    "\n",
    "        odor_name = display_odor_for_trial(dataset_canon, row[\"trial_label\"])\n",
    "        trial_curves.append((odor_name, t, env, theta, _is_trained_odor(dataset_canon, odor_name)))\n",
    "\n",
    "        local_max = np.nanmax(env) if np.isfinite(env).any() else 0.0\n",
    "        if np.isfinite(theta):\n",
    "            local_max = max(local_max, theta)\n",
    "        y_max = max(y_max, float(local_max))\n",
    "\n",
    "    if not trial_curves:\n",
    "        print(f\"[WARN] {fly}: no usable testing trials; skipping.\")\n",
    "        continue\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        \"figure.dpi\": 300, \"savefig.dpi\": 300,\n",
    "        \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "        \"axes.linewidth\": 0.8, \"xtick.direction\": \"out\", \"ytick.direction\": \"out\",\n",
    "        \"font.size\": 10,\n",
    "    })\n",
    "\n",
    "    n = len(trial_curves)\n",
    "    fig_h = max(3.0, n * 1.6 + 1.5)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(10, fig_h), sharex=True)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (odor_name, t, env, theta, is_trained) in zip(axes, trial_curves):\n",
    "        ax.plot(t, env, linewidth=1.2, color='black')\n",
    "\n",
    "        # Nominal valve timing markers (hardware command times)\n",
    "        ax.axvline(ODOR_ON_S,  linestyle='--', linewidth=1.0, color='black')\n",
    "        ax.axvline(ODOR_OFF_S, linestyle='--', linewidth=1.0, color='black')\n",
    "\n",
    "        # Effective plume windows using latency:\n",
    "        on_lat_end   = min(ODOR_ON_S  + ODOR_TRANSIT_LAT_S, X_MAX_LIMIT)\n",
    "        off_lat_end  = min(ODOR_OFF_S + ODOR_TRANSIT_LAT_S, X_MAX_LIMIT)\n",
    "        eff_on_start = min(on_lat_end, X_MAX_LIMIT)\n",
    "        eff_on_end   = min(off_lat_end, X_MAX_LIMIT)\n",
    "\n",
    "        # Shade start latency (red) and effective ON (gray)\n",
    "        if ODOR_TRANSIT_LAT_S > 0:\n",
    "            ax.axvspan(ODOR_ON_S, on_lat_end, alpha=0.25, color='red')\n",
    "        if eff_on_end > eff_on_start:\n",
    "            ax.axvspan(eff_on_start, eff_on_end, alpha=0.15, color='gray')\n",
    "\n",
    "        # Shade end latency (red)\n",
    "        if ODOR_TRANSIT_LAT_S > 0:\n",
    "            ax.axvspan(ODOR_OFF_S, off_lat_end, alpha=0.25, color='red')\n",
    "\n",
    "        # Threshold line\n",
    "        if np.isfinite(theta):\n",
    "            ax.axhline(theta, linestyle='-', linewidth=1.0, color='tab:red', alpha=0.9)\n",
    "\n",
    "        ax.set_ylim(0, y_max * 1.02 if y_max > 0 else 1.0)\n",
    "        ax.set_xlim(0, X_MAX_LIMIT)\n",
    "        ax.margins(x=0, y=0.02)\n",
    "        ax.set_ylabel(\"RMS (a.u.)\", fontsize=10)\n",
    "\n",
    "        if is_trained:\n",
    "            style_trained_title(ax, odor_name)\n",
    "        else:\n",
    "            ax.set_title(odor_name, loc=\"left\", fontsize=11, weight=\"bold\", pad=2, color=\"black\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\", fontsize=11)\n",
    "\n",
    "    # SINGLE legend on the figure\n",
    "    on_handle      = plt.Line2D([0], [0], linestyle='--', linewidth=1.0, color='black', label='Valve on/off (command)')\n",
    "    transit_handle = plt.Rectangle((0,0), 1, 1, alpha=0.25, color='red',  label=f'Odor transit (~{ODOR_TRANSIT_LAT_S:.2f}s)')\n",
    "    span_handle    = plt.Rectangle((0,0), 1, 1, alpha=0.15, color='gray', label='Effective odor-on at fly')\n",
    "    theta_handle   = plt.Line2D([0], [0], linestyle='-',  linewidth=1.0, color='tab:red', label=r'$\\theta = \\mu_{\\mathrm{before}} + k\\,\\sigma_{\\mathrm{before}}$')\n",
    "\n",
    "    fig.legend(\n",
    "        handles=[on_handle, transit_handle, span_handle, theta_handle],\n",
    "        labels=[\n",
    "            'Valve on/off (command)',\n",
    "            f'Odor transit (~{ODOR_TRANSIT_LAT_S:.2f}s) — start & end',\n",
    "            'Effective odor-on at fly',\n",
    "            r'$\\theta = \\mu_\\mathrm{before} + k\\,\\sigma_\\mathrm{before}$'\n",
    "        ],\n",
    "        title=f'Threshold: k = {int(THRESH_STD_MULT) if THRESH_STD_MULT.is_integer() else THRESH_STD_MULT}',\n",
    "        loc='upper right',\n",
    "        bbox_to_anchor=(0.98, 0.97),\n",
    "        frameon=True,\n",
    "        fontsize=9,\n",
    "        title_fontsize=9,\n",
    "    )\n",
    "\n",
    "    fig.suptitle(f\"{fly} RMS of Proboscis - Eye Distance Percentage\", y=0.995, fontsize=14, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    # === SAVE: write this fly's figure into each odor-specific folder observed for this fly ===\n",
    "    odors_present = sorted({name for (name, _, _, _, _) in trial_curves})\n",
    "    for odor_name in odors_present:\n",
    "        odir = OUT_DIR / _safe_dirname(odor_name)\n",
    "        odir.mkdir(parents=True, exist_ok=True)\n",
    "        out_png = odir / f\"{fly}_envelope_trials_by_odor_{AFTER_SHOW_S}_shifted.png\"\n",
    "        fig.savefig(out_png)\n",
    "        print(f\"[OK] Saved {out_png}\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492e6086-ca1d-4559-959d-fa26f964925d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e0ee0-ffe5-43a9-b272-b7b3a81c4b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# fly_envelope_over_time.py — analytic envelope via Hilbert transform over time per trial\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "# ───────────────────────────────── CONFIG ───────────────────────────────\n",
    "DEFAULT_MAIN_DIRECTORY = main_directory  # keep your existing variable/environment\n",
    "OUT_FIG_DIR           = \"RMS_calculations/envelope_over_time_plots\"\n",
    "FPS_DEFAULT           = 40\n",
    "WINDOW_SEC            = 0.25\n",
    "WINDOW_FRAMES         = max(int(WINDOW_SEC * FPS_DEFAULT), 1)\n",
    "MEASURE_COLS    = [\"distance_percentage_2_6\", \"distance_percentage\"]\n",
    "TRAINING_REGEX        = re.compile(r\"training_(\\d+)\", re.I)\n",
    "\n",
    "# NEW — odor timing (seconds)\n",
    "ODOR_ON_S  = 30.0\n",
    "ODOR_OFF_S = 60.0\n",
    "\n",
    "# ─────────────────────────────── HELPERS ────────────────────────────────\n",
    "def _resolve_measure_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    return next((c for c in MEASURE_COLS if c in df.columns), None)\n",
    "\n",
    "def _extract_trials(data_dir: Path):\n",
    "    \"\"\"\n",
    "    Yield (label, csv_path) for files whose names contain 'training'.\n",
    "    The label is 'training_<num>' if matched, else the stem.\n",
    "    \"\"\"\n",
    "    for csv_path in sorted(data_dir.glob(\"*training*.csv\")):\n",
    "        m = TRAINING_REGEX.search(csv_path.stem)\n",
    "        label = f\"training_{m.group(1)}\" if m else csv_path.stem\n",
    "        yield label, csv_path\n",
    "\n",
    "def _compute_envelope(series: pd.Series, win_frames: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Clip raw values to [0, 100], then compute the analytic envelope\n",
    "    and smooth with a centred rolling mean.\n",
    "    \"\"\"\n",
    "    series = series.clip(lower=0, upper=100)           # ← clipping\n",
    "    analytic = hilbert(series.to_numpy())\n",
    "    env = np.abs(analytic)\n",
    "    return (\n",
    "        pd.Series(env, index=series.index)\n",
    "          .rolling(window=win_frames, center=True, min_periods=1)\n",
    "          .mean()\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PLOTTING\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def plot_envelope_subplots(\n",
    "    fly_name: str,\n",
    "    trials_data: dict[str, tuple[np.ndarray, np.ndarray]],\n",
    "    out_path: Path,\n",
    "    y_max: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot analytic envelope over time for multiple trials as stacked subplots,\n",
    "    marking the global peak with a red dot and sharing y-limits [0, y_max].\n",
    "    Adds vertical bars at odor on/off and a shaded region for odor-on interval.\n",
    "    \"\"\"\n",
    "    n = len(trials_data)\n",
    "    if n == 0:\n",
    "        print(f\"[WARN] {fly_name}: no training trials to plot.\")\n",
    "        return\n",
    "\n",
    "    padded_max = y_max * 1.02\n",
    "\n",
    "    plt.rcParams.update({\"figure.dpi\": 300, \"savefig.dpi\": 300})\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(10, 2.5 * n), sharex=True)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (label, (time_s, env_vals)) in zip(axes, trials_data.items()):\n",
    "        # Envelope trace\n",
    "        ax.plot(time_s, env_vals, linewidth=1, clip_on=False)\n",
    "\n",
    "        # Mark global peak\n",
    "        idx = np.nanargmax(env_vals)\n",
    "        ax.plot(time_s[idx], env_vals[idx], marker='o', markersize=10, color='red', zorder=5)\n",
    "\n",
    "        # Odor on/off markers + shaded interval\n",
    "        ax.axvline(ODOR_ON_S,  linestyle='--', linewidth=1)\n",
    "        ax.axvline(ODOR_OFF_S, linestyle='--', linewidth=1)\n",
    "        ax.axvspan(ODOR_ON_S, ODOR_OFF_S, alpha=0.15)\n",
    "\n",
    "        # Axes cosmetics\n",
    "        ax.set_ylim(0, padded_max, auto=False)\n",
    "        ax.autoscale(enable=False, axis=\"y\")\n",
    "        ax.margins(x=0, y=0)\n",
    "        ax.set_ylabel(\"Envelope\")\n",
    "        ax.set_title(label)\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Legend (simple, non-intrusive)\n",
    "        peak_handle = plt.Line2D([0], [0], marker='o', color='red', linestyle='None', markersize=6, label='Peak')\n",
    "        on_handle   = plt.Line2D([0], [0], linestyle='--', label='Odor on/off')\n",
    "        span_handle = plt.Rectangle((0,0), 1, 1, alpha=0.15, label='Odor on window')\n",
    "        ax.legend(handles=[peak_handle, on_handle, span_handle], loc='upper right', frameon=True, fontsize=8)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\")\n",
    "    fig.suptitle(\n",
    "        f\"{fly_name}: Analytic Envelope Over Time — TRAINING (window={WINDOW_SEC}s; odor {ODOR_ON_S:.0f}–{ODOR_OFF_S:.0f}s)\",\n",
    "        y=0.98\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] {out_path}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# WORKFLOW\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def process_fly_envelope(fly_folder: Path):\n",
    "    \"\"\"\n",
    "    Compute analytic envelope for each TRAINING trial then plot as subplots\n",
    "    with a common y-axis from 0 to the fly’s global maximum.\n",
    "    \"\"\"\n",
    "    fly_name    = fly_folder.name\n",
    "    data_dir    = fly_folder / \"RMS_calculations\"\n",
    "    if not data_dir.is_dir():\n",
    "        print(f\"[WARN] {fly_name}: no RMS_calculations directory.\")\n",
    "        return\n",
    "\n",
    "    trials_data: dict[str, tuple[np.ndarray, np.ndarray]] = {}\n",
    "    all_max = 0.0\n",
    "\n",
    "    for label, csv_path in _extract_trials(data_dir):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # time axis\n",
    "        if \"time_seconds\" in df.columns:\n",
    "            time_s = df[\"time_seconds\"].to_numpy(dtype=float)\n",
    "        else:\n",
    "            time_s = np.arange(len(df)) / FPS_DEFAULT\n",
    "\n",
    "        # measurement series\n",
    "        meas_col = _resolve_measure_column(df)\n",
    "        if meas_col is None:\n",
    "            print(f\"[ERROR] {fly_name} {label}: no measure column.\")\n",
    "            continue\n",
    "\n",
    "        series = pd.to_numeric(df[meas_col], errors=\"coerce\").fillna(0.0)\n",
    "        env_vals = _compute_envelope(series, WINDOW_FRAMES).to_numpy()\n",
    "\n",
    "        trial_max = np.nanmax(env_vals)\n",
    "        print(f\"[DEBUG] {fly_name} {label} peak envelope = {trial_max:.3f}\")\n",
    "\n",
    "        trials_data[label] = (time_s, env_vals)\n",
    "        if trial_max > all_max:\n",
    "            all_max = trial_max\n",
    "\n",
    "    print(f\"[DEBUG] {fly_name} global peak envelope = {all_max:.3f}\")\n",
    "\n",
    "    out_dir  = fly_folder / OUT_FIG_DIR\n",
    "    out_path = out_dir / f\"{fly_name}_TRAINING_envelope_over_time_subplots.png\"\n",
    "    plot_envelope_subplots(fly_name, trials_data, out_path, y_max=all_max)\n",
    "\n",
    "def run_envelope_over_time(main_directory: Optional[Union[Path, str]] = None):\n",
    "    root = Path(main_directory) if main_directory else Path(DEFAULT_MAIN_DIRECTORY)\n",
    "    root = root.expanduser().resolve()\n",
    "    for fly in root.iterdir():\n",
    "        if fly.is_dir():\n",
    "            process_fly_envelope(fly)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_envelope_over_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf0e01-f791-48cd-8045-072334c2169a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def collect_all_plots(main_directory: Union[str, Path], dest_folder: str = \"all_envelope_plots\"):\n",
    "    \"\"\"\n",
    "    Collect all envelope_over_time_subplots.png files from fly folders\n",
    "    into a single folder inside main_directory.\n",
    "    \"\"\"\n",
    "    root = Path(main_directory).expanduser().resolve()\n",
    "    dest = root / dest_folder\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "    for fly in root.iterdir():\n",
    "        if not fly.is_dir():\n",
    "            continue\n",
    "        # expected location of plot\n",
    "        plot_path = fly / OUT_FIG_DIR / f\"{fly.name}_TRAINING_envelope_over_time_subplots.png\"\n",
    "        if plot_path.is_file():\n",
    "            new_name = f\"{fly.name}_envelope_over_time_subplots_training.png\"\n",
    "            shutil.copy2(plot_path, dest / new_name)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[OK] Collected {count} plots into {dest}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_envelope_over_time()  # generate plots\n",
    "    collect_all_plots(DEFAULT_MAIN_DIRECTORY)  # gather them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb7f6a-43ff-43d6-bda4-301dc64b90e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Latency to threshold crossing in training_5/6/7/8 (per fly + per trained-odor means + grand mean by odor)\n",
    "# Figure style: research-presentation / professional\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "# ───────── PARAMETERS ─────────\n",
    "MATRIX_NPY        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/envelope_matrix_float16.npy\")\n",
    "CODES_JSON        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/code_maps.json\")\n",
    "\n",
    "FPS_DEFAULT       = 40.0        # fallback if fps missing/invalid\n",
    "BEFORE_SEC        = 30.0\n",
    "DURING_SEC        = 35.0\n",
    "THRESH_STD_MULT   = 4           # θ = μ_before + k·σ_before\n",
    "LATENCY_CEILING_S = 9.5         # > this window → NR\n",
    "TRIALS_OF_INTEREST = [4, 5, 6]  # training_5/6/7/8\n",
    "\n",
    "OUT_DIR = Path(\"/home/ramanlab/Documents/cole/Results/Opto/Training_RESP_Time_DIST\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Titles\n",
    "TITLE_FLY        = \"{} — Time to PER\"\n",
    "TITLE_ODOR       = \"{} — Mean Time to PER\"\n",
    "TITLE_ODOR_GRAND = \"Grand Mean Time to Reaction by Trained Odor\"\n",
    "\n",
    "# ───────── LOAD MATRIX + MAPS ─────────\n",
    "matrix = np.load(MATRIX_NPY)\n",
    "with open(CODES_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ordered_cols = meta[\"column_order\"]\n",
    "code_maps    = meta[\"code_maps\"]\n",
    "rev_maps     = {c: {v:k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "# Keep fps numeric; only decode label-coded columns\n",
    "decode_cols = [c for c in [\"dataset\",\"fly\",\"trial_type\",\"trial_label\"] if c in ordered_cols]\n",
    "meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "\n",
    "df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "for c in decode_cols:\n",
    "    df[c] = df[c].astype(int).map(rev_maps[c]).fillna(\"UNKNOWN\")\n",
    "\n",
    "# Ensure fps exists & is numeric\n",
    "if \"fps\" in df.columns:\n",
    "    if \"fps\" in rev_maps:  # in case it was code-mapped\n",
    "        df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "    df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"fps\"] = np.nan\n",
    "\n",
    "# Keep only training rows\n",
    "df = df[df[\"trial_type\"].str.lower() == \"training\"].copy()\n",
    "df[\"fps\"] = df[\"fps\"].replace([np.inf, -np.inf], np.nan).fillna(FPS_DEFAULT)\n",
    "\n",
    "# Envelope columns exclude meta (including fps)\n",
    "env_cols  = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "# ───────── Canonicalize trained odor names (for foldering) ─────────\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"ethyl butyrate\": \"EB\",\n",
    "    \"optogenetics benzaldehyde\": \"opto_benz\",\n",
    "    \"optogenetics benzaldehyde \": \"opto_benz\",  # guard\n",
    "    \"optogenetics benzaldehyde 1\": \"opto_benz_1\",\n",
    "    \"optogenetics ethyl butyrate\": \"opto_EB\",\n",
    "    \"10s_odor_benz\": \"10s_Odor_Benz\",\n",
    "}\n",
    "def _canon_dataset(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"UNKNOWN\"\n",
    "    key = s.strip().lower()\n",
    "    return ODOR_CANON.get(key, s.strip())\n",
    "\n",
    "def _safe_dirname(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', str(s)).strip('_')\n",
    "\n",
    "df[\"dataset_canon\"] = df[\"dataset\"].apply(_canon_dataset)\n",
    "\n",
    "# Helpers\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def _extract_env(row: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Return non-padded envelope as 1D array (keep >0 finite).\"\"\"\n",
    "    env = row[env_cols].to_numpy(dtype=float)\n",
    "    return env[np.isfinite(env) & (env > 0)]\n",
    "\n",
    "def latency_to_cross(env: np.ndarray, fps: float) -> float | None:\n",
    "    \"\"\"Latency (s) from DURING start to first sample > θ; None if no crossing.\"\"\"\n",
    "    if env.size == 0 or not np.isfinite(fps) or fps <= 0:\n",
    "        return None\n",
    "    b_end = int(BEFORE_SEC * fps)\n",
    "    d_end = b_end + int(DURING_SEC * fps)\n",
    "    total = env.size\n",
    "    b_end = min(b_end, total); d_end = min(d_end, total)\n",
    "    before = env[:b_end]; during = env[b_end:d_end]\n",
    "    if before.size == 0 or during.size == 0:\n",
    "        return None\n",
    "    theta = float(np.nanmean(before)) + THRESH_STD_MULT * float(np.nanstd(before))\n",
    "    idx = np.where(during > theta)[0]\n",
    "    return (float(idx[0]) / fps) if idx.size else None\n",
    "\n",
    "# ───────── Compute latencies table once (for per-fly + group means + grand means) ─────────\n",
    "lat_records = []\n",
    "for _, row in df.iterrows():\n",
    "    tr = _trial_num(row[\"trial_label\"])\n",
    "    if tr not in TRIALS_OF_INTEREST:\n",
    "        continue\n",
    "    env = _extract_env(row)\n",
    "    fps = float(row.get(\"fps\", FPS_DEFAULT))\n",
    "    fps = fps if (np.isfinite(fps) and fps > 0) else FPS_DEFAULT\n",
    "    lat = latency_to_cross(env, fps)  # None if NR\n",
    "    lat_for_mean = np.nan if (lat is None or lat > LATENCY_CEILING_S) else lat\n",
    "    lat_records.append({\n",
    "        \"dataset\": row[\"dataset\"],\n",
    "        \"dataset_canon\": row[\"dataset_canon\"],\n",
    "        \"fly\": row[\"fly\"],\n",
    "        \"trial_num\": tr,\n",
    "        \"latency\": lat,                # per-fly display\n",
    "        \"lat_for_mean\": lat_for_mean   # for means/STD/aggregates\n",
    "    })\n",
    "\n",
    "lat_df = pd.DataFrame(lat_records)\n",
    "\n",
    "# ───────── Style ─────────\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300, \"savefig.dpi\": 300,\n",
    "    \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "    \"axes.linewidth\": 1.0,\n",
    "    \"xtick.direction\": \"out\", \"ytick.direction\": \"out\",\n",
    "    \"font.size\": 11,\n",
    "})\n",
    "\n",
    "# ───────── PER-FLY LATENCIES — save into that fly's trained-odor folder ─────────\n",
    "for fly in sorted(df[\"fly\"].unique()):\n",
    "    df_fly = df[df[\"fly\"] == fly]\n",
    "    if df_fly.empty:\n",
    "        continue\n",
    "    # assume single trained dataset per fly\n",
    "    odor_folder = _safe_dirname(_canon_dataset(df_fly[\"dataset\"].iloc[0]))\n",
    "    odir = OUT_DIR / odor_folder\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sub = lat_df[lat_df[\"fly\"] == fly]\n",
    "    labels = [f\"Training {n}\" for n in TRIALS_OF_INTEREST]\n",
    "\n",
    "    # Collect per trial\n",
    "    latencies = []\n",
    "    for n in TRIALS_OF_INTEREST:\n",
    "        s = sub[sub[\"trial_num\"] == n][\"latency\"]\n",
    "        latencies.append(s.iloc[0] if len(s) else None)\n",
    "\n",
    "    # If this fly NEVER responds → NR panel\n",
    "    any_resp = any((lat is not None) and (lat <= LATENCY_CEILING_S) for lat in latencies)\n",
    "    if not any_resp:\n",
    "        fig, ax = plt.subplots(figsize=(6.5, 3.2))\n",
    "        ax.set_title(TITLE_FLY.format(fly), pad=10, fontsize=14, weight=\"bold\")\n",
    "        ax.set_xticks(np.arange(len(labels))); ax.set_xticklabels(labels)\n",
    "        ax.set_ylim(0, LATENCY_CEILING_S + 2.0)\n",
    "        ax.text(0.5, 0.55, \"NR\", transform=ax.transAxes, ha=\"center\", va=\"center\",\n",
    "                fontsize=18, color=\"#666666\", weight=\"bold\")\n",
    "        ax.set_ylabel(\"Time After Odor Sent(s)\")\n",
    "        ax.axhline(LATENCY_CEILING_S, linestyle=\"--\", linewidth=1.1, color=\"#444444\")\n",
    "        trans = mtransforms.blended_transform_factory(ax.transAxes, ax.transData)\n",
    "        ax.text(0.995, LATENCY_CEILING_S + 0.12, f\"NR if > {LATENCY_CEILING_S:.1f} s\",\n",
    "                transform=trans, ha=\"right\", va=\"bottom\", fontsize=10, color=\"#444444\", clip_on=False)\n",
    "        fig.tight_layout()\n",
    "        out_png = odir / f\"{fly}_training_{'_'.join(map(str,TRIALS_OF_INTEREST))}_latency.png\"\n",
    "        fig.savefig(out_png); plt.close(fig)\n",
    "        print(f\"[OK] saved {out_png} (NR panel)\")\n",
    "        continue\n",
    "\n",
    "    # Otherwise draw bars per trial\n",
    "    bar_vals, annots, colors = [], [], []\n",
    "    for lat in latencies:\n",
    "        if lat is None or lat > LATENCY_CEILING_S:\n",
    "            bar_vals.append(LATENCY_CEILING_S); annots.append(\"NR\"); colors.append(\"#BDBDBD\")\n",
    "        else:\n",
    "            bar_vals.append(lat); annots.append(f\"{lat:.2f}s\"); colors.append(\"#1A1A1A\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 3.6))\n",
    "    x = np.arange(len(labels))\n",
    "    bars = ax.bar(x, bar_vals, width=0.6, color=colors, edgecolor=\"black\", linewidth=1.0)\n",
    "\n",
    "    for b, txt in zip(bars, annots):\n",
    "        ytxt = float(max(b.get_height() * 0.5, 0.35))\n",
    "        ax.text(b.get_x() + b.get_width()/2, ytxt, txt,\n",
    "                ha=\"center\", va=\"center\", fontsize=10,\n",
    "                color=(\"white\" if txt != \"NR\" else \"#444444\"))\n",
    "\n",
    "    ax.set_xticks(x); ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel(\"Time After Odor Sent (s)\")\n",
    "    ax.set_ylim(0, LATENCY_CEILING_S + 2.5)\n",
    "\n",
    "    ax.axhline(LATENCY_CEILING_S, linestyle=\"--\", linewidth=1.1, color=\"#444444\")\n",
    "    trans = mtransforms.blended_transform_factory(ax.transAxes, ax.transData)\n",
    "    ax.text(0.995, LATENCY_CEILING_S + 0.12, f\"NR if > {LATENCY_CEILING_S:.1f} s\",\n",
    "            transform=trans, ha=\"right\", va=\"bottom\", fontsize=10, color=\"#444444\", clip_on=False)\n",
    "\n",
    "    ax.set_title(TITLE_FLY.format(fly), pad=10, fontsize=14, weight=\"bold\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_png = odir / f\"{fly}_training_{'_'.join(map(str,TRIALS_OF_INTEREST))}_latency.png\"\n",
    "    fig.savefig(out_png); plt.close(fig)\n",
    "    print(f\"[OK] saved {out_png}\")\n",
    "\n",
    "# ───────── PER-TRAINED-ODOR (dataset) MEAN LATENCIES + UPWARD SEM — into each odor folder ─────────\n",
    "for odor in sorted(lat_df[\"dataset_canon\"].unique() if not lat_df.empty else []):\n",
    "    sub = lat_df[lat_df[\"dataset_canon\"] == odor]\n",
    "    labels = [f\"Training {n}\" for n in TRIALS_OF_INTEREST]\n",
    "    odir = OUT_DIR / _safe_dirname(odor)\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if sub.empty:\n",
    "        fig, ax = plt.subplots(figsize=(6.8, 3.2))\n",
    "        ax.set_title(TITLE_ODOR.format(odor), pad=10, fontsize=14, weight=\"bold\")\n",
    "        ax.set_xticks(np.arange(len(labels))); ax.set_xticklabels(labels)\n",
    "        ax.set_ylim(0, LATENCY_CEILING_S + 2.0)\n",
    "        ax.text(0.5, 0.55, \"NR\", transform=ax.transAxes, ha=\"center\", va=\"center\",\n",
    "                fontsize=18, color=\"#666666\", weight=\"bold\")\n",
    "        ax.set_ylabel(\"Time After Odor Sent(s)\")\n",
    "        out_png = odir / f\"{odor}_training_{'_'.join(map(str,TRIALS_OF_INTEREST))}_mean_latency.png\"\n",
    "        fig.tight_layout(); fig.savefig(out_png); plt.close(fig)\n",
    "        print(f\"[OK] saved {out_png} (NR panel)\")\n",
    "        continue\n",
    "\n",
    "    means, sems, ns = [], [], []\n",
    "    for n in TRIALS_OF_INTEREST:\n",
    "        s = sub[sub[\"trial_num\"] == n][\"lat_for_mean\"].to_numpy(dtype=float)\n",
    "        finite = s[np.isfinite(s)]\n",
    "        n_resp = int(finite.size)\n",
    "        mu = float(finite.mean()) if n_resp > 0 else np.nan\n",
    "        sd = float(finite.std(ddof=1)) if n_resp > 1 else (0.0 if n_resp == 1 else np.nan)\n",
    "        sem = (sd / np.sqrt(n_resp)) if n_resp > 1 else (0.0 if n_resp == 1 else np.nan)\n",
    "        means.append(mu); sems.append(sem); ns.append(n_resp)\n",
    "\n",
    "    if sum(ns) == 0:\n",
    "        fig, ax = plt.subplots(figsize=(6.8, 3.2))\n",
    "        ax.set_title(TITLE_ODOR.format(odor), pad=10, fontsize=14, weight=\"bold\")\n",
    "        ax.set_xticks(np.arange(len(labels))); ax.set_xticklabels(labels)\n",
    "        ax.set_ylim(0, LATENCY_CEILING_S + 2.0)\n",
    "        ax.text(0.5, 0.55, \"NR\", transform=ax.transAxes, ha=\"center\", va=\"center\",\n",
    "                fontsize=18, color=\"#666666\", weight=\"bold\")\n",
    "        ax.set_ylabel(\"Time After Odor Sent(s)\")\n",
    "        out_png = odir / f\"{odor}_training_{'_'.join(map(str,TRIALS_OF_INTEREST))}_mean_latency.png\"\n",
    "        fig.tight_layout(); fig.savefig(out_png); plt.close(fig)\n",
    "        print(f\"[OK] saved {out_png} (NR panel)\")\n",
    "        continue\n",
    "\n",
    "    # Upward-only SEM bars\n",
    "    y        = np.nan_to_num(np.array(means, dtype=float), nan=0.0)\n",
    "    yerr_up  = np.nan_to_num(np.array(sems,  dtype=float), nan=0.0)\n",
    "    yerr     = np.vstack([np.zeros_like(yerr_up), yerr_up])  # lower=0, upper=SEM\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.8, 3.8))\n",
    "    x = np.arange(len(labels))\n",
    "    bars = ax.bar(x, y, width=0.6, color=\"#1A1A1A\", edgecolor=\"black\", linewidth=1.0)\n",
    "    ax.errorbar(x, y, yerr=yerr, fmt=\"none\", ecolor=\"black\", elinewidth=1.2, capsize=4)\n",
    "\n",
    "    # Annotate each bar: mean (white, inside) + SEM and n above\n",
    "    for xi, b in enumerate(bars):\n",
    "        n_resp  = ns[xi]\n",
    "        mu_val  = y[xi]\n",
    "        sem_val = yerr_up[xi]\n",
    "        if n_resp == 0 or not np.isfinite(mu_val):\n",
    "            label_y = max(0.5, mu_val + 0.08)\n",
    "            ax.text(b.get_x() + b.get_width()/2, label_y, \"NR\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=9, color=\"#444444\")\n",
    "            continue\n",
    "        inner_y = max(mu_val * 0.50, min(mu_val - 0.10, mu_val * 0.90))\n",
    "        ax.text(b.get_x() + b.get_width()/2, inner_y, f\"{mu_val:.2f}s\",\n",
    "                ha=\"center\", va=\"top\", fontsize=10, color=\"white\")\n",
    "        topper_y = float(mu_val + sem_val + 0.06) if np.isfinite(sem_val) else float(mu_val + 0.06)\n",
    "        ax.text(b.get_x() + b.get_width()/2, topper_y, f\"SEM={sem_val:.2f}s\\nn={n_resp}\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=9, color=\"#333333\")\n",
    "\n",
    "    ax.set_xticks(x); ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel(\"Time After Odor Sent(s)\")\n",
    "    ymax = max(LATENCY_CEILING_S + 2.0, float((y + yerr_up).max()) + 1.2)\n",
    "    ax.set_ylim(0, ymax)\n",
    "\n",
    "    ax.axhline(LATENCY_CEILING_S, linestyle=\"--\", linewidth=1.0, color=\"#6f6f6f\")\n",
    "    trans = mtransforms.blended_transform_factory(ax.transAxes, ax.transData)\n",
    "    ax.text(0.995, LATENCY_CEILING_S + 0.08, f\"NR if > {LATENCY_CEILING_S:.1f} s\",\n",
    "            transform=trans, ha=\"right\", va=\"bottom\", fontsize=9, color=\"#6f6f6f\")\n",
    "\n",
    "    ax.set_title(TITLE_ODOR.format(odor), pad=10, fontsize=14, weight=\"bold\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_png = odir / f\"{odor}_training_{'_'.join(map(str,TRIALS_OF_INTEREST))}_mean_latency.png\"\n",
    "    fig.savefig(out_png); plt.close(fig)\n",
    "    print(f\"[OK] saved {out_png}\")\n",
    "\n",
    "# ───────── GRAND MEAN ACROSS TRIALS BY ODOR (one bar per odor; SEM shown) ─────────\n",
    "odors_all = sorted(df[\"dataset_canon\"].unique())\n",
    "summary_rows = []\n",
    "grand_means, grand_sems, grand_ns = [], [], []\n",
    "\n",
    "for odor in odors_all:\n",
    "    s = lat_df[lat_df[\"dataset_canon\"] == odor][\"lat_for_mean\"].to_numpy(dtype=float)\n",
    "    finite = s[np.isfinite(s)]\n",
    "    n_resp = int(finite.size)\n",
    "    mu = float(finite.mean()) if n_resp > 0 else np.nan\n",
    "    sd = float(finite.std(ddof=1)) if n_resp > 1 else (0.0 if n_resp == 1 else np.nan)\n",
    "    sem = (sd / np.sqrt(n_resp)) if n_resp > 1 else (0.0 if n_resp == 1 else np.nan)\n",
    "\n",
    "    grand_means.append(mu)\n",
    "    grand_sems.append(sem)\n",
    "    grand_ns.append(n_resp)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"odor\": odor, \"n_resp\": n_resp,\n",
    "        \"mean_s\": mu if np.isfinite(mu) else np.nan,\n",
    "        \"sem_s\": sem if np.isfinite(sem) else np.nan\n",
    "    })\n",
    "\n",
    "# Save CSV summary (root)\n",
    "pd.DataFrame(summary_rows).to_csv(OUT_DIR / \"grand_mean_by_odor_latency.csv\", index=False)\n",
    "\n",
    "if sum(grand_ns) == 0:\n",
    "    fig, ax = plt.subplots(figsize=(7.2, 3.2))\n",
    "    ax.set_title(TITLE_ODOR_GRAND, pad=10, fontsize=14, weight=\"bold\")\n",
    "    ax.set_xticks(np.arange(len(odors_all))); ax.set_xticklabels(odors_all, rotation=0)\n",
    "    ax.set_ylim(0, LATENCY_CEILING_S + 2.0)\n",
    "    ax.text(0.5, 0.55, \"NR\", transform=ax.transAxes, ha=\"center\", va=\"center\",\n",
    "            fontsize=18, color=\"#666666\", weight=\"bold\")\n",
    "    ax.set_ylabel(\"Time After Odor Sent(s)\")\n",
    "    ax.axhline(LATENCY_CEILING_S, linestyle=\"--\", linewidth=1.0, color=\"#6f6f6f\")\n",
    "    out_png = OUT_DIR / \"grand_mean_by_odor_latency.png\"\n",
    "    fig.tight_layout(); fig.savefig(out_png); plt.close(fig)\n",
    "    print(f\"[OK] saved {out_png} (NR panel)\")\n",
    "else:\n",
    "    y       = np.nan_to_num(np.array(grand_means, dtype=float), nan=0.0)\n",
    "    yerr_up = np.nan_to_num(np.array(grand_sems,  dtype=float),  nan=0.0)\n",
    "    yerr    = np.vstack([np.zeros_like(yerr_up), yerr_up])  # lower=0, upper=SEM\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(7.2, 1.8*len(odors_all)), 3.8))\n",
    "    x = np.arange(len(odors_all))\n",
    "    bars = ax.bar(x, y, width=0.6, color=\"#1A1A1A\", edgecolor=\"black\", linewidth=1.0)\n",
    "    ax.errorbar(x, y, yerr=yerr, fmt=\"none\", ecolor=\"black\", elinewidth=1.2, capsize=4)\n",
    "\n",
    "    # Annotate SEM (or NR) per odor\n",
    "    for xi, b in enumerate(bars):\n",
    "        n_resp = grand_ns[xi]\n",
    "        sem_val = yerr_up[xi]\n",
    "        if n_resp == 0:\n",
    "            label_y = max(0.5, y[xi] + 0.08)\n",
    "            ax.text(b.get_x() + b.get_width()/2, label_y, \"NR\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=9, color=\"#444444\")\n",
    "        else:\n",
    "            label_y = float(y[xi] + sem_val + 0.06) if np.isfinite(sem_val) else float(y[xi] + 0.06)\n",
    "            ax.text(b.get_x() + b.get_width()/2, label_y, f\"SM={sem_val:.2f} s\\nn={n_resp}\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=9, color=\"#333333\")\n",
    "\n",
    "    ax.set_xticks(x); ax.set_xticklabels(odors_all, rotation=0)\n",
    "    ax.set_ylabel(\"Time After Odor Sent(s)\")\n",
    "    ymax = max(LATENCY_CEILING_S + 2.0, float((y + yerr_up).max()) + 1.2)\n",
    "    ax.set_ylim(0, ymax)\n",
    "\n",
    "    ax.axhline(LATENCY_CEILING_S, linestyle=\"--\", linewidth=1.0, color=\"#6f6f6f\")\n",
    "    ax.set_title(TITLE_ODOR_GRAND, pad=10, fontsize=14, weight=\"bold\")\n",
    "\n",
    "    out_png = OUT_DIR / \"grand_mean_by_odor_latency.png\"\n",
    "    fig.tight_layout(); fig.savefig(out_png); plt.close(fig)\n",
    "    print(f\"[OK] saved {out_png}\")\n",
    "\n",
    "print(\"[DONE] Per-fly, per-trained-odor, and grand-mean-by-odor latency plots exported.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171f689-4e56-47ae-9a83-a811e1c5e445",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Combined Angle / RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d8091-0290-447a-b180-8fc6be17f2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Combine centered angle % with distance %, then RMS + Hilbert envelope\n",
    "from pathlib import Path\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "# ───────── prerequisites ─────────\n",
    "assert 'main_directory' in globals(), \"Define main_directory = '/path/to/root' before running.\"\n",
    "ROOT = Path(main_directory).expanduser().resolve()\n",
    "assert ROOT.is_dir(), f\"Not a directory: {ROOT}\"\n",
    "\n",
    "# ───────── config ─────────\n",
    "FPS_DEFAULT     = 40.0\n",
    "WINDOW_SEC      = 0.25\n",
    "WINDOW_FRAMES   = max(int(WINDOW_SEC * FPS_DEFAULT), 1)\n",
    "\n",
    "# Raw per-trial CSV search patterns (supports one or two patterns)\n",
    "IN_SUFFIX_ANG  = (\"*merged.csv\", \"*class_2_6.csv\")   # or just (\"*merged.csv\",)\n",
    "IN_SUFFIX_DIST = (\"*merged.csv\", \"*class_2_6.csv\")   # or just (\"*merged.csv\",)\n",
    "\n",
    "MONTHS = (\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "          \"july\",\"august\",\"september\",\"october\",\"november\",\"december\")\n",
    "\n",
    "# Odor timing (seconds)\n",
    "ODOR_ON_S  = 30.0\n",
    "ODOR_OFF_S = 60.0\n",
    "\n",
    "# Column candidates\n",
    "TIME_CANDS   = [\"time_s\", \"time_seconds\", \"t_s\", \"time\"]\n",
    "ANGLE_COLS   = [\"angle_centered_pct\", \"angle_centered_percentage\", \"angle_pct\"]\n",
    "DIST_COLS    = [\"distance_percentage_2_6\", \"distance_percentage\", \"distance_pct\", \"measure\", \"value\"]\n",
    "\n",
    "# ───────── utils ─────────\n",
    "TESTING_REGEX = re.compile(r\"testing_(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "def is_month_folder(p: Path) -> bool:\n",
    "    return p.is_dir() and p.name.lower().startswith(MONTHS)\n",
    "\n",
    "def infer_category_from_path(p: Path) -> str | None:\n",
    "    parts = [s.lower() for s in p.parts]\n",
    "    if \"testing\" in parts: return \"testing\"\n",
    "    if \"training\" in parts: return \"training\"\n",
    "    name = p.name.lower()\n",
    "    if \"testing\" in name: return \"testing\"\n",
    "    if \"training\" in name: return \"training\"\n",
    "    return None\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, cands: list[str]) -> str | None:\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _time_axis(df: pd.DataFrame) -> np.ndarray:\n",
    "    c = _pick_col(df, TIME_CANDS)\n",
    "    if c:\n",
    "        return pd.to_numeric(df[c], errors='coerce').to_numpy()\n",
    "    if \"frame\" in df.columns:\n",
    "        return pd.to_numeric(df[\"frame\"], errors='coerce').to_numpy() / FPS_DEFAULT\n",
    "    return np.arange(len(df)) / FPS_DEFAULT\n",
    "\n",
    "def _rolling_rms(x: np.ndarray, win_frames: int) -> np.ndarray:\n",
    "    s = pd.Series(pd.to_numeric(x, errors='coerce')).fillna(0.0)\n",
    "    return (s.pow(2).rolling(window=win_frames, center=True, min_periods=1).mean().pow(0.5)).to_numpy()\n",
    "\n",
    "def _hilbert_envelope(x: np.ndarray, win_frames: int) -> np.ndarray:\n",
    "    env = np.abs(hilbert(np.nan_to_num(x, nan=0.0)))\n",
    "    # light smoothing to match your previous style\n",
    "    return pd.Series(env).rolling(window=win_frames, center=True, min_periods=1).mean().to_numpy()\n",
    "\n",
    "def angle_multiplier(angle_pct: np.ndarray) -> np.ndarray:\n",
    "    ap = np.asarray(angle_pct, dtype=float)\n",
    "    ap = np.clip(ap, -100.0, 100.0)\n",
    "    conds = [\n",
    "        (ap < -40),\n",
    "        (ap >= -40) & (ap < -25),\n",
    "        (ap >= -25) & (ap < -10),\n",
    "        (ap >= -10) & (ap <= 10),\n",
    "        (ap > 10)  & (ap <= 25),\n",
    "        (ap > 25)  & (ap <= 40),\n",
    "        (ap > 40)  & (ap <= 60),\n",
    "        (ap > 60)  & (ap <= 100),\n",
    "    ]\n",
    "    vals = [0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 2.00]\n",
    "    return np.select(conds, vals, default=np.nan)\n",
    "\n",
    "# ───────── discovery for angle & distance trials (raw per-trial CSVs) ─────────\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def _locate_trials_with_cols(fly_dir: Path, suffix_globs: Iterable[str] | str, required_cols: list[str]):\n",
    "    \"\"\"\n",
    "    Return [(label, path, category)] for CSVs containing any of required_cols, within month subfolders.\n",
    "    Accepts one or many suffix patterns (e.g., \"*merged.csv\", \"*combined.csv\").\n",
    "    \"\"\"\n",
    "    # Normalize to iterable\n",
    "    if isinstance(suffix_globs, str):\n",
    "        suffixes = [suffix_globs]\n",
    "    else:\n",
    "        suffixes = list(suffix_globs)\n",
    "\n",
    "    month_folders = [sub for sub in fly_dir.rglob(\"*\") if is_month_folder(sub)]\n",
    "    csvs = []\n",
    "    for month_folder in month_folders:\n",
    "        for suff in suffixes:\n",
    "            csvs.extend(Path(p) for p in glob.iglob(str(month_folder / \"**\" / suff), recursive=True))\n",
    "\n",
    "    out = []\n",
    "    for p in sorted(set(csvs)):\n",
    "        try:\n",
    "            df = pd.read_csv(p, nrows=5)\n",
    "            if _pick_col(df, required_cols):\n",
    "                cat = infer_category_from_path(p) or \"testing\"\n",
    "                out.append((p.stem, p, cat))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def _index_testing_by_id(entries):\n",
    "    \"\"\"Map testing_<#> -> path; plus fallback by lowercase stem.\"\"\"\n",
    "    idx = {}\n",
    "    fallback = {}\n",
    "    for label, path, cat in entries:\n",
    "        if cat != \"testing\":\n",
    "            continue\n",
    "        m = TESTING_REGEX.search(label)\n",
    "        if m:\n",
    "            idx[m.group(0).lower()] = path\n",
    "        else:\n",
    "            fallback[label.lower()] = path\n",
    "    return idx, fallback\n",
    "\n",
    "# ───────── core compute+plot ─────────\n",
    "def compute_and_plot_for_fly(fly_dir: Path):\n",
    "    fly_name = fly_dir.name\n",
    "\n",
    "    # Find raw testing files that have angle and distance signals\n",
    "    angle_entries   = _locate_trials_with_cols(fly_dir, IN_SUFFIX_ANG, ANGLE_COLS)\n",
    "    distance_entries= _locate_trials_with_cols(fly_dir, IN_SUFFIX_DIST, DIST_COLS)\n",
    "\n",
    "    if not distance_entries:\n",
    "        print(f\"[{fly_name}] No testing distance trials found — skipping.\")\n",
    "        return\n",
    "\n",
    "    ang_idx,  ang_fallback  = _index_testing_by_id(angle_entries)\n",
    "    dist_idx, dist_fallback = _index_testing_by_id(distance_entries)\n",
    "\n",
    "    out_dir_csv  = fly_dir / \"angle_distance_rms_envelope\"\n",
    "    out_dir_figs = out_dir_csv / \"plots\"\n",
    "    out_dir_csv.mkdir(parents=True, exist_ok=True)\n",
    "    out_dir_figs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n_done, n_skip = 0, 0\n",
    "\n",
    "    # Anchor iteration on distance trials (time base)\n",
    "    for test_id, dist_path in sorted(dist_idx.items()):\n",
    "        # Match angle file to the same testing_<#>\n",
    "        angle_path = ang_idx.get(test_id)\n",
    "        if angle_path is None:\n",
    "            # try fallback: identical stem\n",
    "            angle_path = ang_fallback.get(Path(dist_path).stem.lower())\n",
    "        if angle_path is None:\n",
    "            print(f\"[WARN] {fly_name} {test_id}: no matching angle file — skipped.\")\n",
    "            n_skip += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load distance\n",
    "            df_d = pd.read_csv(dist_path)\n",
    "            t_d = _time_axis(df_d)\n",
    "            dist_col = _pick_col(df_d, DIST_COLS)\n",
    "            if not dist_col:\n",
    "                raise ValueError(\"No distance_% column in distance trial file.\")\n",
    "            dist_pct = pd.to_numeric(df_d[dist_col], errors='coerce').fillna(0.0).clip(lower=0, upper=100).to_numpy()\n",
    "\n",
    "            # Load angle and align to distance time base\n",
    "            df_a = pd.read_csv(angle_path)\n",
    "            angle_col = _pick_col(df_a, ANGLE_COLS)\n",
    "            if not angle_col:\n",
    "                raise ValueError(\"No angle_centered_% column in angle trial file.\")\n",
    "            t_a = _time_axis(df_a)\n",
    "            ang_pct = pd.to_numeric(df_a[angle_col], errors='coerce').to_numpy()\n",
    "\n",
    "            # Interpolate angle onto distance time base\n",
    "            order = np.argsort(t_a)\n",
    "            t_a_sorted = t_a[order]\n",
    "            ang_sorted = ang_pct[order]\n",
    "            good = np.isfinite(t_a_sorted) & np.isfinite(ang_sorted)\n",
    "            if not np.any(good):\n",
    "                raise ValueError(\"Angle series has no finite values for interpolation.\")\n",
    "            t_a_g = t_a_sorted[good]\n",
    "            ang_g = ang_sorted[good]\n",
    "            ang_on_dist_t = np.interp(t_d, t_a_g, ang_g, left=ang_g[0], right=ang_g[-1])\n",
    "\n",
    "            # Combine angle % and distance % (replace prior RMS×mult):\n",
    "            #   combined_base = distance_pct * angle_multiplier(angle_pct_interp)\n",
    "            mult = angle_multiplier(ang_on_dist_t)\n",
    "            combined_base = dist_pct * mult\n",
    "\n",
    "            # Rolling RMS of the combined signal\n",
    "            combined_rms = _rolling_rms(combined_base, WINDOW_FRAMES)\n",
    "\n",
    "            # Hilbert envelope of the RMS (smoothed)\n",
    "            envelope_rms = _hilbert_envelope(combined_rms, WINDOW_FRAMES)\n",
    "\n",
    "            # Persist\n",
    "            out_df = pd.DataFrame({\n",
    "                \"time_s\": t_d,\n",
    "                \"angle_centered_pct_interp\": ang_on_dist_t,\n",
    "                \"distance_percentage\": dist_pct,\n",
    "                \"multiplier\": mult,\n",
    "                \"combined_base\": combined_base,\n",
    "                \"rolling_rms\": combined_rms,\n",
    "                \"envelope_of_rms\": envelope_rms\n",
    "            })\n",
    "            out_csv = out_dir_csv / f\"{test_id}_angle_distance_rms_envelope.csv\"\n",
    "            out_df.to_csv(out_csv, index=False)\n",
    "\n",
    "            # Plot envelope (primary output) with odor markers\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(t_d, envelope_rms, linewidth=1.5)\n",
    "            plt.axvline(ODOR_ON_S,  color='red', linewidth=2)\n",
    "            plt.axvline(ODOR_OFF_S, color='red', linewidth=2)\n",
    "            plt.title(f\"{fly_name} — {test_id}: Envelope( RMS( distance × angle-mult ) )\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Envelope of RMS (arb.)\")\n",
    "            plt.margins(x=0)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            out_png = out_dir_figs / f\"{fly_name}_{test_id}_env_rms_angle_distance.png\"\n",
    "            plt.savefig(out_png, bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"[OK] {fly_name} {test_id} → CSV: {out_csv.name} | FIG: {out_png.name}\")\n",
    "            n_done += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {fly_name} {test_id} → {e}\")\n",
    "            n_skip += 1\n",
    "\n",
    "    print(f\"[{fly_name}] completed: {n_done}, skipped: {n_skip}\")\n",
    "\n",
    "def run_all(root: Path = ROOT):\n",
    "    for fly_dir in sorted([d for d in root.iterdir() if d.is_dir()]):\n",
    "        compute_and_plot_for_fly(fly_dir)\n",
    "\n",
    "# ───────── execute ─────────\n",
    "run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9c63e-8e30-4114-b8cb-3aec31f5b661",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a7be0-22b1-4985-8cad-2d9ad7721777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────── INPUT: source directories ─────────\n",
    "ROOTS = [\n",
    "    Path(\"/home/ramanlab/Documents/cole/Data/flys/10s_Odor_Benz/\"),\n",
    "    Path(\"/home/ramanlab/Documents/cole/Data/flys/opto_benz/\"),\n",
    "    Path(\"/home/ramanlab/Documents/cole/Data/flys/opto_EB/\"),\n",
    "    Path(\"/home/ramanlab/Documents/cole/Data/flys/opto_benz_1/\"),\n",
    "\n",
    "]\n",
    "\n",
    "# ───────── Destination directory ─────────\n",
    "DEST_ROOT = Path(\"/securedstorage/DATAsec/cole/Data-secured/\")\n",
    "\n",
    "# Valid month prefixes\n",
    "MONTHS = [\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\n",
    "]\n",
    "\n",
    "# ───────── Step 1: Copy all data to secured storage ─────────\n",
    "for root in ROOTS:\n",
    "    if not root.exists():\n",
    "        print(f\"Source folder missing: {root}\")\n",
    "        continue\n",
    "\n",
    "    dest_path = DEST_ROOT / root.name\n",
    "    print(f\"\\nCopying from {root} → {dest_path}\")\n",
    "    dest_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for item in root.rglob(\"*\"):\n",
    "        relative_path = item.relative_to(root)\n",
    "        target = dest_path / relative_path\n",
    "\n",
    "        if item.is_dir():\n",
    "            target.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            if target.exists():\n",
    "                print(f\"Skipping (already exists): {target}\")\n",
    "                continue\n",
    "            shutil.copy2(item, target)\n",
    "            print(f\"Copied: {target}\")\n",
    "\n",
    "print(\"\\nCopy phase completed successfully.\")\n",
    "\n",
    "# ───────── Step 2: Clean up source folders ─────────\n",
    "for root in ROOTS:\n",
    "    print(f\"\\nCleaning up {root}...\")\n",
    "\n",
    "    for fly_folder in root.iterdir():\n",
    "        if not fly_folder.is_dir():\n",
    "            continue  # Skip files directly under ACV/Benz/etc.\n",
    "\n",
    "        # ---- Rule 4: Delete folder if it doesn't start with a valid month ----\n",
    "        folder_name_lower = fly_folder.name.lower()\n",
    "        if not any(folder_name_lower.startswith(month) for month in MONTHS):\n",
    "            shutil.rmtree(fly_folder)\n",
    "            print(f\"Deleted non-month folder: {fly_folder}\")\n",
    "            continue  # Skip further cleanup for this folder since it's gone\n",
    "\n",
    "        # ---- Rule 2 & 3: Inside a valid month fly folder ----\n",
    "        for item in fly_folder.iterdir():\n",
    "            # Preserve RMS_calculations folder\n",
    "            if item.name == \"RMS_calculations\":\n",
    "                print(f\"Preserving folder: {item}\")\n",
    "                continue\n",
    "\n",
    "            # Preserve any CSV files directly inside the fly folder\n",
    "            if item.is_file() and item.suffix.lower() == \".csv\":\n",
    "                print(f\"Preserving CSV file: {item}\")\n",
    "                continue\n",
    "\n",
    "            # Delete other files\n",
    "            if item.is_file():\n",
    "                item.unlink()\n",
    "                print(f\"Deleted file: {item}\")\n",
    "\n",
    "            # Delete directories other than RMS_calculations\n",
    "            elif item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "                print(f\"Deleted folder: {item}\")\n",
    "\n",
    "print(\"\\nCleanup completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc7b90-59e7-49ab-95f4-2983a1363e62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Single CSV / Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ad39b-a9ea-428b-90ce-8389a5c09431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Wide CSV of per-frame DIRECTION VALUE across MANY main_directories\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "# ───────── INPUT: add as many roots as you need ─────────\n",
    "ROOTS = [\n",
    "    Path(\"/securedstorage/DATAsec/cole/Data-secured/opto_EB/\"),\n",
    "    Path(\"/securedstorage/DATAsec/cole/Data-secured/opto_benz/\"),\n",
    "    Path(\"/securedstorage/DATAsec/cole/Data-secured/opto_benz_1/\"),\n",
    "]\n",
    "\n",
    "# ───────── CONFIG (direction_value aggregation) ─────────\n",
    "MEASURE_COLS  = [\"envelope_of_rms\"]\n",
    "FPS_DEFAULT   = 40\n",
    "\n",
    "# Output file (combined for all datasets)\n",
    "OUT_WIDE_CSV = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/all_direction_values_rows_wide.csv\")\n",
    "\n",
    "TRIAL_REGEX = re.compile(r\"(testing|training)_(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "# Timestamp + frame columns we’ll look for (to estimate FPS for metadata)\n",
    "TIMESTAMP_CANDIDATES = [\"UTC_ISO\", \"Timestamp\", \"Number\", \"MonoNs\"]\n",
    "FRAME_CANDIDATES     = [\"Frame\", \"FrameNumber\", \"Frame Number\"]\n",
    "FALLBACK_FPS = 40\n",
    "\n",
    "def _pick_timestamp_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in TIMESTAMP_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _pick_frame_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in FRAME_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _to_seconds_series(df: pd.DataFrame, ts_col: str) -> pd.Series:\n",
    "    s = df[ts_col]\n",
    "    if ts_col in (\"UTC_ISO\", \"Timestamp\"):\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\", utc=(ts_col == \"UTC_ISO\"))\n",
    "        secs = dt.astype(\"int64\") / 1e9  # NaT -> NaN\n",
    "        t0 = np.nanmin(secs.values)\n",
    "        return (secs - t0).astype(float)\n",
    "    if ts_col == \"Number\":\n",
    "        vals = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "        t0 = np.nanmin(vals.values)\n",
    "        return vals - t0\n",
    "    if ts_col == \"MonoNs\":\n",
    "        vals = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "        secs = vals / 1e9\n",
    "        t0 = np.nanmin(secs.values)\n",
    "        return secs - t0\n",
    "    raise ValueError(f\"Unsupported timestamp column: {ts_col}\")\n",
    "\n",
    "def _estimate_fps_from_seconds(seconds_series: pd.Series) -> Optional[float]:\n",
    "    mask = seconds_series.notna()\n",
    "    if mask.sum() < 2:\n",
    "        return None\n",
    "    duration = seconds_series[mask].iloc[-1] - seconds_series[mask].iloc[0]\n",
    "    if duration <= 0:\n",
    "        return None\n",
    "    return mask.sum() / duration\n",
    "\n",
    "def _resolve_measure_column(df: pd.DataFrame) -> str | None:\n",
    "    return next((c for c in MEASURE_COLS if c in df.columns), None)\n",
    "\n",
    "def _infer_trial_type(p: Path) -> str:\n",
    "    s = (p.stem + \"/\" + \"/\".join(q.name for q in p.parents)).lower()\n",
    "    if \"testing\" in s:  return \"testing\"\n",
    "    if \"training\" in s: return \"training\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def _trial_label(p: Path) -> str:\n",
    "    m = TRIAL_REGEX.search(p.stem)\n",
    "    if not m:\n",
    "        chain = (p.stem + \"/\" + \"/\".join(q.name for q in p.parents)).lower()\n",
    "        m = TRIAL_REGEX.search(chain)\n",
    "    if m:\n",
    "        kind, num = m.group(1).lower(), m.group(2)\n",
    "        return f\"{kind}_{num}\"\n",
    "    stem = p.stem\n",
    "    m2 = re.search(r\"(\\d+)$\", stem)\n",
    "    if m2:\n",
    "        return f\"{_infer_trial_type(p)}_{m2.group(1)}\"\n",
    "    return stem\n",
    "\n",
    "def _find_trial_csvs(fly_dir: Path):\n",
    "    search_root = fly_dir / \"envelope_of_rms\"\n",
    "    if not search_root.is_dir():\n",
    "        search_root = fly_dir\n",
    "    patterns = [\"**/*testing*.csv\", \"**/*training*.csv\"]\n",
    "    seen = set()\n",
    "    for pat in patterns:\n",
    "        for csv in search_root.glob(pat):\n",
    "            if csv.is_file():\n",
    "                rp = csv.resolve()\n",
    "                if rp not in seen:\n",
    "                    seen.add(rp)\n",
    "                    yield rp\n",
    "\n",
    "# ───────── PASS 1: discover items and determine max row length ─────────\n",
    "items = []   # [{dataset, fly, csv_path, trial_type, trial_label, measure_col, n_frames}]\n",
    "max_len = 0\n",
    "\n",
    "for root in ROOTS:\n",
    "    root = root.expanduser().resolve()\n",
    "    assert root.is_dir(), f\"Not a directory: {root}\"\n",
    "    dataset = root.name\n",
    "\n",
    "    for fly_dir in sorted(p for p in root.iterdir() if p.is_dir()):\n",
    "        fly = fly_dir.name\n",
    "        for csv_path in _find_trial_csvs(fly_dir):\n",
    "            try:\n",
    "                header_df = pd.read_csv(csv_path, nrows=0)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skip {csv_path.name}: header read error: {e}\")\n",
    "                continue\n",
    "            col = _resolve_measure_column(header_df)\n",
    "            if col is None:\n",
    "                print(f\"[SKIP] {csv_path.name}: none of {MEASURE_COLS} present.\")\n",
    "                continue\n",
    "            try:\n",
    "                n_frames = pd.read_csv(csv_path, usecols=[col]).shape[0]\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skip {csv_path.name}: count error: {e}\")\n",
    "                continue\n",
    "\n",
    "            items.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"fly\": fly,\n",
    "                \"csv_path\": csv_path,\n",
    "                \"trial_type\": _infer_trial_type(csv_path),\n",
    "                \"trial_label\": _trial_label(csv_path),\n",
    "                \"measure_col\": col,\n",
    "                \"n_frames\": n_frames\n",
    "            })\n",
    "            max_len = max(max_len, n_frames)\n",
    "\n",
    "if not items:\n",
    "    raise RuntimeError(\"No eligible testing/training CSVs with 'direction_value' found in provided roots.\")\n",
    "\n",
    "print(f\"[INFO] Datasets: {[r.name for r in ROOTS]}\")\n",
    "print(f\"[INFO] Discovered {len(items)} videos. Max frames = {max_len}\")\n",
    "\n",
    "# ───────── PASS 2: read direction_value and write combined wide CSV ─────────\n",
    "cols = [\"dataset\", \"fly\", \"trial_type\", \"trial_label\", \"fps\"] + [f\"dir_val_{i}\" for i in range(max_len)]\n",
    "pd.DataFrame(columns=cols).to_csv(OUT_WIDE_CSV, index=False)\n",
    "\n",
    "for it in items:\n",
    "    dataset     = it[\"dataset\"]\n",
    "    fly         = it[\"fly\"]\n",
    "    csv_path    = it[\"csv_path\"]\n",
    "    trial_type  = it[\"trial_type\"]\n",
    "    label       = it[\"trial_label\"]\n",
    "    measure_col = it[\"measure_col\"]\n",
    "\n",
    "    # --- Determine FPS from timestamps, if possible ---\n",
    "    try:\n",
    "        hdr2 = pd.read_csv(csv_path, nrows=0)\n",
    "    except Exception:\n",
    "        hdr2 = pd.DataFrame()\n",
    "\n",
    "    frame_col = _pick_frame_column(hdr2) if not hdr2.empty else None\n",
    "    ts_col    = _pick_timestamp_column(hdr2) if not hdr2.empty else None\n",
    "\n",
    "    fps = np.nan\n",
    "    if frame_col is not None and ts_col is not None:\n",
    "        try:\n",
    "            df_ts = pd.read_csv(csv_path, usecols=[frame_col, ts_col])\n",
    "            secs  = _to_seconds_series(df_ts, ts_col)\n",
    "            fps_from_csv = _estimate_fps_from_seconds(secs)\n",
    "            fps = float(fps_from_csv) if (fps_from_csv and np.isfinite(fps_from_csv) and fps_from_csv > 0) else float(FALLBACK_FPS)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] FPS inference failed for {csv_path.name}: {e}\")\n",
    "            fps = float(FALLBACK_FPS)\n",
    "    else:\n",
    "        fps = float(FALLBACK_FPS)\n",
    "    # --- END FPS BLOCK ---\n",
    "\n",
    "    # Read raw direction values\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[measure_col])\n",
    "        vals = pd.to_numeric(df[measure_col], errors=\"coerce\").astype(float).to_numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Read failed {csv_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Build output row\n",
    "    row = [dataset, fly, trial_type, label, fps] + list(vals)\n",
    "\n",
    "    # pad/truncate to max_len\n",
    "    if len(vals) < max_len:\n",
    "        row += [np.nan] * (max_len - len(vals))\n",
    "    elif len(vals) > max_len:\n",
    "        row = row[:5 + max_len]  # 5 metadata columns\n",
    "\n",
    "    pd.DataFrame([row], columns=cols).to_csv(OUT_WIDE_CSV, mode=\"a\", header=False, index=False)\n",
    "\n",
    "print(f\"[OK] Wrote combined direction-value table: {OUT_WIDE_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5868cf-b038-48bb-8806-74b6c71b93a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Convert wide envelope CSV → 16-bit numeric matrix + code key\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ===== INPUT / OUTPUT =====\n",
    "INPUT_CSV = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/all_direction_values_rows_wide.csv\")  # change if your file lives elsewhere\n",
    "OUT_DIR   = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/\")                           # change if desired\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MATRIX_NPY = OUT_DIR / \"envelope_matrix_float16.npy\"   # 16-bit floating matrix\n",
    "CODE_KEY   = OUT_DIR / \"code_key.txt\"                  # human-readable mapping & schema\n",
    "CODES_JSON = OUT_DIR / \"code_maps.json\"                # machine-readable mappings (optional)\n",
    "\n",
    "# ===== LOAD =====\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Identify metadata columns (present subset)\n",
    "meta_cols_all = [\"dataset\", \"fly\", \"trial_type\", \"trial_label\", \"fps\"]\n",
    "meta_cols = [c for c in meta_cols_all if c in df.columns]\n",
    "assert meta_cols, \"No metadata columns found. Expected at least one of: dataset, fly, trial_type, trial_label.\"\n",
    "\n",
    "# Envelope columns (everything else)\n",
    "env_cols = [c for c in df.columns if c not in meta_cols]\n",
    "assert len(env_cols) > 0, \"No envelope columns found.\"\n",
    "\n",
    "# ===== BUILD INTEGER CODES FOR METADATA =====\n",
    "# Codes start at 1; 0 is reserved for 'unknown'\n",
    "code_maps = {}\n",
    "for col in meta_cols:\n",
    "    uniques = pd.Series(df[col].astype(str).fillna(\"UNKNOWN\")).unique().tolist()\n",
    "    mapping = {\"UNKNOWN\": 0}\n",
    "    next_code = 1\n",
    "    for u in uniques:\n",
    "        if u not in mapping:\n",
    "            mapping[u] = next_code\n",
    "            next_code += 1\n",
    "    code_maps[col] = mapping\n",
    "\n",
    "# Apply codes to a copy\n",
    "df_num = df.copy()\n",
    "for col, mapping in code_maps.items():\n",
    "    df_num[col] = df_num[col].astype(str).map(mapping).fillna(0).astype(np.int32)\n",
    "\n",
    "# Ensure envelope columns are numeric and NaN-free\n",
    "df_num[env_cols] = df_num[env_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df_num[env_cols] = df_num[env_cols].fillna(0.0)\n",
    "\n",
    "# ===== BUILD THE MATRIX (float16) =====\n",
    "# Order: [meta_cols...] + [env_0...env_N]\n",
    "ordered_cols = meta_cols + env_cols\n",
    "matrix_f16 = df_num[ordered_cols].to_numpy(dtype=np.float16)\n",
    "\n",
    "# ===== SAVE ARTIFACTS =====\n",
    "np.save(MATRIX_NPY, matrix_f16)\n",
    "\n",
    "# Human-readable key file\n",
    "with CODE_KEY.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Envelope matrix schema (float16), row-wise\\n\")\n",
    "    f.write(\"# Columns (in order):\\n\")\n",
    "    for i, col in enumerate(ordered_cols):\n",
    "        f.write(f\"{i:>5}: {col}\\n\")\n",
    "    f.write(\"\\n# Metadata code maps (string → integer code)\\n\")\n",
    "    for col in meta_cols:\n",
    "        f.write(f\"\\n[{col}]\\n\")\n",
    "        # Sort by numeric code\n",
    "        inv = sorted(((code, name) for name, code in code_maps[col].items()), key=lambda x: x[0])\n",
    "        for code, name in inv:\n",
    "            f.write(f\"{code:>5} : {name}\\n\")\n",
    "    f.write(\"\\nNotes:\\n\")\n",
    "    f.write(\"- Matrix dtype is float16 (16-bit). Metadata codes are stored as float16 numbers in the matrix.\\n\")\n",
    "    f.write(\"- Envelope NaNs (shorter videos) were replaced with 0.0.\\n\")\n",
    "    f.write(\"- Code '0' means UNKNOWN for the metadata fields.\\n\")\n",
    "\n",
    "# Optional: machine-readable mappings\n",
    "with CODES_JSON.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "    json.dump({\"column_order\": ordered_cols, \"code_maps\": code_maps}, jf, indent=2)\n",
    "\n",
    "print(f\"[OK] Saved 16-bit matrix: {MATRIX_NPY}  (shape={matrix_f16.shape}, dtype={matrix_f16.dtype})\")\n",
    "print(f\"[OK] Saved key:           {CODE_KEY}\")\n",
    "print(f\"[OK] Saved JSON maps:     {CODES_JSON}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504eafd-2887-479f-a9a0-9e0975bfe428",
   "metadata": {},
   "source": [
    "## Matrix's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b095c2b-df2e-4f37-b1bc-b3b67f4b2034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Reaction matrices per odor + fly-category counts (During & After)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# ───────── USER KNOB: spacing between rows (increase this to add space)\n",
    "ROW_GAP = 0.6\n",
    "HEIGHT_PER_GAP_IN = 3.0\n",
    "BOTTOM_SHIFT_IN = 0.50\n",
    "\n",
    "# ───────── PARAMETERS ─────────\n",
    "MATRIX_NPY        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/envelope_matrix_float16.npy\")\n",
    "CODES_JSON        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/code_maps.json\")\n",
    "FPS_DEFAULT       = 40\n",
    "BEFORE_SEC        = 30.0\n",
    "DURING_SEC        = 30.0\n",
    "AFTER_WINDOW_SEC  = 30.0\n",
    "THRESH_STD_MULT   = 4\n",
    "MIN_SAMPLES_OVER  = 20\n",
    "\n",
    "ODOR_TRANSIT_LAT_S = overall_mean_latency_s\n",
    "\n",
    "OUT_DIR           = Path(\"/home/ramanlab/Documents/cole/Results/Opto/Matrixs_DISTxANGLE\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Canon keys for grouping\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"Ethyl Butyrate\": \"EB\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz\",\n",
    "    \"Optogenetics Ethyl Butyrate\": \"opto_EB\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz_1\",\n",
    "}\n",
    "DISPLAY_LABEL = {\n",
    "    \"ACV\": \"ACV\",\n",
    "    \"3-octonol\": \"3-Octonol\",\n",
    "    \"Benz\": \"Benzaldehyde\",\n",
    "    \"10s_Odor_Benz\": \"Benzaldehyde\",\n",
    "    \"EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz\": \"Benzaldehyde\",\n",
    "    \"opto_EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz_1\": \"Benzaldehyde\",\n",
    "}\n",
    "ODOR_ORDER = [\"ACV\", \"3-octonol\", \"Benz\", \"EB\", \"10s_Odor_Benz\", \"opto_benz\", \"opto_EB\", \"opto_benz_1\"]\n",
    "\n",
    "# ───────── LOAD + DECODE ─────────\n",
    "matrix = np.load(MATRIX_NPY)\n",
    "with open(CODES_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "ordered_cols = meta[\"column_order\"]\n",
    "code_maps    = meta[\"code_maps\"]\n",
    "rev_maps     = {c: {v:k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "decode_cols = [c for c in [\"dataset\", \"fly\", \"trial_type\", \"trial_label\"] if c in ordered_cols]\n",
    "meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "for c in decode_cols:\n",
    "    df[c] = df[c].astype(int).map(rev_maps[c]).fillna(\"UNKNOWN\")\n",
    "\n",
    "if \"fps\" in df.columns:\n",
    "    if \"fps\" in rev_maps:\n",
    "        df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "    df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"fps\"] = np.nan\n",
    "\n",
    "df = df[df[\"trial_type\"].str.lower() == \"testing\"].copy()\n",
    "\n",
    "FPS_FALLBACK = FPS_DEFAULT\n",
    "df[\"fps\"] = df[\"fps\"].fillna(FPS_FALLBACK).replace([np.inf, -np.inf], FPS_FALLBACK)\n",
    "\n",
    "env_cols = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "def _canon_odor(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"UNKNOWN\"\n",
    "    return ODOR_CANON.get(s.strip().lower(), s.strip())\n",
    "df[\"dataset_canon\"] = df[\"dataset\"].apply(_canon_odor)\n",
    "\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def display_odor_for_trial(dataset_canon: str, trial_label: str) -> str:\n",
    "    n = _trial_num(trial_label)\n",
    "    if n in (1, 3):  # hexanol controls\n",
    "        return \"Hexanol\"\n",
    "    if n in (2, 4, 5):  # trained odor\n",
    "        return DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "\n",
    "    if dataset_canon == \"ACV\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"3-octonol\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Citral\"\n",
    "        if n == 8: return \"Linalool\"\n",
    "    elif dataset_canon == \"Benz\":\n",
    "        if n == 6: return \"Citral\"\n",
    "        if n == 7: return \"Linalool\"\n",
    "    elif dataset_canon == \"EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"10s_Odor_Benz\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "    elif dataset_canon == \"opto_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz_1\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Ethyl Butyrate\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    return trial_label\n",
    "\n",
    "def score_trial_from_env(env_row: pd.Series, fps: float) -> tuple[int, int]:\n",
    "    env = env_row.to_numpy(dtype=float)\n",
    "    env = env[np.isfinite(env) & (env > 0)]\n",
    "    if env.size == 0:\n",
    "        return (0, 0)\n",
    "    total = env.size\n",
    "    b_end   = int(round(BEFORE_SEC * fps))\n",
    "    shift   = int(round(ODOR_TRANSIT_LAT_S * fps))\n",
    "    d_start = b_end + shift\n",
    "    d_end   = b_end + int(round(DURING_SEC * fps)) + shift\n",
    "    a_end   = d_end + int(round(AFTER_WINDOW_SEC * fps))\n",
    "\n",
    "    b_end   = max(0, min(b_end, total))\n",
    "    d_start = max(b_end, min(d_start, total))\n",
    "    d_end   = max(d_start, min(d_end, total))\n",
    "    a_end   = max(d_end, min(a_end, total))\n",
    "\n",
    "    before = env[:b_end]\n",
    "    during = env[d_start:d_end]\n",
    "    after  = env[d_end:a_end]\n",
    "\n",
    "    if before.size == 0:\n",
    "        return (0, 0)\n",
    "\n",
    "    theta = float(np.nanmean(before)) + THRESH_STD_MULT * float(np.nanstd(before))\n",
    "    during_hit = int(np.sum(during > theta) >= MIN_SAMPLES_OVER) if during.size else 0\n",
    "    after_hit  = int(np.sum(after  > theta) >= MIN_SAMPLES_OVER) if after.size  else 0\n",
    "    return during_hit, after_hit\n",
    "\n",
    "# ───────── Score all rows ─────────\n",
    "scores = []\n",
    "for _, row in df.iterrows():\n",
    "    row_fps = float(row.get(\"fps\", FPS_FALLBACK))\n",
    "    d_hit, a_hit = score_trial_from_env(row[env_cols], row_fps)\n",
    "    scores.append({\n",
    "        \"dataset\": row[\"dataset_canon\"],\n",
    "        \"fly\": row[\"fly\"],\n",
    "        \"trial\": row[\"trial_label\"],\n",
    "        \"trial_num\": _trial_num(row[\"trial_label\"]),\n",
    "        \"during_hit\": d_hit,\n",
    "        \"after_hit\": a_hit\n",
    "    })\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# ───────── Colormaps and helpers ─────────\n",
    "cmap = ListedColormap([\"0.7\", \"1.0\", \"0.0\"])  # gray, white, black\n",
    "norm = BoundaryNorm([-1.5, -0.5, 0.5, 1.5], cmap.N)\n",
    "\n",
    "def style_trained_xticks_vertical(ax, labels, trained_disp: str, fontsize: int):\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90, ha=\"center\", va=\"top\", fontsize=fontsize)\n",
    "    txts = []\n",
    "    for tick in ax.get_xticklabels():\n",
    "        txt = tick.get_text()\n",
    "        if txt.strip().lower() == trained_disp.lower():\n",
    "            tick.set_text(trained_disp.upper())\n",
    "            tick.set_color(\"tab:blue\")\n",
    "        txts.append(tick.get_text())\n",
    "    ax.set_xticklabels(txts, rotation=90, ha=\"center\", va=\"top\", fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"x\", pad=2)\n",
    "\n",
    "def compute_fly_category_counts(mat: np.ndarray, labels: list[str], trained_disp: str, include_hexanol: bool = False):\n",
    "    if mat.size == 0:\n",
    "        return {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "    trained_idx = [j for j, lab in enumerate(labels)\n",
    "                   if lab.strip().lower() == trained_disp.lower()]\n",
    "    other_idx = [j for j, lab in enumerate(labels)\n",
    "                 if lab.strip().lower() != trained_disp.lower()\n",
    "                 and (include_hexanol or lab.strip().lower() != \"hexanol\")]\n",
    "    if len(trained_idx) == 0:\n",
    "        return {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "    counts = {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "    for i in range(mat.shape[0]):\n",
    "        row = mat[i, :]\n",
    "        row = np.where(row < 0, 0, row)\n",
    "        t_hit = np.any(row[trained_idx] == 1)\n",
    "        o_hit = np.any(row[other_idx]   == 1) if len(other_idx) else False\n",
    "        if t_hit and not o_hit:\n",
    "            counts[\"Trained only\"] += 1\n",
    "        elif t_hit and o_hit:\n",
    "            counts[\"Trained + Others\"] += 1\n",
    "        elif (not t_hit) and o_hit:\n",
    "            counts[\"Others only\"] += 1\n",
    "    return counts\n",
    "\n",
    "def plot_category_counts(ax, counts: dict, n_flies: int, title: str):\n",
    "    cats = [\"Trained only\", \"Trained + Others\", \"Others only\"]\n",
    "    raw = np.array([counts.get(c, 0) for c in cats], dtype=float)\n",
    "    vals_pct = 100.0 * raw / float(n_flies) if n_flies > 0 else np.zeros_like(raw)\n",
    "    x = np.arange(len(cats))\n",
    "    bars = ax.bar(x, vals_pct, width=0.75, edgecolor=\"black\", linewidth=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cats, rotation=15, ha=\"right\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.set_ylabel(\"% of flies\")\n",
    "    ax.set_title(title, fontsize=12, weight=\"bold\")\n",
    "    ax.margins(x=0.05)\n",
    "    for b, pct in zip(bars, vals_pct):\n",
    "        ax.text(b.get_x() + b.get_width()/2, b.get_height() + 1.5, f\"{pct:.0f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "def shade_latency_on_timeseries(ax, before_sec: float = BEFORE_SEC, latency_s: float = ODOR_TRANSIT_LAT_S):\n",
    "    x0 = before_sec\n",
    "    x1 = before_sec + latency_s\n",
    "    ax.axvspan(x0, x1, color=\"red\", alpha=0.30, lw=0)\n",
    "\n",
    "# ──────── helper: safe dir name\n",
    "def _safe_dirname(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', str(s)).strip('_')\n",
    "\n",
    "# ───────── Build & save per-odor figures (per-odor subfolders) ─────────\n",
    "present = scores_df[\"dataset\"].unique().tolist()\n",
    "ordered_present = [o for o in ODOR_ORDER if o in present]\n",
    "extras = sorted([o for o in present if o not in ODOR_ORDER])\n",
    "for odor in ordered_present + extras:\n",
    "    sub = scores_df[scores_df[\"dataset\"] == odor].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"[WARN] No testing trials for {odor}\")\n",
    "        continue\n",
    "\n",
    "    # per-odor output directory\n",
    "    odir = OUT_DIR / _safe_dirname(odor)\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    flies  = sorted(sub[\"fly\"].unique())\n",
    "    trials = sorted(sub[\"trial\"].unique(), key=_trial_num)\n",
    "    pretty_cols = [display_odor_for_trial(odor, t) for t in trials]\n",
    "\n",
    "    D = -np.ones((len(flies), len(trials)), dtype=int)\n",
    "    A = -np.ones((len(flies), len(trials)), dtype=int)\n",
    "    for i, fly in enumerate(flies):\n",
    "        fly_rows = sub[sub[\"fly\"] == fly]\n",
    "        for j, t in enumerate(trials):\n",
    "            s = fly_rows[fly_rows[\"trial\"] == t]\n",
    "            if s.empty: continue\n",
    "            D[i, j] = int(s[\"during_hit\"].iloc[0])\n",
    "            A[i, j] = int(s[\"after_hit\"].iloc[0])\n",
    "\n",
    "    odor_label   = DISPLAY_LABEL.get(odor, odor)\n",
    "    trained_disp = DISPLAY_LABEL.get(odor, odor)\n",
    "    n_flies = len(flies)\n",
    "    n_trials = len(trials)\n",
    "\n",
    "    base_fig_w = max(10.0, 0.70 * n_trials + 6.0)\n",
    "    base_fig_h = max(5.0, n_flies * 0.26 + 3.8)\n",
    "    fig_w = base_fig_w\n",
    "    fig_h = base_fig_h + ROW_GAP * HEIGHT_PER_GAP_IN\n",
    "    fig_h += BOTTOM_SHIFT_IN\n",
    "    xtick_fs = 9 if n_trials <= 10 else (8 if n_trials <= 16 else 7)\n",
    "\n",
    "    during_counts = compute_fly_category_counts(D, pretty_cols, trained_disp, include_hexanol=True)\n",
    "    after_counts  = compute_fly_category_counts(A, pretty_cols, trained_disp, include_hexanol=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_w, fig_h), constrained_layout=False)\n",
    "    gs  = gridspec.GridSpec(2, 2, height_ratios=[3.0, 1.25], width_ratios=[1, 1], hspace=ROW_GAP, wspace=0.10)\n",
    "\n",
    "    axD  = fig.add_subplot(gs[0, 0])\n",
    "    axA  = fig.add_subplot(gs[0, 1])\n",
    "    axDc = fig.add_subplot(gs[1, 0])\n",
    "    axAc = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    imD = axD.imshow(D, cmap=cmap, norm=norm, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axD.set_title(f\"{odor_label} — During\\n(DURING shifted by +{ODOR_TRANSIT_LAT_S:.2f} s)\", fontsize=14, weight=\"bold\", linespacing=1.1)\n",
    "    style_trained_xticks_vertical(axD, pretty_cols, trained_disp, fontsize=xtick_fs)\n",
    "    axD.set_yticks([]); axD.set_ylabel(f\"{n_flies} Flies\", fontsize=11)\n",
    "\n",
    "    imA = axA.imshow(A, cmap=cmap, norm=norm, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axA.set_title(f\"{odor_label} — After (first {int(AFTER_WINDOW_SEC)} s)\", fontsize=14, weight=\"bold\")\n",
    "    style_trained_xticks_vertical(axA, pretty_cols, trained_disp, fontsize=xtick_fs)\n",
    "    axA.set_yticks([]); axA.set_ylabel(f\"{n_flies} Flies\", fontsize=11)\n",
    "\n",
    "    plot_category_counts(axDc, during_counts, n_flies, title=\"During — Fly Reaction Categories\")\n",
    "    plot_category_counts(axAc, after_counts,  n_flies, title=f\"After (first {int(AFTER_WINDOW_SEC)} s) — Fly Reaction Categories\")\n",
    "\n",
    "    red_patch = Patch(facecolor=\"red\", edgecolor=\"red\", alpha=0.30, label=f\"Odor transit {ODOR_TRANSIT_LAT_S:.2f} s (pre-DURING)\")\n",
    "    axD.legend(handles=[red_patch], loc=\"upper left\", frameon=True, fontsize=9)\n",
    "\n",
    "    shift_frac = BOTTOM_SHIFT_IN / fig_h\n",
    "    for ax in (axDc, axAc):\n",
    "        pos = ax.get_position()\n",
    "        new_y0 = max(0.05, pos.y0 - shift_frac)\n",
    "        ax.set_position([pos.x0, new_y0, pos.width, pos.height])\n",
    "\n",
    "    # Save into per-odor folder\n",
    "    out_png = odir / f\"reaction_matrix_{odor.replace(' ', '_')}_{AFTER_WINDOW_SEC}_latency_{ODOR_TRANSIT_LAT_S:.3f}s.png\"\n",
    "    fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] saved {out_png}\")\n",
    "\n",
    "    key_path = odir / f\"row_key_{odor.replace(' ', '_')}_{AFTER_WINDOW_SEC}.txt\"\n",
    "    with key_path.open(\"w\") as fh:\n",
    "        for i, fly in enumerate(flies):\n",
    "            fh.write(f\"Row {i}: {fly}\\n\")\n",
    "    print(f\"[OK] saved {key_path}\")\n",
    "\n",
    "print(\"[DONE] Per-odor exports saved into subfolders under OUT_DIR.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cee2b2-1224-4298-a0de-f10e8e133e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Reaction matrices per odor + fly-category counts (During & After)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# ───────── USER KNOB: spacing between rows (increase this to add space)\n",
    "ROW_GAP = 0.6            # try 0.10 … 0.60 (higher = more space between rows)\n",
    "HEIGHT_PER_GAP_IN = 3.0  # how many inches of figure height to add per 1.0 ROW_GAP\n",
    "BOTTOM_SHIFT_IN = 0.50   # inches to lower the bottom row; increase to move further down\n",
    "\n",
    "# ───────── PARAMETERS ─────────\n",
    "MATRIX_NPY        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/envelope_matrix_float16.npy\")\n",
    "CODES_JSON        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/code_maps.json\")\n",
    "FPS_DEFAULT       = 40\n",
    "BEFORE_SEC        = 30.0\n",
    "DURING_SEC        = 30.0\n",
    "AFTER_WINDOW_SEC  = 30.0\n",
    "THRESH_STD_MULT   = 4\n",
    "MIN_SAMPLES_OVER  = 20\n",
    "\n",
    "# Shift DURING window by latency at both start and end:\n",
    "# e.g., DURING [30,60] → [30+lat, 60+lat]\n",
    "ODOR_TRANSIT_LAT_S = overall_mean_latency_s\n",
    "\n",
    "OUT_DIR           = Path(\"/home/ramanlab/Documents/cole/Results/Opto/Matrixs_DISTxANGLE\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Canon keys for grouping\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"Ethyl Butyrate\": \"EB\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz_1\",\n",
    "    \"Optogenetics Ethyl Butyrate\": \"opto_EB\",\n",
    "}\n",
    "DISPLAY_LABEL = {\n",
    "    \"ACV\": \"ACV\",\n",
    "    \"3-octonol\": \"3-Octonol\",\n",
    "    \"Benz\": \"Benzaldehyde\",\n",
    "    \"10s_Odor_Benz\": \"Benzaldehyde\",\n",
    "    \"EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz\": \"Benzaldehyde\",\n",
    "    \"opto_EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz_1\": \"Benzaldehyde\",\n",
    "}\n",
    "ODOR_ORDER = [\"ACV\", \"3-octonol\", \"Benz\", \"EB\", \"10s_Odor_Benz\", \"opto_benz\", \"opto_EB\", \"opto_benz_1\"]\n",
    "\n",
    "# ───────── LOAD + DECODE ─────────\n",
    "matrix = np.load(MATRIX_NPY)\n",
    "with open(CODES_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "ordered_cols = meta[\"column_order\"]\n",
    "code_maps    = meta[\"code_maps\"]\n",
    "rev_maps     = {c: {v:k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "decode_cols = [c for c in [\"dataset\", \"fly\", \"trial_type\", \"trial_label\"] if c in ordered_cols]\n",
    "meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "# decode label-coded columns\n",
    "for c in decode_cols:\n",
    "    df[c] = df[c].astype(int).map(rev_maps[c]).fillna(\"UNKNOWN\")\n",
    "\n",
    "# ensure fps numeric\n",
    "if \"fps\" in df.columns:\n",
    "    if \"fps\" in rev_maps:\n",
    "        df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "    df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"fps\"] = np.nan\n",
    "\n",
    "# testing only\n",
    "df = df[df[\"trial_type\"].str.lower() == \"testing\"].copy()\n",
    "\n",
    "# fill missing fps\n",
    "FPS_FALLBACK = FPS_DEFAULT\n",
    "df[\"fps\"] = df[\"fps\"].fillna(FPS_FALLBACK).replace([np.inf, -np.inf], FPS_FALLBACK)\n",
    "\n",
    "# envelope columns exclude meta\n",
    "env_cols = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "def _canon_odor(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"UNKNOWN\"\n",
    "    return ODOR_CANON.get(s.strip().lower(), s.strip())\n",
    "df[\"dataset_canon\"] = df[\"dataset\"].apply(_canon_odor)\n",
    "\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "# ───────── Custom trial→display-odor mapping per dataset ─────────\n",
    "def display_odor_for_trial(dataset_canon: str, trial_label: str) -> str:\n",
    "    n = _trial_num(trial_label)\n",
    "    if n in (1, 3):  # hexanol controls\n",
    "        return \"Hexanol\"\n",
    "    if n in (2, 4, 5):  # trained odor\n",
    "        return DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "\n",
    "    if dataset_canon == \"ACV\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"3-octonol\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Citral\"\n",
    "        if n == 8: return \"Linalool\"\n",
    "    elif dataset_canon == \"Benz\":\n",
    "        if n == 6: return \"Citral\"\n",
    "        if n == 7: return \"Linalool\"\n",
    "    elif dataset_canon == \"EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"10s_Odor_Benz\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "    elif dataset_canon == \"opto_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz_1\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Ethyl Butyrate\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    return trial_label\n",
    "\n",
    "# ───────── Scoring on envelope row ─────────\n",
    "def score_trial_from_env(env_row: pd.Series, fps: float) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Compute During/After hits using a baseline from BEFORE.\n",
    "    DURING is fully shifted by ODOR_TRANSIT_LAT_S at start and end:\n",
    "      DURING: [BEFORE_SEC + ODOR_TRANSIT_LAT_S, BEFORE_SEC + DURING_SEC + ODOR_TRANSIT_LAT_S]\n",
    "      AFTER:  the next AFTER_WINDOW_SEC immediately following DURING.\n",
    "    \"\"\"\n",
    "    env = env_row.to_numpy(dtype=float)\n",
    "    env = env[np.isfinite(env) & (env > 0)]\n",
    "    if env.size == 0:\n",
    "        return (0, 0)\n",
    "\n",
    "    total = env.size\n",
    "\n",
    "    # Indices (in samples)\n",
    "    b_end   = int(round(BEFORE_SEC * fps))  # end of BEFORE\n",
    "    shift   = int(round(ODOR_TRANSIT_LAT_S * fps))\n",
    "    d_start = b_end + shift\n",
    "    d_end   = b_end + int(round(DURING_SEC * fps)) + shift\n",
    "    a_end   = d_end + int(round(AFTER_WINDOW_SEC * fps))\n",
    "\n",
    "    # Clip/guard\n",
    "    b_end   = max(0, min(b_end, total))\n",
    "    d_start = max(b_end, min(d_start, total))\n",
    "    d_end   = max(d_start, min(d_end, total))\n",
    "    a_end   = max(d_end, min(a_end, total))\n",
    "\n",
    "    # Windows\n",
    "    before = env[:b_end]\n",
    "    during = env[d_start:d_end]   # fully shifted DURING window\n",
    "    after  = env[d_end:a_end]\n",
    "\n",
    "    if before.size == 0:\n",
    "        return (0, 0)\n",
    "\n",
    "    # Threshold from BEFORE baseline\n",
    "    theta = float(np.nanmean(before)) + THRESH_STD_MULT * float(np.nanstd(before))\n",
    "\n",
    "    # Hits (require at least MIN_SAMPLES_OVER above theta)\n",
    "    during_hit = int(np.sum(during > theta) >= MIN_SAMPLES_OVER) if during.size else 0\n",
    "    after_hit  = int(np.sum(after  > theta) >= MIN_SAMPLES_OVER) if after.size  else 0\n",
    "    return during_hit, after_hit\n",
    "\n",
    "# ───────── Score all rows ─────────\n",
    "scores = []\n",
    "for _, row in df.iterrows():\n",
    "    row_fps = float(row.get(\"fps\", FPS_FALLBACK))\n",
    "    d_hit, a_hit = score_trial_from_env(row[env_cols], row_fps)\n",
    "    scores.append({\n",
    "        \"dataset\": row[\"dataset_canon\"],\n",
    "        \"fly\": row[\"fly\"],\n",
    "        \"trial\": row[\"trial_label\"],\n",
    "        \"trial_num\": _trial_num(row[\"trial_label\"]),\n",
    "        \"during_hit\": d_hit,\n",
    "        \"after_hit\": a_hit\n",
    "    })\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# ───────── Colormaps and helpers ─────────\n",
    "cmap = ListedColormap([\"0.7\", \"1.0\", \"0.0\"])  # gray, white, black\n",
    "norm = BoundaryNorm([-1.5, -0.5, 0.5, 1.5], cmap.N)\n",
    "\n",
    "def style_trained_xticks_vertical(ax, labels, trained_disp: str, fontsize: int):\n",
    "    \"\"\"Vertical x labels; trained odor BLUE + UPPERCASE.\"\"\"\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90, ha=\"center\", va=\"top\", fontsize=fontsize)\n",
    "    txts = []\n",
    "    for tick in ax.get_xticklabels():\n",
    "        txt = tick.get_text()\n",
    "        if txt.strip().lower() == trained_disp.lower():\n",
    "            tick.set_text(trained_disp.upper())\n",
    "            tick.set_color(\"tab:blue\")\n",
    "        txts.append(tick.get_text())\n",
    "    ax.set_xticklabels(txts, rotation=90, ha=\"center\", va=\"top\", fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"x\", pad=2)\n",
    "\n",
    "def compute_fly_category_counts(mat: np.ndarray, labels: list[str], trained_disp: str, include_hexanol: bool = False):\n",
    "    if mat.size == 0:\n",
    "        return {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "\n",
    "    trained_idx = [j for j, lab in enumerate(labels)\n",
    "                   if lab.strip().lower() == trained_disp.lower()]\n",
    "\n",
    "    other_idx = [j for j, lab in enumerate(labels)\n",
    "                 if lab.strip().lower() != trained_disp.lower()\n",
    "                 and (include_hexanol or lab.strip().lower() != \"hexanol\")]\n",
    "    if len(trained_idx) == 0:\n",
    "        return {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "\n",
    "    counts = {\"Trained only\": 0, \"Trained + Others\": 0, \"Others only\": 0}\n",
    "    for i in range(mat.shape[0]):\n",
    "        row = mat[i, :]\n",
    "        row = np.where(row < 0, 0, row)  # treat missing (-1) as 0 for categorization\n",
    "        t_hit = np.any(row[trained_idx] == 1)\n",
    "        o_hit = np.any(row[other_idx]   == 1) if len(other_idx) else False\n",
    "\n",
    "        if t_hit and not o_hit:\n",
    "            counts[\"Trained only\"] += 1\n",
    "        elif t_hit and o_hit:\n",
    "            counts[\"Trained + Others\"] += 1\n",
    "        elif (not t_hit) and o_hit:\n",
    "            counts[\"Others only\"] += 1\n",
    "    return counts\n",
    "\n",
    "def plot_category_counts(ax, counts: dict, n_flies: int, title: str):\n",
    "    cats = [\"Trained only\", \"Trained + Others\", \"Others only\"]\n",
    "    raw = np.array([counts.get(c, 0) for c in cats], dtype=float)\n",
    "    vals_pct = 100.0 * raw / float(n_flies) if n_flies > 0 else np.zeros_like(raw)\n",
    "\n",
    "    x = np.arange(len(cats))\n",
    "    bars = ax.bar(x, vals_pct, width=0.75, edgecolor=\"black\", linewidth=0.8)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cats, rotation=15, ha=\"right\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.set_ylabel(\"% of flies\")\n",
    "    ax.set_title(title, fontsize=12, weight=\"bold\")\n",
    "    ax.margins(x=0.05)\n",
    "\n",
    "    for b, pct in zip(bars, vals_pct):\n",
    "        ax.text(b.get_x() + b.get_width()/2,\n",
    "                b.get_height() + 1.5,\n",
    "                f\"{pct:.0f}%\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "# ─────── helper: safe dir name\n",
    "def _safe_dirname(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', str(s)).strip('_')\n",
    "\n",
    "# ───────── Build & save per-odor figures (into per-odor subfolders) ─────────\n",
    "# Only iterate over odors present; preserve preferred order, then extras\n",
    "present = scores_df[\"dataset\"].unique().tolist()\n",
    "ordered_present = [o for o in ODOR_ORDER if o in present]\n",
    "extras = sorted([o for o in present if o not in ODOR_ORDER])\n",
    "\n",
    "for odor in ordered_present + extras:\n",
    "    sub = scores_df[scores_df[\"dataset\"] == odor].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"[WARN] No testing trials for {odor}\")\n",
    "        continue\n",
    "\n",
    "    # per-odor output directory\n",
    "    odir = OUT_DIR / _safe_dirname(odor)\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    flies  = sorted(sub[\"fly\"].unique())\n",
    "    # Build trial order: trained odor first (2,4,5), then 1,3,6,7,8,9\n",
    "    existing_trials = list(sub[\"trial\"].unique())\n",
    "\n",
    "    def _tnum(lbl):\n",
    "        m = re.search(r\"(\\d+)\", str(lbl))\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    desired_order = [2, 4, 5, 1, 3, 6, 7, 8, 9]\n",
    "\n",
    "    by_num = {}\n",
    "    for t in existing_trials:\n",
    "        n = _tnum(t)\n",
    "        if n not in by_num:\n",
    "            by_num[n] = t\n",
    "\n",
    "    ordered_trials = [by_num[n] for n in desired_order if n in by_num]\n",
    "    leftovers = sorted([n for n in by_num.keys() if n not in set(desired_order) and n >= 0])\n",
    "    ordered_trials += [by_num[n] for n in leftovers]\n",
    "\n",
    "    trials = ordered_trials\n",
    "    pretty_cols = [display_odor_for_trial(odor, t) for t in trials]\n",
    "\n",
    "    # Matrices with sentinel -1 for missing, else 0/1\n",
    "    D = -np.ones((len(flies), len(trials)), dtype=int)\n",
    "    A = -np.ones((len(flies), len(trials)), dtype=int)\n",
    "    for i, fly in enumerate(flies):\n",
    "        fly_rows = sub[sub[\"fly\"] == fly]\n",
    "        for j, t in enumerate(trials):\n",
    "            s = fly_rows[fly_rows[\"trial\"] == t]\n",
    "            if s.empty: continue\n",
    "            D[i, j] = int(s[\"during_hit\"].iloc[0])\n",
    "            A[i, j] = int(s[\"after_hit\"].iloc[0])\n",
    "\n",
    "    odor_label   = DISPLAY_LABEL.get(odor, odor)\n",
    "    trained_disp = DISPLAY_LABEL.get(odor, odor)\n",
    "    n_flies = len(flies)\n",
    "    n_trials = len(trials)\n",
    "\n",
    "    # Figure size (height grows with flies and with ROW_GAP)\n",
    "    base_fig_w = max(10.0, 0.70 * n_trials + 6.0)\n",
    "    base_fig_h = max(5.0, n_flies * 0.26 + 3.8)\n",
    "    fig_w = base_fig_w\n",
    "    fig_h = base_fig_h + ROW_GAP * HEIGHT_PER_GAP_IN\n",
    "    fig_h += BOTTOM_SHIFT_IN   # keep layout comfortable while lowering bottom row\n",
    "\n",
    "    xtick_fs = 9 if n_trials <= 10 else (8 if n_trials <= 16 else 7)\n",
    "\n",
    "    # NEW: Compute fly-category counts for During & After\n",
    "    during_counts = compute_fly_category_counts(D, pretty_cols, trained_disp, include_hexanol=True)\n",
    "    after_counts  = compute_fly_category_counts(A, pretty_cols, trained_disp, include_hexanol=True)\n",
    "\n",
    "    # Create figure (manual layout)\n",
    "    fig = plt.figure(figsize=(fig_w, fig_h), constrained_layout=False)\n",
    "    gs  = gridspec.GridSpec(\n",
    "        2, 2,\n",
    "        height_ratios=[3.0, 1.25],\n",
    "        width_ratios=[1, 1],\n",
    "        hspace=ROW_GAP,\n",
    "        wspace=0.10\n",
    "    )\n",
    "\n",
    "    axD  = fig.add_subplot(gs[0, 0])   # top-left  (During matrix)\n",
    "    axA  = fig.add_subplot(gs[0, 1])   # top-right (After matrix)\n",
    "    axDc = fig.add_subplot(gs[1, 0])   # bottom-left  (During categories)\n",
    "    axAc = fig.add_subplot(gs[1, 1])   # bottom-right (After categories)\n",
    "\n",
    "    # Top: matrices — vertical x labels, trained odor in blue\n",
    "    imD = axD.imshow(D, cmap=cmap, norm=norm, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axD.set_title(\n",
    "        f\"{odor_label} — During (shifted +{ODOR_TRANSIT_LAT_S:.2f}s)\",\n",
    "        fontsize=14, weight=\"bold\"\n",
    "    )\n",
    "    style_trained_xticks_vertical(axD, pretty_cols, trained_disp, fontsize=xtick_fs)\n",
    "    axD.set_yticks([]); axD.set_ylabel(f\"{n_flies} Flies\", fontsize=11)\n",
    "\n",
    "    imA = axA.imshow(A, cmap=cmap, norm=norm, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axA.set_title(f\"{odor_label} — After (first {int(AFTER_WINDOW_SEC)} s)\", fontsize=14, weight=\"bold\")\n",
    "    style_trained_xticks_vertical(axA, pretty_cols, trained_disp, fontsize=xtick_fs)\n",
    "    axA.set_yticks([]); axA.set_ylabel(f\"{n_flies} Flies\", fontsize=11)\n",
    "\n",
    "    # Bottom: category count bars\n",
    "    plot_category_counts(axDc, during_counts, n_flies, title=\"During — Fly Reaction Categories\")\n",
    "    plot_category_counts(axAc, after_counts,  n_flies, title=f\"After (first {int(AFTER_WINDOW_SEC)} s) — Fly Reaction Categories\")\n",
    "\n",
    "    # Lower the bottom row by BOTTOM_SHIFT_IN (inches)\n",
    "    shift_frac = BOTTOM_SHIFT_IN / fig_h\n",
    "    for ax in (axDc, axAc):\n",
    "        pos = ax.get_position()\n",
    "        new_y0 = max(0.05, pos.y0 - shift_frac)\n",
    "        ax.set_position([pos.x0, new_y0, pos.width, pos.height])\n",
    "\n",
    "    # Save — into per-odor folder\n",
    "    out_png = odir / f\"reaction_matrix_{odor.replace(' ', '_')}_{AFTER_WINDOW_SEC}_latency_{ODOR_TRANSIT_LAT_S:.3f}s_unordered.png\"\n",
    "    fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] saved {out_png}\")\n",
    "\n",
    "    # Row index → fly key — into per-odor folder\n",
    "    key_path = odir / f\"row_key_{odor.replace(' ', '_')}_{AFTER_WINDOW_SEC}.txt\"\n",
    "    with key_path.open(\"w\") as fh:\n",
    "        for i, fly in enumerate(flies):\n",
    "            fh.write(f\"Row {i}: {fly}\\n\")\n",
    "    print(f\"[OK] saved {key_path}\")\n",
    "\n",
    "    # ───────── CSV per odor with actual odor names — into per-odor folder ─────────\n",
    "    sub_for_csv = sub.copy()\n",
    "    sub_for_csv[\"odor_sent\"] = sub_for_csv[\"trial\"].apply(lambda t: display_odor_for_trial(odor, t))\n",
    "    order_map = {t: i for i, t in enumerate(trials)}\n",
    "    sub_for_csv[\"trial_ord\"] = sub_for_csv[\"trial\"].map(order_map).fillna(10**9).astype(int)\n",
    "    sub_for_csv = sub_for_csv.sort_values([\"fly\", \"trial_ord\", \"trial_num\", \"trial\"])\n",
    "\n",
    "    export_cols = [\"dataset\", \"fly\", \"trial_num\", \"odor_sent\", \"during_hit\", \"after_hit\"]\n",
    "    out_csv = odir / f\"binary_reactions_{odor.replace(' ', '_')}.csv\"\n",
    "    sub_for_csv[export_cols].to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] saved {out_csv}\")\n",
    "\n",
    "print(\"[DONE] Per-odor exports saved into subfolders under OUT_DIR.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73c3dc-fc2c-4546-b61b-1e5f17fe5223",
   "metadata": {},
   "source": [
    "## Envolope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de8f78-b953-43a4-8583-f35848fad624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Per-fly envelope plots from MATRIX with trained-odor styling\n",
    "# (after-period limited to 30 s) + per-trial threshold line\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= PARAMETERS =========\n",
    "MATRIX_NPY        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/envelope_matrix_float16.npy\")\n",
    "CODES_JSON        = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/code_maps.json\")\n",
    "\n",
    "FPS_DEFAULT       = 40.0       # fallback if fps missing/invalid\n",
    "ODOR_ON_S         = 30.0\n",
    "ODOR_OFF_S        = 60.0\n",
    "AFTER_SHOW_S      = 30.0       # show only first 30 s after odor OFF\n",
    "\n",
    "# Threshold params — matches your scoring code design (baseline is [0, ODOR_ON_S))\n",
    "THRESH_STD_MULT   = 4.0        # θ = μ_before + k·σ\n",
    "\n",
    "# Odor transit latency (mean time for plume to reach the fly)\n",
    "ODOR_TRANSIT_LAT_S = overall_mean_latency_s\n",
    "\n",
    "# IMPORTANT: extend visible window so AFTER is measured from shifted OFF\n",
    "X_MAX_LIMIT       = ODOR_OFF_S + ODOR_TRANSIT_LAT_S + AFTER_SHOW_S\n",
    "\n",
    "OUT_DIR = Path(\"/home/ramanlab/Documents/cole/Results/Opto/Envlope_DISTxANGLE\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= LOAD MATRIX + METADATA =========\n",
    "matrix = np.load(MATRIX_NPY, allow_pickle=False)\n",
    "with open(CODES_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ordered_cols = meta[\"column_order\"]\n",
    "code_maps    = meta[\"code_maps\"]\n",
    "rev_maps     = {c: {v:k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "decode_cols = [c for c in [\"dataset\",\"fly\",\"trial_type\",\"trial_label\"] if c in ordered_cols]\n",
    "meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "\n",
    "df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "# Decode labels -> strings\n",
    "for c in decode_cols:\n",
    "    df[c] = df[c].astype(int).map(rev_maps[c]).fillna(\"UNKNOWN\")\n",
    "\n",
    "# Ensure fps exists and is numeric\n",
    "if \"fps\" in df.columns:\n",
    "    if \"fps\" in rev_maps:  # if coded (rare)\n",
    "        df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "    df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"fps\"] = np.nan\n",
    "\n",
    "# Keep only testing trials\n",
    "df = df[df[\"trial_type\"].str.lower()==\"testing\"].copy()\n",
    "\n",
    "# Fill missing/invalid fps with fallback\n",
    "df[\"fps\"] = df[\"fps\"].replace([np.inf, -np.inf], np.nan).fillna(FPS_DEFAULT)\n",
    "\n",
    "# env_* columns exclude meta (including fps)\n",
    "env_cols  = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "# ========= Canon keys & display names =========\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"Ethyl Butyrate\": \"EB\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz\",\n",
    "    \"Optogenetics benzaldehyde\": \"opto_benz_1\",\n",
    "    \"Optogenetics Ethyl Butyrate\": \"opto_EB\",\n",
    "}\n",
    "DISPLAY_LABEL = {\n",
    "    \"ACV\": \"ACV\",\n",
    "    \"3-octonol\": \"3-Octonol\",\n",
    "    \"Benz\": \"Benzaldehyde\",\n",
    "    \"10s_Odor_Benz\": \"Benzaldehyde\",\n",
    "    \"EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz\": \"Benzaldehyde\",\n",
    "    \"opto_benz_1\": \"Benzaldehyde\",\n",
    "    \"opto_EB\": \"Ethyl Butyrate\",\n",
    "}\n",
    "\n",
    "def _canon_dataset(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"UNKNOWN\"\n",
    "    return ODOR_CANON.get(s.strip().lower(), s.strip())\n",
    "\n",
    "df[\"dataset_canon\"] = df[\"dataset\"].apply(_canon_dataset)\n",
    "\n",
    "# helper: safe dir name\n",
    "def _safe_dirname(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', str(s)).strip('_')\n",
    "\n",
    "# ========= Helpers =========\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def display_odor_for_trial(dataset_canon: str, trial_label: str) -> str:\n",
    "    n = _trial_num(trial_label)\n",
    "    if n in (1, 3):  # controls\n",
    "        return \"Hexanol\"\n",
    "    if n in (2, 4, 5):  # trained odor\n",
    "        return DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "\n",
    "    if dataset_canon == \"ACV\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"3-octonol\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Citral\"\n",
    "        if n == 8: return \"Linalool\"\n",
    "    elif dataset_canon == \"Benz\":\n",
    "        if n == 6: return \"Citral\"\n",
    "        if n == 7: return \"Linalool\"\n",
    "    elif dataset_canon == \"EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"10s_Odor_Benz\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "    elif dataset_canon == \"opto_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz_1\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Ethyl Butyrate\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    return trial_label\n",
    "\n",
    "def _extract_env(row: pd.Series) -> np.ndarray:\n",
    "    env = row[env_cols].to_numpy(dtype=float)\n",
    "    env = env[np.isfinite(env) & (env > 0)]\n",
    "    return env\n",
    "\n",
    "def _compute_theta(env_full: np.ndarray, fps: float) -> float:\n",
    "    \"\"\"θ = mean(before) + k*std(before), where before = [0, ODOR_ON_S).\"\"\"\n",
    "    if env_full.size == 0 or fps <= 0:\n",
    "        return np.nan\n",
    "    b_end = int(ODOR_ON_S * fps)\n",
    "    b_end = min(b_end, env_full.size)\n",
    "    before = env_full[:b_end]\n",
    "    if before.size == 0:\n",
    "        return np.nan\n",
    "    mu = float(np.nanmean(before))\n",
    "    sd = float(np.nanstd(before))\n",
    "    return mu + THRESH_STD_MULT * sd\n",
    "\n",
    "def _is_trained_odor(dataset_canon: str, odor_name: str) -> bool:\n",
    "    trained = DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "    return str(odor_name).strip().lower() == str(trained).strip().lower()\n",
    "\n",
    "def style_trained_title(ax, odor_label: str):\n",
    "    ax.set_title(\n",
    "        odor_label.upper(),\n",
    "        loc=\"left\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "        pad=2,\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "\n",
    "# ========= MAKE FIGURES PER FLY =========\n",
    "for fly, g in df.groupby(\"fly\"):\n",
    "    g = g.sort_values(\"trial_label\", key=lambda s: s.map(_trial_num))\n",
    "    dataset_canon = _canon_dataset(g[\"dataset\"].iloc[0])\n",
    "\n",
    "    # (odor_name, t_visible, env_visible, theta, is_trained)\n",
    "    trial_curves = []\n",
    "    y_max = 0.0\n",
    "\n",
    "    for _, row in g.iterrows():\n",
    "        env_full = _extract_env(row)\n",
    "        if env_full.size == 0:\n",
    "            continue\n",
    "\n",
    "        row_fps = float(row.get(\"fps\", FPS_DEFAULT)) if np.isfinite(row.get(\"fps\", np.nan)) else FPS_DEFAULT\n",
    "        t_full = np.arange(env_full.size, dtype=float) / max(row_fps, 1e-9)\n",
    "\n",
    "        theta = _compute_theta(env_full, row_fps)\n",
    "\n",
    "        # Clip to [0, X_MAX_LIMIT] for visualization\n",
    "        mask = (t_full <= X_MAX_LIMIT + 1e-9)\n",
    "        t = t_full[mask]\n",
    "        env = env_full[mask]\n",
    "        if t.size == 0:\n",
    "            continue\n",
    "\n",
    "        odor_name = display_odor_for_trial(dataset_canon, row[\"trial_label\"])\n",
    "        trial_curves.append((odor_name, t, env, theta, _is_trained_odor(dataset_canon, odor_name)))\n",
    "\n",
    "        local_max = np.nanmax(env) if np.isfinite(env).any() else 0.0\n",
    "        if np.isfinite(theta):\n",
    "            local_max = max(local_max, theta)\n",
    "        y_max = max(y_max, float(local_max))\n",
    "\n",
    "    if not trial_curves:\n",
    "        print(f\"[WARN] {fly}: no usable testing trials; skipping.\")\n",
    "        continue\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        \"figure.dpi\": 300, \"savefig.dpi\": 300,\n",
    "        \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "        \"axes.linewidth\": 0.8, \"xtick.direction\": \"out\", \"ytick.direction\": \"out\",\n",
    "        \"font.size\": 10,\n",
    "    })\n",
    "\n",
    "    n = len(trial_curves)\n",
    "    fig_h = max(3.0, n * 1.6 + 1.5)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(10, fig_h), sharex=True)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (odor_name, t, env, theta, is_trained) in zip(axes, trial_curves):\n",
    "        ax.plot(t, env, linewidth=1.2, color='black')\n",
    "\n",
    "        # Nominal valve timing markers (hardware command times)\n",
    "        ax.axvline(ODOR_ON_S,  linestyle='--', linewidth=1.0, color='black')\n",
    "        ax.axvline(ODOR_OFF_S, linestyle='--', linewidth=1.0, color='black')\n",
    "\n",
    "        # Effective plume windows using latency:\n",
    "        on_lat_end   = min(ODOR_ON_S  + ODOR_TRANSIT_LAT_S, X_MAX_LIMIT)\n",
    "        off_lat_end  = min(ODOR_OFF_S + ODOR_TRANSIT_LAT_S, X_MAX_LIMIT)\n",
    "        eff_on_start = min(on_lat_end, X_MAX_LIMIT)\n",
    "        eff_on_end   = min(off_lat_end, X_MAX_LIMIT)\n",
    "\n",
    "        # Shade start latency (red) and effective ON (gray)\n",
    "        if ODOR_TRANSIT_LAT_S > 0:\n",
    "            ax.axvspan(ODOR_ON_S, on_lat_end, alpha=0.25, color='red')\n",
    "        if eff_on_end > eff_on_start:\n",
    "            ax.axvspan(eff_on_start, eff_on_end, alpha=0.15, color='gray')\n",
    "\n",
    "        # Shade end latency (red)\n",
    "        if ODOR_TRANSIT_LAT_S > 0:\n",
    "            ax.axvspan(ODOR_OFF_S, off_lat_end, alpha=0.25, color='red')\n",
    "\n",
    "        # Threshold line\n",
    "        if np.isfinite(theta):\n",
    "            ax.axhline(theta, linestyle='-', linewidth=1.0, color='tab:red', alpha=0.9)\n",
    "\n",
    "        ax.set_ylim(0, y_max * 1.02 if y_max > 0 else 1.0)\n",
    "        ax.set_xlim(0, X_MAX_LIMIT)\n",
    "        ax.margins(x=0, y=0.02)\n",
    "        ax.set_ylabel(\"DIST x ANGLE RMS\", fontsize=10)\n",
    "\n",
    "        if is_trained:\n",
    "            style_trained_title(ax, odor_name)\n",
    "        else:\n",
    "            ax.set_title(odor_name, loc=\"left\", fontsize=11, weight=\"bold\", pad=2, color=\"black\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\", fontsize=11)\n",
    "\n",
    "    # SINGLE legend on the figure\n",
    "    on_handle      = plt.Line2D([0], [0], linestyle='--', linewidth=1.0, color='black', label='Valve on/off (command)')\n",
    "    transit_handle = plt.Rectangle((0,0), 1, 1, alpha=0.25, color='red',  label=f'Odor transit (~{ODOR_TRANSIT_LAT_S:.2f}s)')\n",
    "    span_handle    = plt.Rectangle((0,0), 1, 1, alpha=0.15, color='gray', label='Effective odor-on at fly')\n",
    "    theta_handle   = plt.Line2D([0], [0], linestyle='-',  linewidth=1.0, color='tab:red', label=r'$\\theta = \\mu_{\\mathrm{before}} + k\\,\\sigma_{\\mathrm{before}}$')\n",
    "\n",
    "    fig.legend(\n",
    "        handles=[on_handle, transit_handle, span_handle, theta_handle],\n",
    "        labels=[\n",
    "            'Valve on/off (command)',\n",
    "            f'Odor transit (~{ODOR_TRANSIT_LAT_S:.2f}s) — start & end',\n",
    "            'Effective odor-on at fly',\n",
    "            r'$\\theta = \\mu_\\mathrm{before} + k\\,\\sigma_\\mathrm{before}$'\n",
    "        ],\n",
    "        title=f'Threshold: k = {int(THRESH_STD_MULT) if THRESH_STD_MULT.is_integer() else THRESH_STD_MULT}',\n",
    "        loc='upper right',\n",
    "        bbox_to_anchor=(0.98, 0.97),\n",
    "        frameon=True,\n",
    "        fontsize=9,\n",
    "        title_fontsize=9,\n",
    "    )\n",
    "\n",
    "    fig.suptitle(f\"{fly} RMS of Proboscis - Eye Distance Percentage\", y=0.995, fontsize=14, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    # === SAVE: write this fly's figure into each odor-specific folder observed for this fly ===\n",
    "    odors_present = sorted({name for (name, _, _, _, _) in trial_curves})\n",
    "    for odor_name in odors_present:\n",
    "        odir = OUT_DIR / _safe_dirname(odor_name)\n",
    "        odir.mkdir(parents=True, exist_ok=True)\n",
    "        out_png = odir / f\"{fly}_envelope_trials_by_odor_{AFTER_SHOW_S}_shifted.png\"\n",
    "        fig.savefig(out_png)\n",
    "        print(f\"[OK] Saved {out_png}\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86def2-805f-4912-abeb-1d436ab830a2",
   "metadata": {},
   "source": [
    "## Combined + RMS Alone Envolpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4aa314-a850-43dc-bc24-d19a989f25ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — Overlay envelopes per fly & per testing-trial across TWO matrices\n",
    "# Threshold: θ = μ_global(fly, source, all pre-odor) + k * σ_trial(pre-odor)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= INPUTS (two sources to overlay) =========\n",
    "SOURCES = {\n",
    "    \"RMS x Angle\": {\n",
    "        \"MATRIX_NPY\": Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/envelope_matrix_float16.npy\"),\n",
    "        \"CODES_JSON\": Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto_combined/code_maps.json\"),\n",
    "    },\n",
    "    \"RMS\": {\n",
    "        \"MATRIX_NPY\": Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/envelope_matrix_float16.npy\"),\n",
    "        \"CODES_JSON\": Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/code_maps.json\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "# ========= PARAMETERS =========\n",
    "FPS_DEFAULT         = 40.0\n",
    "ODOR_ON_S           = 30.0\n",
    "ODOR_OFF_S          = 60.0\n",
    "AFTER_SHOW_S        = 30.0\n",
    "THRESH_STD_MULT     = 4.0  # k\n",
    "\n",
    "# If overall_mean_latency_s isn't in scope, default to 0.0\n",
    "try:\n",
    "    ODOR_TRANSIT_LAT_S = float(overall_mean_latency_s)\n",
    "    if not np.isfinite(ODOR_TRANSIT_LAT_S): ODOR_TRANSIT_LAT_S = 0.0\n",
    "except Exception:\n",
    "    ODOR_TRANSIT_LAT_S = 0.0\n",
    "\n",
    "X_MAX_LIMIT         = ODOR_OFF_S + ODOR_TRANSIT_LAT_S + AFTER_SHOW_S\n",
    "\n",
    "OUT_DIR = Path(\"/home/ramanlab/Documents/cole/Results/Manual/Compare_Envlopes\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= CANONICALIZATION =========\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"ethyl butyrate\": \"EB\",\n",
    "    \"optogenetics benzaldehyde\": \"opto_benz\",\n",
    "    \"optogenetics benzaldehyde\": \"opto_benz_1\",\n",
    "    \"optogenetics ethyl butyrate\": \"opto_EB\",\n",
    "}\n",
    "DISPLAY_LABEL = {\n",
    "    \"ACV\": \"ACV\",\n",
    "    \"3-octonol\": \"3-Octonol\",\n",
    "    \"Benz\": \"Benzaldehyde\",\n",
    "    \"10s_Odor_Benz\": \"Benzaldehyde\",\n",
    "    \"EB\": \"Ethyl Butyrate\",\n",
    "    \"ret_EB\": \"Ethyl Butyrate\",\n",
    "    \"opto_benz\": \"Benzaldehyde\",\n",
    "    \"opto_benz_1\": \"Benzaldehyde\",\n",
    "    \"opto_EB\": \"Ethyl Butyrate\",\n",
    "}\n",
    "\n",
    "def _canon_dataset(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"UNKNOWN\"\n",
    "    return ODOR_CANON.get(s.strip().lower(), s.strip())\n",
    "\n",
    "# NEW: safe folder names\n",
    "def _safe_dirname(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', str(s)).strip('_')\n",
    "\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def display_odor_for_trial(dataset_canon: str, trial_label: str) -> str:\n",
    "    n = _trial_num(trial_label)\n",
    "    if n in (1, 3):  # controls\n",
    "        return \"Hexanol\"\n",
    "    if n in (2, 4, 5):  # trained odor\n",
    "        return DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "\n",
    "    if dataset_canon == \"ACV\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"3-octonol\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Citral\"\n",
    "        if n == 8: return \"Linalool\"\n",
    "    elif dataset_canon == \"Benz\":\n",
    "        if n == 6: return \"Citral\"\n",
    "        if n == 7: return \"Linalool\"\n",
    "    elif dataset_canon == \"EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"ret_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"10s_Odor_Benz\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "    elif dataset_canon == \"opto_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz_1\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Ethyl Butyrate\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    return trial_label\n",
    "\n",
    "# ========= LOADING HELPERS =========\n",
    "def load_source_df(tag: str, paths: dict):\n",
    "    matrix = np.load(paths[\"MATRIX_NPY\"], allow_pickle=False)\n",
    "    with open(paths[\"CODES_JSON\"], \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    ordered_cols = meta[\"column_order\"]\n",
    "    code_maps    = meta[\"code_maps\"]\n",
    "    rev_maps     = {c: {v:k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "    decode_cols = [c for c in [\"dataset\",\"fly\",\"trial_type\",\"trial_label\"] if c in ordered_cols]\n",
    "    meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "\n",
    "    df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "    # decode label columns\n",
    "    for c in decode_cols:\n",
    "        df[c] = df[c].astype(int).map(rev_maps[c]).fillna(\"UNKNOWN\")\n",
    "\n",
    "    if \"fps\" in df.columns:\n",
    "        if \"fps\" in rev_maps:  # rarely coded\n",
    "            df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "        df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"fps\"] = np.nan\n",
    "\n",
    "    # testing only\n",
    "    df = df[df[\"trial_type\"].str.lower()==\"testing\"].copy()\n",
    "    df[\"fps\"] = df[\"fps\"].replace([np.inf, -np.inf], np.nan).fillna(FPS_DEFAULT)\n",
    "\n",
    "    df[\"dataset_canon\"] = df[\"dataset\"].apply(_canon_dataset)\n",
    "    env_cols = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "    df[\"_env_cols\"] = [env_cols]*len(df)  # attach per-row for extraction\n",
    "    df[\"_source\"]   = tag\n",
    "    return df\n",
    "\n",
    "def _extract_env(row: pd.Series) -> np.ndarray:\n",
    "    env_cols = row.get(\"_env_cols\", [])\n",
    "    env = row[env_cols].to_numpy(dtype=float)\n",
    "    env = env[np.isfinite(env) & (env > 0)]\n",
    "    return env\n",
    "\n",
    "def _trial_baseline(env_full: np.ndarray, fps: float) -> np.ndarray:\n",
    "    \"\"\"Return pre-odor samples for this trial [0, ODOR_ON_S).\"\"\"\n",
    "    if env_full.size == 0 or not np.isfinite(fps) or fps <= 0:\n",
    "        return np.array([], dtype=float)\n",
    "    b_end = int(ODOR_ON_S * fps)\n",
    "    b_end = min(b_end, env_full.size)\n",
    "    return env_full[:b_end]\n",
    "\n",
    "# ========= LOAD BOTH SOURCES =========\n",
    "dfs = []\n",
    "for tag, p in SOURCES.items():\n",
    "    try:\n",
    "        dfs.append(load_source_df(tag, p))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load {tag}: {e}\")\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No sources loaded.\")\n",
    "\n",
    "all_df = pd.concat(dfs, ignore_index=True)\n",
    "all_flies = sorted(all_df[\"fly\"].unique(), key=lambda x: str(x))\n",
    "\n",
    "# ========= PLOTTING STYLES PER SOURCE =========\n",
    "SOURCE_STYLES = {\n",
    "    \"RMS x Angle\": dict(color=\"tab:blue\",   label=\"RMS x Angle\"),\n",
    "    \"RMS\":          dict(color=\"tab:orange\", label=\"RMS\"),\n",
    "}\n",
    "default_style = dict(color=None, label=None)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300, \"savefig.dpi\": 300,\n",
    "    \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "    \"axes.linewidth\": 0.8, \"xtick.direction\": \"out\", \"ytick.direction\": \"out\",\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "# ========= BUILD & PLOT =========\n",
    "for fly in all_flies:\n",
    "    df_fly = all_df[all_df[\"fly\"] == fly].copy()\n",
    "    if df_fly.empty:\n",
    "        continue\n",
    "\n",
    "    # ---- NEW: compute μ_global per (fly, source) from ALL trials' pre-odor samples\n",
    "    global_mu_by_source = {}\n",
    "    for src, gsrc in df_fly.groupby(\"_source\"):\n",
    "        pooled = []\n",
    "        for _, row in gsrc.iterrows():\n",
    "            env_full = _extract_env(row)\n",
    "            fps = float(row.get(\"fps\", FPS_DEFAULT))\n",
    "            pre = _trial_baseline(env_full, fps)\n",
    "            if pre.size:\n",
    "                pooled.append(pre[np.isfinite(pre)])\n",
    "        if pooled:\n",
    "            pooled_all = np.concatenate(pooled)\n",
    "            pooled_all = pooled_all[np.isfinite(pooled_all)]\n",
    "            if pooled_all.size:\n",
    "                global_mu_by_source[src] = float(np.mean(pooled_all))\n",
    "                continue\n",
    "        global_mu_by_source[src] = np.nan  # fallback later\n",
    "\n",
    "    # keys: trial_label -> list of (source, t, env, theta, odor_name, dataset_canon, is_trained)\n",
    "    trials = {}\n",
    "    y_max  = 0.0\n",
    "\n",
    "    for _, row in df_fly.iterrows():\n",
    "        env_full = _extract_env(row)\n",
    "        if env_full.size == 0:\n",
    "            continue\n",
    "\n",
    "        fps = float(row.get(\"fps\", FPS_DEFAULT))\n",
    "        t_full = np.arange(env_full.size, dtype=float) / max(fps, 1e-9)\n",
    "\n",
    "        # clip visual window\n",
    "        mask = (t_full <= X_MAX_LIMIT + 1e-9)\n",
    "        t    = t_full[mask]\n",
    "        env  = env_full[mask]\n",
    "        if t.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Trial-specific σ from this trial's pre-odor; μ is global per source\n",
    "        pre_this = _trial_baseline(env_full, fps)\n",
    "        if pre_this.size:\n",
    "            sigma_trial = float(np.nanstd(pre_this))\n",
    "            mu_trial    = float(np.nanmean(pre_this))  # for fallback only\n",
    "        else:\n",
    "            sigma_trial = np.nan\n",
    "            mu_trial    = np.nan\n",
    "\n",
    "        src = row[\"_source\"]\n",
    "        mu_global = global_mu_by_source.get(src, np.nan)\n",
    "        if not np.isfinite(mu_global):\n",
    "            mu_global = mu_trial\n",
    "\n",
    "        theta = (mu_global + THRESH_STD_MULT * sigma_trial) if (np.isfinite(mu_global) and np.isfinite(sigma_trial)) else np.nan\n",
    "\n",
    "        dsc        = str(row.get(\"dataset_canon\", \"UNKNOWN\"))\n",
    "        trial_lab  = str(row.get(\"trial_label\", \"UNKNOWN\"))\n",
    "        odor_name  = display_odor_for_trial(dsc, trial_lab)\n",
    "        is_trained = str(odor_name).strip().lower() == str(DISPLAY_LABEL.get(dsc, dsc)).strip().lower()\n",
    "\n",
    "        trials.setdefault(trial_lab, []).append(\n",
    "            (src, t, env, theta, odor_name, dsc, is_trained)\n",
    "        )\n",
    "\n",
    "        local_max = np.nanmax(env) if np.isfinite(env).any() else 0.0\n",
    "        if np.isfinite(theta):\n",
    "            local_max = max(local_max, theta)\n",
    "        y_max = max(y_max, float(local_max))\n",
    "\n",
    "    if not trials:\n",
    "        print(f\"[WARN] {fly}: no usable testing trials across sources; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # stable trial order\n",
    "    trial_labels_sorted = sorted(trials.keys(), key=_trial_num)\n",
    "    n = len(trial_labels_sorted)\n",
    "    fig_h = max(3.0, n * 1.6 + 1.5)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(10, fig_h), sharex=True)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, trial_lab in zip(axes, trial_labels_sorted):\n",
    "        curves = trials[trial_lab]\n",
    "\n",
    "        # Title odor from first curve\n",
    "        odor_name  = curves[0][4]\n",
    "        dsc        = curves[0][5]\n",
    "        is_trained = curves[0][6]\n",
    "\n",
    "        # Valve command lines\n",
    "        ax.axvline(ODOR_ON_S,  linestyle='--', linewidth=1.0, color='black')\n",
    "        ax.axvline(ODOR_OFF_S, linestyle='--', linewidth=1.0, color='black')\n",
    "\n",
    "        # Latency shading\n",
    "        on_lat_end   = min(ODOR_ON_S  + ODOR_TRANSIT_LAT_S, X_MAX_LIMIT)\n",
    "        off_lat_end  = min(ODOR_OFF_S + ODOR_TRANSIT_LAT_S, X_MAX_LIMIT)\n",
    "        eff_on_start = min(on_lat_end, X_MAX_LIMIT)\n",
    "        eff_on_end   = min(off_lat_end, X_MAX_LIMIT)\n",
    "        if ODOR_TRANSIT_LAT_S > 0:\n",
    "            ax.axvspan(ODOR_ON_S, on_lat_end, alpha=0.25, color='red')\n",
    "        if eff_on_end > eff_on_start:\n",
    "            ax.axvspan(eff_on_start, eff_on_end, alpha=0.15, color='gray')\n",
    "        if ODOR_TRANSIT_LAT_S > 0:\n",
    "            ax.axvspan(ODOR_OFF_S, off_lat_end, alpha=0.25, color='red')\n",
    "\n",
    "        # Overlay curves from each source on this trial\n",
    "        for (src, t, env, theta, _odor, _dsc, _is_trained) in curves:\n",
    "            st = SOURCE_STYLES.get(src, default_style)\n",
    "            line = ax.plot(t, env, linewidth=1.3, **{k:v for k,v in st.items() if v is not None})\n",
    "            # Per-trace threshold (global μ + trial σ)\n",
    "            if np.isfinite(theta):\n",
    "                ax.axhline(theta, linestyle=':', linewidth=1.0, color=line[0].get_color(), alpha=0.9)\n",
    "\n",
    "        ax.set_ylim(0, y_max * 1.02 if y_max > 0 else 1.0)\n",
    "        ax.set_xlim(0, X_MAX_LIMIT)\n",
    "        ax.margins(x=0, y=0.02)\n",
    "        ax.set_ylabel(\"DIST or DISTxANGLE\", fontsize=10)\n",
    "\n",
    "        if is_trained:\n",
    "            ax.set_title(f\"{odor_name} — {trial_lab}\", loc=\"left\", fontsize=11, weight=\"bold\", pad=2, color=\"tab:blue\")\n",
    "        else:\n",
    "            ax.set_title(f\"{odor_name} — {trial_lab}\", loc=\"left\", fontsize=11, weight=\"bold\", pad=2, color=\"black\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\", fontsize=11)\n",
    "\n",
    "    # Legend\n",
    "    src_handles = [plt.Line2D([0],[0], linewidth=1.3, color=SOURCE_STYLES[s]['color'], label=SOURCE_STYLES[s]['label'])\n",
    "                   for s in SOURCES.keys() if s in SOURCE_STYLES]\n",
    "    on_handle      = plt.Line2D([0], [0], linestyle='--', linewidth=1.0, color='black', label='Valve on/off (command)')\n",
    "    transit_handle = plt.Rectangle((0,0), 1, 1, alpha=0.25, color='red',  label=f'Odor transit (~{ODOR_TRANSIT_LAT_S:.2f}s)')\n",
    "    span_handle    = plt.Rectangle((0,0), 1, 1, alpha=0.15, color='gray', label='Effective odor-on at fly')\n",
    "    theta_handle   = plt.Line2D([0], [0], linestyle=':',  linewidth=1.0, color='black', label=r'$\\theta=\\mu_{\\mathrm{global}}+k\\sigma_{\\mathrm{trial}}$')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.legend(\n",
    "        handles=src_handles + [on_handle, transit_handle, span_handle, theta_handle],\n",
    "        loc='upper right',\n",
    "        bbox_to_anchor=(0.98, 0.97),\n",
    "        frameon=True,\n",
    "        fontsize=9,\n",
    "        title=f'k = {int(THRESH_STD_MULT) if float(THRESH_STD_MULT).is_integer() else THRESH_STD_MULT}',\n",
    "        title_fontsize=9,\n",
    "    )\n",
    "\n",
    "    fig.suptitle(f\"{fly} — Envelope overlay by testing trial (global μ per source, σ per trial)\", y=0.995, fontsize=14, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    # === SAVE: write this fly's overlay into each odor-specific folder observed for this fly ===\n",
    "    odors_present = sorted({curves[0][4] for curves in trials.values()})\n",
    "    for odor_name in odors_present:\n",
    "        odir = OUT_DIR / _safe_dirname(odor_name)\n",
    "        odir.mkdir(parents=True, exist_ok=True)\n",
    "        out_png = odir / f\"{fly}_overlay_envelope_by_trial_{AFTER_SHOW_S}s_shifted.png\"\n",
    "        fig.savefig(out_png)\n",
    "        print(f\"[OK] Saved {out_png}\")\n",
    "\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1816fb-25a8-4d59-b065-a18563c99d14",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3036da-dda2-4196-9f67-cb99e54efe97",
   "metadata": {},
   "source": [
    "# Special Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490111e8-a2c5-43cc-b786-22f9e197a04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER CELL — RMS trace + per-trial heatmap (testing 2/4/5/8 only; hard 30–60s ON; no latency)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype']  = 42\n",
    "mpl.rcParams['savefig.transparent'] = False  # ensure opaque background\n",
    "\n",
    "FILL_GRAY = \"#e6e6e6\"  # light gray, opaque (no alpha)\n",
    "RASTERIZE_HEATMAPS = True  # keep heatmaps raster in vector outputs (clean EPS)\n",
    "\n",
    "# ========= PARAMETERS =========\n",
    "MATRIX_NPY  = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/envelope_matrix_float16.npy\")\n",
    "CODES_JSON  = Path(\"/home/ramanlab/Documents/cole/Data/single_matrix_opto/code_maps.json\")\n",
    "OUT_DIR     = Path(\"/home/ramanlab/Documents/cole/Results/rms_trace_plus_heatmaps_opto\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FPS_DEFAULT = 40.0\n",
    "ODOR_ON_S   = 30.0\n",
    "ODOR_OFF_S  = 60.0\n",
    "AFTER_S     = 30.0                 # show only first 30 s after OFF\n",
    "X_MAX_LIMIT = ODOR_OFF_S + AFTER_S # = 90 s\n",
    "\n",
    "THRESH_STD_MULT = 4.0              # θ = μ_before + k·σ\n",
    "KEEP_TEST_NUMS  = {2, 4, 5, 6, 8}     # testing trials to include\n",
    "\n",
    "# ---- Visual knobs ----\n",
    "HM_ONOFF_LS    = \"--\"   # heatmap odor on/off line style\n",
    "HM_ONOFF_LW    = 1.5    # heatmap odor on/off line width\n",
    "TRACE_ONOFF_LW = 1.5    # trace odor on/off line width\n",
    "\n",
    "# ---- Figure / colorbar layout knobs ----\n",
    "FIG_W            = 16.0  # wider figure\n",
    "CBAR_COL_RATIO   = 0.04  # dedicated colorbar column width (relative to plot column)\n",
    "HSPACE           = 0.60  # vertical spacing between rows\n",
    "WSPACE           = 0.125  # spacing between plot column and colorbar\n",
    "TITLE_Y          = 0.95  # extra gap above first axes (used with constrained_layout)\n",
    "\n",
    "# ========= LOAD MATRIX + METADATA =========\n",
    "matrix = np.load(MATRIX_NPY, allow_pickle=False)\n",
    "with open(CODES_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ordered_cols = meta[\"column_order\"]\n",
    "code_maps    = meta[\"code_maps\"]\n",
    "rev_maps     = {c: {v: k for k, v in m.items()} for c, m in code_maps.items()}\n",
    "\n",
    "decode_cols = [c for c in [\"dataset\",\"fly\",\"trial_type\",\"trial_label\"] if c in ordered_cols]\n",
    "meta_cols   = decode_cols + ([\"fps\"] if \"fps\" in ordered_cols else [])\n",
    "\n",
    "df = pd.DataFrame(matrix, columns=ordered_cols)\n",
    "\n",
    "# Decode categorical codes (if present)\n",
    "for c in decode_cols:\n",
    "    if df[c].dtype != object:\n",
    "        df[c] = df[c].astype(int).map(rev_maps.get(c, {})).fillna(df[c].astype(str))\n",
    "\n",
    "# FPS handling\n",
    "if \"fps\" in df.columns:\n",
    "    if \"fps\" in rev_maps:  # rare coded fps\n",
    "        df[\"fps\"] = df[\"fps\"].astype(int).map(rev_maps[\"fps\"])\n",
    "    df[\"fps\"] = pd.to_numeric(df[\"fps\"], errors=\"coerce\").fillna(FPS_DEFAULT)\n",
    "else:\n",
    "    df[\"fps\"] = FPS_DEFAULT\n",
    "\n",
    "# Keep only testing trials\n",
    "df = df[df[\"trial_type\"].str.lower() == \"testing\"].copy()\n",
    "\n",
    "# Envelope cols (all non-meta)\n",
    "env_cols = [c for c in ordered_cols if c not in meta_cols]\n",
    "\n",
    "# ========= ODOR CANON + LABELS =========\n",
    "ODOR_CANON = {\n",
    "    \"acv\": \"ACV\",\n",
    "    \"apple cider vinegar\": \"ACV\",\n",
    "    \"apple-cider-vinegar\": \"ACV\",\n",
    "    \"3-octonol\": \"3-octonol\",\n",
    "    \"3 octonol\": \"3-octonol\",\n",
    "    \"3-octanol\": \"3-octonol\",\n",
    "    \"3 octanol\": \"3-octonol\",\n",
    "    \"benz\": \"Benz\",\n",
    "    \"benzaldehyde\": \"Benz\",\n",
    "    \"benz-ald\": \"Benz\",\n",
    "    \"benzadhyde\": \"Benz\",\n",
    "    \"ethyl butyrate\": \"EB\",\n",
    "    \"ret_eb\": \"ret_EB\",\n",
    "    \"10s_odor_benz\": \"10s_Odor_Benz\",\n",
    "    \"opto eb\": \"opto_EB\",\n",
    "    \"opto_eb\": \"opto_EB\",\n",
    "    \"opto benz\": \"opto_benz\",\n",
    "    \"opto_benz\": \"opto_benz\",\n",
    "}\n",
    "DISPLAY_LABEL = {\n",
    "    \"ACV\": \"ACV\",\n",
    "    \"3-octonol\": \"3-Octonol\",\n",
    "    \"Benz\": \"Benzaldehyde\",\n",
    "    \"EB\": \"Ethyl Butyrate\",\n",
    "    \"ret_EB\": \"Ethyl Butyrate (ret.)\",\n",
    "    \"10s_Odor_Benz\": \"Benzaldehyde\",\n",
    "    \"opto_benz\": \"Benzaldehyde\",\n",
    "    \"opto_EB\": \"Ethyl Butyrate\",\n",
    "}\n",
    "\n",
    "def _canon_dataset(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"UNKNOWN\"\n",
    "    key = s.strip()\n",
    "    low = key.lower()\n",
    "    return ODOR_CANON.get(low, key)\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def _trial_num(label: str) -> int:\n",
    "    m = re.search(r\"(\\d+)\", str(label))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def _extract_env(row: pd.Series) -> np.ndarray:\n",
    "    env = row[env_cols].to_numpy(dtype=float)\n",
    "    env = env[np.isfinite(env) & (env > 0)]\n",
    "    return env\n",
    "\n",
    "def _compute_theta(env_full: np.ndarray, fps: float) -> float:\n",
    "    if env_full.size == 0 or fps <= 0:\n",
    "        return np.nan\n",
    "    b_end = int(ODOR_ON_S * fps)\n",
    "    b_end = min(b_end, env_full.size)\n",
    "    before = env_full[:b_end]\n",
    "    if before.size == 0:\n",
    "        return np.nan\n",
    "    mu = float(np.nanmean(before))\n",
    "    sd = float(np.nanstd(before))\n",
    "    return mu + THRESH_STD_MULT * sd\n",
    "\n",
    "# --- Odor naming per dataset + trial (ADDED BACK) ---\n",
    "def display_odor_for_trial(dataset_canon: str, trial_label: str) -> str:\n",
    "    n = _trial_num(trial_label)\n",
    "    if n in (1, 3):  # controls\n",
    "        return \"Hexanol\"\n",
    "    if n in (2, 4, 5):  # trained odor\n",
    "        return DISPLAY_LABEL.get(dataset_canon, dataset_canon)\n",
    "\n",
    "    if dataset_canon == \"ACV\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    elif dataset_canon == \"3-octonol\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Citral\"\n",
    "        if n == 8: return \"Linalool\"\n",
    "    elif dataset_canon == \"Benz\":\n",
    "        if n == 6: return \"Citral\"\n",
    "        if n == 7: return \"Linalool\"\n",
    "    elif dataset_canon == \"EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"ret_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"10s_Odor_Benz\":\n",
    "        if n == 6: return \"Benzaldehyde\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "    elif dataset_canon == \"opto_EB\":\n",
    "        if n == 6: return \"Apple Cider Vinegar\"\n",
    "        if n == 7: return \"3-Octonol\"\n",
    "        if n == 8: return \"Benzaldehyde\"\n",
    "        if n == 9: return \"Citral\"\n",
    "        if n == 10: return \"Linalool\"\n",
    "    elif dataset_canon == \"opto_benz\":\n",
    "        if n == 6: return \"3-Octonol\"\n",
    "        if n == 7: return \"Benzaldehyde\"\n",
    "        if n == 8: return \"Citral\"\n",
    "        if n == 9: return \"Linalool\"\n",
    "    return trial_label\n",
    "\n",
    "# ========= MAKE FIGURES PER FLY =========\n",
    "for fly, g in df.groupby(\"fly\"):\n",
    "    # Only keep testing 2/4/5/8\n",
    "    g = g[g[\"trial_label\"].map(_trial_num).isin(KEEP_TEST_NUMS)].copy()\n",
    "    if g.empty:\n",
    "        print(f\"[SKIP] {fly}: no testing trials in {sorted(KEEP_TEST_NUMS)}.\")\n",
    "        continue\n",
    "\n",
    "    g = g.sort_values(\"trial_label\", key=lambda s: s.map(_trial_num))\n",
    "    dataset_canon = _canon_dataset(g[\"dataset\"].iloc[0])\n",
    "\n",
    "    # Collect per-trial data and global limits\n",
    "    trials = []\n",
    "    y_max = 0.0\n",
    "    heat_vmin, heat_vmax = np.inf, -np.inf\n",
    "\n",
    "    for _, row in g.iterrows():\n",
    "        env_full = _extract_env(row)\n",
    "        if env_full.size == 0:\n",
    "            continue\n",
    "\n",
    "        fps = float(row[\"fps\"]) if np.isfinite(row[\"fps\"]) else FPS_DEFAULT\n",
    "        t_full = np.arange(env_full.size, dtype=float) / max(fps, 1e-9)\n",
    "\n",
    "        # clip to [0, X_MAX_LIMIT]\n",
    "        mask = (t_full <= X_MAX_LIMIT + 1e-9)\n",
    "        t = t_full[mask]\n",
    "        env = env_full[mask]\n",
    "        if t.size == 0:\n",
    "            continue\n",
    "\n",
    "        theta = _compute_theta(env_full, fps)\n",
    "        tn = _trial_num(row[\"trial_label\"])\n",
    "        odor_name = display_odor_for_trial(dataset_canon, row[\"trial_label\"])\n",
    "\n",
    "        local_max = np.nanmax(env) if np.isfinite(env).any() else 0.0\n",
    "        if np.isfinite(theta):\n",
    "            local_max = max(local_max, theta)\n",
    "        y_max = max(y_max, float(local_max))\n",
    "\n",
    "        if np.isfinite(env).any():\n",
    "            heat_vmin = min(heat_vmin, float(np.nanmin(env)))\n",
    "            heat_vmax = max(heat_vmax, float(np.nanmax(env)))\n",
    "\n",
    "        trials.append((tn, odor_name, t, env, theta))\n",
    "\n",
    "    if not trials:\n",
    "        print(f\"[SKIP] {fly}: no usable trials after filtering.\")\n",
    "        continue\n",
    "\n",
    "    # Safe heat range\n",
    "    if (not np.isfinite(heat_vmin)) or (not np.isfinite(heat_vmax)) or (heat_vmin == heat_vmax):\n",
    "        heat_vmin, heat_vmax = 0.0, max(1.0, y_max)\n",
    "\n",
    "    # Layout: 2 rows per trial (trace, heatmap) and 2 columns ([plots | colorbar])\n",
    "    n = len(trials)\n",
    "    height_ratios = []\n",
    "    for _ in range(n):\n",
    "        height_ratios.extend([2.2, 1.2])  # trace taller than heatmap\n",
    "\n",
    "    # >>> Use constrained_layout so colorbar ticks/labels are NEVER clipped\n",
    "    fig_h = max(4.0, n * 2.2 + 1.5)\n",
    "    fig = plt.figure(figsize=(FIG_W, fig_h), constrained_layout=True)\n",
    "    gs = GridSpec(\n",
    "        nrows=2*n, ncols=2, figure=fig,\n",
    "        height_ratios=height_ratios,\n",
    "        width_ratios=[1.0, CBAR_COL_RATIO],\n",
    "        hspace=HSPACE, wspace=WSPACE\n",
    "    )\n",
    "\n",
    "    axes_heat = []\n",
    "    for idx, (tn, odor_name, t, env, theta) in enumerate(trials):\n",
    "        ax_trace = fig.add_subplot(gs[2*idx, 0])\n",
    "        ax_heat  = fig.add_subplot(gs[2*idx + 1, 0])\n",
    "        axes_heat.append(ax_heat)\n",
    "\n",
    "        # ---- top: RMS trace ----\n",
    "        ax_trace.plot(t, env, linewidth=1.2, color=\"black\")\n",
    "        ax_trace.axvline(ODOR_ON_S,  linestyle=\"--\", linewidth=TRACE_ONOFF_LW, color=\"black\")\n",
    "        ax_trace.axvline(ODOR_OFF_S, linestyle=\"--\", linewidth=TRACE_ONOFF_LW, color=\"black\")\n",
    "        ax_trace.axvspan(ODOR_ON_S, ODOR_OFF_S, color=FILL_GRAY)\n",
    "\n",
    "        if np.isfinite(theta):\n",
    "            ax_trace.axhline(theta, linestyle=\"-\", linewidth=1.0, color=\"tab:red\",\n",
    "                             label=r'$\\theta = \\mu_\\mathrm{before} + 4\\sigma$')\n",
    "\n",
    "        ax_trace.set_xlim(0, X_MAX_LIMIT)\n",
    "        ax_trace.set_ylim(0, y_max * 1.03 if y_max > 0 else 1.0)\n",
    "        ax_trace.set_ylabel(\"RMS\", fontsize=10)\n",
    "        ax_trace.set_title(f\"{odor_name}\", loc=\"left\", fontsize=11, weight=\"bold\", pad=2)\n",
    "\n",
    "        if idx == 0 and np.isfinite(theta):\n",
    "            ax_trace.legend(loc=\"upper right\", fontsize=9, frameon=True, framealpha=1.0)\n",
    "\n",
    "        # ---- bottom: 1×T heatmap ----\n",
    "        heat = env[np.newaxis, :]\n",
    "        ax_heat.imshow(\n",
    "            heat,\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "            extent=(t[0] if t.size else 0, t[-1] if t.size else X_MAX_LIMIT, 0, 1),\n",
    "            vmin=heat_vmin, vmax=heat_vmax,\n",
    "            cmap=\"viridis\", interpolation=\"nearest\",\n",
    "            rasterized=RASTERIZE_HEATMAPS\n",
    "        )\n",
    "        \n",
    "        # Gray ON window + thick dashed black ON/OFF lines\n",
    "        ax_heat.axvline(ODOR_ON_S,  linestyle=HM_ONOFF_LS, linewidth=HM_ONOFF_LW, color=\"black\")\n",
    "        ax_heat.axvline(ODOR_OFF_S, linestyle=HM_ONOFF_LS, linewidth=HM_ONOFF_LW, color=\"black\")\n",
    "\n",
    "        ax_heat.set_xlim(0, X_MAX_LIMIT)\n",
    "        ax_heat.set_yticks([])   # remove ticks\n",
    "        ax_heat.set_ylabel(\"\")   # remove label\n",
    "\n",
    "        if idx == (n - 1):\n",
    "            ax_heat.set_xlabel(\"Time (s)\", fontsize=11)\n",
    "        else:\n",
    "            ax_heat.set_xticklabels([])\n",
    "\n",
    "    # ---- dedicated colorbar column (prevents overlap) ----\n",
    "    cax = fig.add_subplot(gs[:, 1])  # entire right column\n",
    "    sm  = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=heat_vmin, vmax=heat_vmax), cmap=\"viridis\")\n",
    "    cbar = fig.colorbar(sm, cax=cax)\n",
    "    cbar.set_label(\"RMS\", fontsize=10)\n",
    "    cbar.ax.tick_params(labelsize=9)\n",
    "\n",
    "    # Suptitle with extra gap from first axes; constrained_layout manages spacing\n",
    "    fig.suptitle(f\"{fly} — RMS trace (top) + Heatmap (bottom)\",\n",
    "                 y=TITLE_Y, fontsize=14, weight=\"bold\")\n",
    "\n",
    "    out_base = OUT_DIR / f\"{fly}_rms_trace_plus_heatmaps_testing_2_4_5_8\"\n",
    "    fig.savefig(out_base.with_suffix(\".png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    fig.savefig(out_base.with_suffix(\".eps\"), format=\"eps\", bbox_inches=\"tight\")  # no alpha anywhere\n",
    "\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] Saved {out_base}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f210cf9-abc1-4e4b-8162-a6d123362e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
